from typing import Dict, Any
from pydantic import BaseModel, Field
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

from config.settings import get_settings
from utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()


class GitHubQuestionAnalysis(BaseModel):
    is_github_question: bool = Field(
        description="True si la question n√©cessite des informations depuis l'API GitHub (PRs, owner, issues, contributors, etc.)"
    )
    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description="Score de confiance de la classification (0.0 √† 1.0)"
    )
    reasoning: str = Field(
        description="Explication courte de pourquoi c'est (ou pas) une question GitHub"
    )


def create_github_question_detection_chain(provider: str = "anthropic"):
    logger.info(f"üîó Cr√©ation chain d√©tection questions GitHub (provider: {provider})")
    
    if provider.lower() == "anthropic":
        llm = ChatAnthropic(
            model="claude-3-5-sonnet-20241022",
            anthropic_api_key=settings.anthropic_api_key,
            temperature=0.1,  
            max_tokens=500
        )
        logger.info("‚úÖ LLM Anthropic initialis√©: claude-3-5-sonnet-20241022")
    else:
        llm = ChatOpenAI(
            model="gpt-4",
            openai_api_key=settings.openai_api_key,
            temperature=0.1,
            max_tokens=500
        )
        logger.info("‚úÖ LLM OpenAI initialis√©: gpt-4")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Tu es un assistant sp√©cialis√© dans la classification de questions.

Ta t√¢che est de d√©terminer si une question n√©cessite des informations depuis l'API GitHub pour y r√©pondre.

**Questions GitHub** (is_github_question = True):
Ces questions n√©cessitent des informations que seule l'API GitHub peut fournir:
- Pull Requests: "De quoi parle le dernier PR?", "Quels fichiers ont chang√© dans le PR #66?", "Combien de PR ouvertes?"
- Owner/Repository: "Qui est l'owner?", "√Ä qui appartient ce repo?", "Combien d'√©toiles?"
- Issues: "Combien d'issues ouvertes?", "Quels bugs sont en cours?"
- Contributors: "Qui contribue?", "Qui a fait le plus de commits?"
- Activit√©: "Quand a eu lieu le dernier commit?", "Le projet est-il actif?"

**Questions NON-GitHub** (is_github_question = False):
Ces questions peuvent √™tre r√©pondues par l'analyse du code source du projet:
- Langages/Technologies: "Quel langage est utilis√©?", "Quelles d√©pendances?"
- Architecture: "Comment est structur√© le projet?", "Quelle architecture?"
- Code: "Comment fonctionne cette fonction?", "O√π est d√©finie la classe X?"
- Fonctionnalit√©s: "Que fait ce projet?", "Comment utiliser ce module?"

R√àGLE SIMPLE:
- Si l'information vient des **m√©tadonn√©es GitHub** (PRs, issues, stars, contributors) ‚Üí True
- Si l'information vient du **code source analys√©** ‚Üí False"""),
        ("human", "Question: {question}")
    ])
    
    chain = prompt | llm.with_structured_output(GitHubQuestionAnalysis)
    
    logger.info("‚úÖ Chain d√©tection questions GitHub cr√©√©e avec succ√®s")
    return chain


async def detect_github_question(
    question: str,
    provider: str = "anthropic",
    fallback_to_openai: bool = True
) -> GitHubQuestionAnalysis:
    """
    D√©tecte si une question n√©cessite des informations GitHub via un LLM.
    
    Args:
        question: Question √† analyser
        provider: Provider principal ("anthropic" ou "openai")
        fallback_to_openai: Si True, fallback vers OpenAI en cas d'√©chec
        
    Returns:
        Analyse structur√©e de la question
    """
    logger.info("üîç D√©tection question GitHub via LLM...")
    logger.info(f"‚ùì Question: '{question[:100]}...'")
    
    try:
        chain = create_github_question_detection_chain(provider=provider)
        
        logger.info(f"üöÄ Analyse avec {provider}...")
        analysis = await chain.ainvoke({"question": question})
        
        logger.info(f"‚úÖ D√©tection termin√©e: is_github={analysis.is_github_question}, type={analysis.question_type}")
        logger.info(f"   Confiance: {analysis.confidence:.2f}")
        logger.info(f"   Raisonnement: {analysis.reasoning}")
        
        return analysis
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec d√©tection avec {provider}: {e}")
        
        if fallback_to_openai and provider.lower() != "openai":
            try:
                logger.info("üîÑ Fallback vers OpenAI...")
                chain_fallback = create_github_question_detection_chain(provider="openai")
                analysis = await chain_fallback.ainvoke({"question": question})
                
                logger.info(f"‚úÖ D√©tection termin√©e (fallback): is_github={analysis.is_github_question}")
                return analysis
            
            except Exception as fallback_error:
                logger.error(f"‚ùå Fallback OpenAI √©chou√©: {fallback_error}")
                return GitHubQuestionAnalysis(
                    is_github_question=False,
                    confidence=0.3,
                    reasoning="Erreur lors de la d√©tection - d√©faut: pas une question GitHub"
                )
        
        logger.error(f"‚ùå D√©tection question GitHub √©chou√©e: {e}")
        return GitHubQuestionAnalysis(
            is_github_question=False,
            confidence=0.3,
            reasoning="Erreur lors de la d√©tection - d√©faut: pas une question GitHub"
        )


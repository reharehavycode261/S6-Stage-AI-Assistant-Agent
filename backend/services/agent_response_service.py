"""
Service de r√©ponse directe pour les questions informatives (Type 1).

Ce service:
- G√©n√®re des r√©ponses √† des questions sur le projet
- Analyse le contexte du projet (code, structure, technologies)
- Poste la r√©ponse directement dans Monday.com
- N'utilise PAS le workflow complet (pas de PR, pas de validation humaine)
"""

from typing import Dict, Any, Optional
from utils.logger import get_logger
from config.settings import get_settings
from services.github_context_enricher import github_context_enricher
import openai

logger = get_logger(__name__)
settings = get_settings()


class AgentResponseService:
    """
    Service pour g√©n√©rer et poster des r√©ponses informatives.
    
    Flux AM√âLIOR√â:
    1. Recevoir une question
    2. EXPLORER LE PROJET COMPLET:
       - Clone le repository (prepare_environment)
       - Analyse la structure, le code, les technologies (analyze_requirements)
    3. G√©n√©rer une r√©ponse enrichie avec OpenAI bas√©e sur l'exploration
    4. Poster la r√©ponse dans Monday.com
    
    IMPORTANT: 
    - Ex√©cute prepare_environment et analyze_requirements pour avoir le contexte complet
    - S'arr√™te AVANT impl√©mentation (pas de modifications)
    - S'arr√™te AVANT validation humaine
    """
    
    def __init__(self):
        """Initialise le service de r√©ponse."""
        self.openai_client = None
        if settings.openai_api_key:
            self.openai_client = openai.AsyncOpenAI(api_key=settings.openai_api_key)
        else:
            logger.warning("‚ö†Ô∏è OpenAI API key non configur√©e - R√©ponses d√©sactiv√©es")
        
        from services.github import GitHubInformationOrchestrator, GitHubCollectorConfig
        self.github_orchestrator = GitHubInformationOrchestrator(settings.github_token)
        
        self.github_config = GitHubCollectorConfig(
            limit_prs=3,
            limit_issues=5,
            limit_commits=5,
            limit_branches=5,
            limit_releases=3,
            limit_contributors=5,
            limit_labels=10,
            limit_milestones=3,
            include_pr_files=False,
            include_commit_files=False,
            include_closed_issues=False,
            include_all_branches=False
        )
    
    async def generate_and_post_response(
        self,
        question: str,
        task_context: Dict[str, Any],
        monday_item_id: str,
        task: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse √† une question et la poste dans Monday.com.
        
        NOUVEAU FLUX:
        1. Explorer le projet complet via ProjectExplorationService
           - Clone le repository (prepare_environment)
           - Analyse compl√®te (analyze_requirements)
        2. G√©n√©rer une r√©ponse enrichie avec OpenAI bas√©e sur l'exploration r√©elle
        3. Poster la r√©ponse dans Monday.com
        
        Args:
            question: Question pos√©e par l'utilisateur
            task_context: Contexte de la t√¢che (titre, description, statut, etc.)
            monday_item_id: ID de l'item Monday.com o√π poster la r√©ponse
            task: Objet Task pour l'exploration (optionnel)
            
        Returns:
            R√©sultat de l'op√©ration avec la r√©ponse g√©n√©r√©e
        """
        logger.info(f"üí¨ G√©n√©ration r√©ponse pour question: '{question[:50]}...'")
        
        if not self.openai_client:
            error_msg = "OpenAI API non configur√©e - impossible de g√©n√©rer une r√©ponse"
            logger.error(f"‚ùå {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }
        
        try:
            project_context = await self._explore_project_full(question, task_context, task)
            
            response_text = await self._generate_response(question, task_context, project_context)
            
            logger.info(f"‚úÖ R√©ponse g√©n√©r√©e: {len(response_text)} caract√®res")
            
            creator_name = None
            if task:
                if isinstance(task, dict):
                    creator_name = task.get('creator_name')
                elif hasattr(task, 'creator_name'):
                    creator_name = task.creator_name
            
            user_language = task_context.get('user_language', 'en')
            project_language = task_context.get('project_language', 'en')
            
            post_result = await self._post_response_to_monday(
                response_text=response_text,
                monday_item_id=monday_item_id,
                original_question=question,
                creator_name=creator_name,
                user_language=user_language,
                project_language=project_language
            )
            
            if post_result.get("success"):
                logger.info(f"‚úÖ R√©ponse post√©e dans Monday.com: item {monday_item_id}")
                return {
                    "success": True,
                    "response_text": response_text,
                    "monday_update_id": post_result.get("update_id"),
                    "message": "R√©ponse g√©n√©r√©e et post√©e avec succ√®s",
                    "project_explored": project_context.get("success", False)
                }
            else:
                logger.error(f"‚ùå √âchec post r√©ponse Monday.com: {post_result.get('error')}")
                return {
                    "success": False,
                    "error": f"√âchec post Monday.com: {post_result.get('error')}",
                    "response_text": response_text  
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration r√©ponse: {e}", exc_info=True)
            return {
                "success": False,
                "error": f"Erreur g√©n√©ration r√©ponse: {str(e)}"
            }
    
    async def _explore_project_full(
        self, 
        question: str, 
        task_context: Dict[str, Any],
        task: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        Explore le projet complet via les n≈ìuds du workflow.
        
        Ex√©cute:
        - prepare_environment: Clone le repo, setup l'environnement
        - analyze_requirements: Analyse compl√®te du code, structure, technologies
        
        N'ex√©cute PAS:
        - N≈ìud d'impl√©mentation (pas de modifications)
        - N≈ìud de tests
        - N≈ìud de PR
        - N≈ìud de validation humaine
        
        Args:
            question: Question de l'utilisateur
            task_context: Contexte de la t√¢che
            task: Objet Task (optionnel, charg√© si non fourni)
            
        Returns:
            Contexte enrichi du projet avec analyse compl√®te
        """
        logger.info("="*80)
        logger.info("üîç D√âBUT EXPLORATION COMPL√àTE DU PROJET")
        logger.info("="*80)
        logger.info(f"‚ùì Question: '{question[:100]}...'")
        logger.info(f"üì¶ Task fourni: {'Oui' if task else 'Non'}")
        logger.info("="*80)
        
        try:
            if not task:
                logger.info("‚è≥ Task non fourni - chargement depuis la base...")
                task = await self._load_task_from_context(task_context)
                if not task:
                    logger.error("="*80)
                    logger.error("‚ùå √âCHEC CHARGEMENT TASK - FALLBACK ANALYSE BASIQUE")
                    logger.error("="*80)
                    logger.warning("‚ö†Ô∏è Impossible de charger la t√¢che - fallback analyse basique")
                    return await self._analyze_project_context_basic(task_context)
                else:
                    logger.info(f"‚úÖ Task charg√©e avec succ√®s: {task.get('title', 'N/A')}")
            else:
                logger.info(f"‚úÖ Task d√©j√† fournie: {task.get('title', 'N/A') if isinstance(task, dict) else task}")
            
            logger.info("üöÄ Lancement ProjectExplorationService...")
            from services.project_exploration_service import project_exploration_service
            
            exploration_result = await project_exploration_service.explore_project_for_question(
                task=task,
                question=question,
                task_context=task_context
            )
            
            logger.info("="*80)
            logger.info("üìä R√âSULTAT EXPLORATION")
            logger.info("="*80)
            logger.info(f"‚úÖ Succ√®s: {exploration_result.get('success')}")
            if not exploration_result.get("success"):
                logger.error(f"‚ùå Erreur: {exploration_result.get('error')}")
                logger.error(f"üìç Phase: {exploration_result.get('phase')}")
            logger.info("="*80)
            
            if exploration_result.get("success"):
                logger.info("‚úÖ‚úÖ‚úÖ Exploration compl√®te du projet TERMIN√âE AVEC SUCC√àS")
                project_context = exploration_result.get("project_context", {})
                
                logger.info(f"üìä Technologies d√©tect√©es: {project_context.get('technologies', [])}")
                logger.info(f"üìÅ Fichiers analys√©s: {len(project_context.get('file_structure', []))}")
                logger.info(f"üîó Repository: {project_context.get('repository_url', 'N/A')}")
                
                project_context.update({
                    "title": task_context.get("title", ""),
                    "description": task_context.get("description", ""),
                    "repository_url": task_context.get("repository_url", ""),
                    "exploration_successful": True
                })
                
                github_context = await self._enrich_with_github_info(
                    question=question,
                    repository_url=project_context.get("repository_url")
                )
                if github_context:
                    project_context["github_info"] = github_context
                    logger.info(f"‚úÖ Contexte enrichi avec informations GitHub")
                
                return project_context
            else:
                error = exploration_result.get("error", "Erreur inconnue")
                logger.error("="*80)
                logger.error("‚ùå EXPLORATION INCOMPL√àTE OU √âCHOU√âE")
                logger.error("="*80)
                logger.warning(f"‚ö†Ô∏è Exploration incompl√®te: {error}")
                logger.info("üìã Tentative utilisation contexte partiel...")
                
                partial_context = exploration_result.get("partial_context", {})
                if partial_context:
                    logger.info("‚úÖ Contexte partiel disponible - utilisation")
                    partial_context.update({
                        "title": task_context.get("title", ""),
                        "description": task_context.get("description", ""),
                        "exploration_successful": False,
                        "exploration_error": error
                    })
                    return partial_context
                else:
                    logger.warning("‚ö†Ô∏è Aucun contexte partiel - fallback analyse basique")
                    return await self._analyze_project_context_basic(task_context)
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur exploration projet: {e}", exc_info=True)
            logger.info("üìã Fallback: analyse basique sans exploration")
            return await self._analyze_project_context_basic(task_context)
    
    async def _load_task_from_context(self, task_context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Charge les donn√©es de la t√¢che depuis le contexte.
        
        Args:
            task_context: Contexte de la t√¢che
            
        Returns:
            Dictionnaire avec les donn√©es de la t√¢che ou None
        """
        try:
            from services.database_persistence_service import db_persistence
            
            logger.info("="*80)
            logger.info("üì• CHARGEMENT TASK DEPUIS CONTEXTE")
            logger.info("="*80)
            logger.info(f"üìã Cl√©s disponibles dans task_context: {list(task_context.keys())}")
            
            task_id = task_context.get("tasks_id")
            if not task_id:
                logger.error("‚ùå tasks_id NON TROUV√â dans task_context")
                logger.error(f"üì¶ task_context complet: {task_context}")
                return None
            
            logger.info(f"üîç Chargement t√¢che ID: {task_id}")
            
            task_data = await db_persistence.get_task_by_id(task_id)
            if not task_data:
                logger.error(f"‚ùå T√¢che {task_id} NON TROUV√âE en base de donn√©es")
                return None
            
            logger.info(f"‚úÖ T√¢che {task_id} charg√©e avec succ√®s")
            logger.info(f"üìã Titre: {task_data.get('title', 'N/A')}")
            logger.info(f"üîó Repository: {task_data.get('repository_url', 'N/A')}")
            logger.info("="*80)
            
            return task_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement t√¢che: {e}", exc_info=True)
            return None
    
    async def _detect_github_question(self, question: str) -> bool:
        """
        D√©tecte si la question concerne des informations GitHub via un LLM.
        
        Args:
            question: Question de l'utilisateur
            
        Returns:
            True si c'est une question GitHub, False sinon
        """
        from ai.chains.github_question_detection_chain import detect_github_question
        
        logger.info("üîç D√©tection question GitHub via LLM...")
        
        try:
            analysis = await detect_github_question(
                question=question,
                provider="anthropic",
                fallback_to_openai=True
            )
            
            if analysis.is_github_question:
                logger.info(f"‚úÖ Question GitHub d√©tect√©e (confiance: {analysis.confidence:.2f})")
                logger.info(f"   Raisonnement: {analysis.reasoning}")
            else:
                logger.info(f"‚ÑπÔ∏è  Question NON-GitHub - Analyse du code suffira (confiance: {analysis.confidence:.2f})")
                logger.info(f"   Raisonnement: {analysis.reasoning}")
            
            return analysis.is_github_question
        
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection question GitHub: {e}", exc_info=True)
            return False
    
    async def _enrich_with_github_info(
        self,
        question: str,
        repository_url: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """
        Enrichit le contexte avec TOUTES les informations GitHub disponibles via l'orchestrateur OO.
        Le LLM d√©cidera ensuite quelles informations utiliser pour r√©pondre.
        
        NOUVELLE ARCHITECTURE ORIENT√âE OBJET:
        - Utilise GitHubInformationOrchestrator
        - Collecte automatique de toutes les donn√©es (PRs, Issues, Commits, Branches, Releases, etc.)
        - Format automatique pour le LLM
        - Extensible facilement avec de nouveaux collecteurs
        
        Args:
            question: Question de l'utilisateur
            repository_url: URL du repository GitHub
            
        Returns:
            Contexte GitHub complet ou None si pas pertinent
        """
        if not repository_url:
            return None
        
        # D√©tecter si la question concerne GitHub via LLM
        is_github_question = await self._detect_github_question(question)
        
        if not is_github_question:
            return None
        
        logger.info("="*80)
        logger.info("üì¶ ORCHESTRATEUR GITHUB - COLLECTE COMPL√àTE (OO)")
        logger.info("="*80)
        logger.info("‚ÑπÔ∏è  Architecture: Collecteurs orient√©s objet extensibles")
        logger.info("‚ÑπÔ∏è  Strat√©gie: R√©cup√©rer TOUTES les infos, le LLM choisira")
        
        try:
            collected_data = await self.github_orchestrator.collect_all(
                repository_url=repository_url,
                config=self.github_config,
                collectors=None  
            )
            
            if not collected_data.get("success"):
                logger.error(f"‚ùå √âchec collecte GitHub: {collected_data.get('error')}")
                return None
            
            github_context = collected_data.get("data", {})
            
            successful_collectors = [
                key for key, value in github_context.items()
                if value.get("success", False)
            ]
            
            logger.info(f"‚úÖ Contexte GitHub complet r√©cup√©r√©:")
            logger.info(f"   - Collecteurs r√©ussis: {len(successful_collectors)}/{len(github_context)}")
            logger.info(f"   - Types de donn√©es: {', '.join(successful_collectors)}")
            logger.info("="*80)
            
            return github_context if github_context else None
        
        except Exception as e:
            logger.error(f"‚ùå Erreur orchestrateur GitHub: {e}", exc_info=True)
            return None
    
    async def _analyze_project_context_basic(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyse basique du contexte sans exploration compl√®te (fallback).
        
        Args:
            task_context: Contexte de la t√¢che
            
        Returns:
            Contexte enrichi basique du projet
        """
        logger.debug("üîç Analyse basique du contexte du projet (sans exploration)...")
        
        repository_url = task_context.get("repository_url")
        title = task_context.get("title", "")
        description = task_context.get("description", "")
        
        context = {
            "has_repository": bool(repository_url),
            "repository_url": repository_url,
            "title": title,
            "description": description,
            "technologies": [],
            "file_structure": None,
            "exploration_successful": False,
            "exploration_mode": "basic"
        }
        
        technologies = self._detect_technologies(description)
        context["technologies"] = technologies
        
        logger.debug(f"üìã Technologies d√©tect√©es (basique): {technologies}")
        
        return context
    
    def _detect_technologies(self, text: str) -> list:
        """
        D√©tecte les technologies mentionn√©es dans le texte.
        
        Args:
            text: Texte √† analyser (description, titre, etc.)
            
        Returns:
            Liste des technologies d√©tect√©es
        """
        if not text:
            return []
        
        text_lower = text.lower()
        
        tech_keywords = {
            "python": "Python",
            "java": "Java",
            "javascript": "JavaScript",
            "typescript": "TypeScript",
            "react": "React",
            "vue": "Vue.js",
            "angular": "Angular",
            "django": "Django",
            "flask": "Flask",
            "fastapi": "FastAPI",
            "spring": "Spring",
            "node": "Node.js",
            "express": "Express.js",
            "postgres": "PostgreSQL",
            "mysql": "MySQL",
            "mongodb": "MongoDB",
            "redis": "Redis",
            "docker": "Docker",
            "kubernetes": "Kubernetes",
            "aws": "AWS",
            "azure": "Azure",
            "gcp": "Google Cloud",
        }
        
        detected = []
        for keyword, tech_name in tech_keywords.items():
            if keyword in text_lower:
                detected.append(tech_name)
        
        return detected
    
    async def _generate_response(
        self,
        question: str,
        task_context: Dict[str, Any],
        project_context: Dict[str, Any]
    ) -> str:
        """
        G√©n√®re une r√©ponse √† la question avec OpenAI.
        
        ‚úÖ OPTIMIS√â: D√©tecte automatiquement les questions GitHub et r√©cup√®re les vraies donn√©es.
        
        Args:
            question: Question de l'utilisateur
            task_context: Contexte de la t√¢che
            project_context: Contexte du projet analys√©
            
        Returns:
            Texte de la r√©ponse
        """
        if self._is_greeting(question):
            logger.info("üëã Salutation d√©tect√©e - R√©ponse format√©e sp√©ciale")
            user_language = task_context.get('user_language', 'en')
            return await self._get_greeting_response(user_language)
        
        github_data = None
        
        if project_context.get("github_info"):
            logger.info("‚ôªÔ∏è  R√©utilisation des donn√©es GitHub d√©j√† collect√©es (√©vite double collecte)")
            github_data = github_context_enricher.extract_github_data({"success": True, "data": project_context["github_info"]})
            logger.info(f"‚úÖ Donn√©es structur√©es (cache): {list(github_data.keys())}")
        else:
            needs_github = await self._detect_github_question(question)
            
            if needs_github and task_context.get("repository_url"):
                logger.info(f"üîç Question GitHub d√©tect√©e pour {task_context.get('repository_url')}")
                
                orchestrator_result = await self._fetch_github_raw(
                    question,
                    task_context.get("repository_url"),
                    task_context.get("repository_name"),
                    task_context.get("default_branch", "main")
                )
                
                if orchestrator_result and orchestrator_result.get("success"):
                    github_data = github_context_enricher.extract_github_data(orchestrator_result)
                    logger.info(f"‚úÖ Donn√©es structur√©es: {list(github_data.keys())}")
                else:
                    logger.warning("‚ö†Ô∏è Aucune donn√©es GitHub r√©cup√©r√©e")
        
        prompt = self._create_response_prompt(
            question, 
            task_context, 
            project_context,
            github_data=github_data
        )
        
        logger.debug(f"ü§ñ G√©n√©ration r√©ponse avec OpenAI ({len(prompt)} caract√®res de prompt)")
        
        user_language = task_context.get('user_language', 'en')
        system_prompt = await self._get_system_prompt_for_language(user_language)
        
        response = await self.openai_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.7,  
            max_tokens=1500
        )
        
        response_text = response.choices[0].message.content.strip()
        
        logger.info(f"‚úÖ R√©ponse g√©n√©r√©e: {len(response_text)} caract√®res")
        
        return response_text
    
    async def _get_greeting_response(self, language_code: str) -> str:
        """
        G√©n√®re dynamiquement une r√©ponse de salutation dans N'IMPORTE QUELLE langue via LLM.
        
        Args:
            language_code: Code langue ISO 639-1 (2 lettres)
            
        Returns:
            Message de salutation multilingue
        """
        try:
            from services.project_language_detector import project_language_detector
            
            language_name = project_language_detector.LANGUAGE_NAMES.get(
                language_code, 
                language_code.upper()
            )
            
            logger.info(f"üëã G√©n√©ration message de salutation pour: {language_name} ({language_code})")
            
            # Template de r√©f√©rence en anglais
            reference_greeting = """Hello! üëã I'm **VyData**, your AI development assistant.

I can help you with:

üí¨ **Answering your questions** about your project
- Explain code and architecture
- Analyze technology choices
- Provide information on Git history

üîç **Analyzing code** in depth
- Project structure
- Dependencies and technologies used
- Commits, branches and pull requests

üõ†Ô∏è **Implementing new features**
- Add complete features
- Create components and services
- Follow best practices

üêõ **Fixing bugs**
- Identify and resolve issues
- Optimize performance
- Improve code quality

üìã **Creating Pull Requests** automatically
- Clean and tested code
- Complete documentation
- Ready for code review

How can I help you today? üòä"""
            
            # Si c'est l'anglais, retourner directement
            if language_code == 'en':
                return reference_greeting
            
            # Sinon, g√©n√©rer via LLM
            system_prompt = f"""Tu es un expert en traduction pour assistants IA.
Traduis le message de salutation suivant en {language_name}.

R√àGLES CRITIQUES:
1. Garde EXACTEMENT la m√™me structure avec les sections
2. Garde TOUS les emojis (üëã, üí¨, üîç, üõ†Ô∏è, üêõ, üìã, üòä)
3. Traduis le contenu de mani√®re naturelle et engageante
4. Conserve le ton professionnel mais amical
5. Adapte les salutations au contexte culturel (tutoiement/vouvoiement)
6. R√©ponds UNIQUEMENT avec la traduction, sans commentaire"""

            user_prompt = f"""Traduis ce message de salutation en {language_name} ({language_code}):

{reference_greeting}

IMPORTANT: R√©ponds UNIQUEMENT avec la traduction du message, rien d'autre."""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            
            translated_greeting = response.choices[0].message.content.strip()
            
            logger.info(f"‚úÖ Message de salutation g√©n√©r√© pour {language_name} ({len(translated_greeting)} caract√®res)")
            
            return translated_greeting
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration salutation: {e}")
            logger.warning(f"‚ö†Ô∏è Fallback vers anglais")
            # Fallback en anglais
            return """Hello! üëã I'm **VyData**, your AI development assistant.

I can help you with:

üí¨ **Answering your questions** about your project
- Explain code and architecture
- Analyze technology choices
- Provide information on Git history

üîç **Analyzing code** in depth
- Project structure
- Dependencies and technologies used
- Commits, branches and pull requests

üõ†Ô∏è **Implementing new features**
- Add complete features
- Create components and services
- Follow best practices

üêõ **Fixing bugs**
- Identify and resolve issues
- Optimize performance
- Improve code quality

üìã **Creating Pull Requests** automatically
- Clean and tested code
- Complete documentation
- Ready for code review

How can I help you today? üòä"""
    
    async def _get_system_prompt_for_language(self, language_code: str) -> str:
        """
        G√©n√®re dynamiquement le prompt syst√®me dans N'IMPORTE QUELLE langue via LLM.
        
        Args:
            language_code: Code langue ISO 639-1 (2 lettres)
            
        Returns:
            Prompt syst√®me dans la bonne langue
        """
        try:
            from services.project_language_detector import project_language_detector
            
            language_name = project_language_detector.LANGUAGE_NAMES.get(
                language_code, 
                language_code.upper()
            )
            
            logger.info(f"ü§ñ G√©n√©ration prompt syst√®me pour la langue: {language_name} ({language_code})")
            
            # Template de r√©f√©rence en anglais
            reference_prompt = """You are VyData, an AI assistant expert in software development.
You answer questions about development projects in a way that is:
- Clear and concise
- Technical but accessible
- Based on REAL data when available
- Honest (if you don't know, say so)
- Professional and friendly

Response format:
- Start with an appropriate emoji (üí°, üìã, üîç, etc.)
- Use the REAL DATA provided (commits, PRs, structure)
- Structure your response with bullets or numbers if needed
- End with an offer of additional help if relevant"""
            
            # Si c'est l'anglais, retourner directement
            if language_code == 'en':
                return reference_prompt
            
            # Sinon, g√©n√©rer via LLM
            system_prompt = f"""Tu es un expert en traduction technique pour assistants IA de d√©veloppement.
Traduis le prompt syst√®me suivant en {language_name}.

R√àGLES CRITIQUES:
1. Garde EXACTEMENT la m√™me structure et les m√™mes sections
2. Garde TOUS les emojis (üí°, üìã, üîç)
3. Traduis le contenu de mani√®re naturelle et fluide
4. Conserve le ton professionnel mais amical
5. R√©ponds UNIQUEMENT avec la traduction, sans commentaire"""

            user_prompt = f"""Traduis ce prompt syst√®me en {language_name} ({language_code}):

{reference_prompt}

IMPORTANT: R√©ponds UNIQUEMENT avec la traduction du prompt, rien d'autre."""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            
            translated_prompt = response.choices[0].message.content.strip()
            
            logger.info(f"‚úÖ Prompt syst√®me g√©n√©r√© pour {language_name} ({len(translated_prompt)} caract√®res)")
            
            return translated_prompt
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration prompt syst√®me: {e}")
            logger.warning(f"‚ö†Ô∏è Fallback vers anglais")
            # Fallback en anglais
            return """You are VyData, an AI assistant expert in software development.
You answer questions about development projects in a way that is:
- Clear and concise
- Technical but accessible
- Based on REAL data when available
- Honest (if you don't know, say so)
- Professional and friendly

Response format:
- Start with an appropriate emoji (üí°, üìã, üîç, etc.)
- Use the REAL DATA provided (commits, PRs, structure)
- Structure your response with bullets or numbers if needed
- End with an offer of additional help if relevant"""
    
    async def _fetch_github_raw(
        self,
        question: str,
        repository_url: str,
        repository_name: str,
        default_branch: str = "main"
    ) -> Dict[str, Any]:
        """
        ‚úÖ NOUVELLE M√âTHODE: R√©cup√®re les donn√©es BRUTES de l'orchestrateur.
        Sans extraction interm√©diaire - laisse l'enricher OO g√©rer tout.
        
        Args:
            question: Question pour d√©terminer quels collecteurs activer
            repository_url: URL du repository
            repository_name: Nom du repository
            default_branch: Branche par d√©faut
            
        Returns:
            R√©sultat brut de l'orchestrateur GitHub
        """
        try:
            question_lower = question.lower()

            collectors_needed = []
            if any(word in question_lower for word in ["commit", "dernier", "last", "historique"]):
                collectors_needed.append("commits")
            if any(word in question_lower for word in ["pull request", "pr", "merge"]):
                collectors_needed.append("pull_requests")
            if any(word in question_lower for word in ["structure", "fichiers", "files", "arborescence"]):
                collectors_needed.append("repository")
            
            if not collectors_needed:
                collectors_needed = ["repository", "commits", "pull_requests"]
            
            logger.info(f"üì• Collecte GitHub: {collectors_needed}")
            
            result = await self.github_orchestrator.collect_all(
                repository_url=repository_url,
                config=self.github_config
            )
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration GitHub raw: {e}", exc_info=True)
            return {"success": False, "error": str(e)}
    
    def _is_greeting(self, question: str) -> bool:
        """
        D√©tecte si le message est une salutation.
        
        Args:
            question: Question √† analyser
            
        Returns:
            True si c'est une salutation
        """
        question_lower = question.lower().strip()
        
        greetings = [
            "hello", "hi", "hey", "bonjour", "salut", "bonsoir",
            "good morning", "good afternoon", "good evening",
            "yo", "sup", "wassup"
        ]
        
        if len(question_lower.split()) <= 3:
            return any(greeting in question_lower for greeting in greetings)
        
        return False
    
    async def _detect_github_question(self, question: str) -> bool:
        """
        ‚úÖ OPTIMIS√â: D√©tection rapide des questions n√©cessitant GitHub.
        
        Args:
            question: Question √† analyser
            
        Returns:
            True si la question n√©cessite des donn√©es GitHub
        """
        question_lower = question.lower()
        
        github_keywords = [
            "commit", "pull request", "pr", "branch", "branche",
            "dernier", "last", "structure", "fichiers", "files",
            "historique", "history", "contributor", "contributeur"
        ]
        
        return any(keyword in question_lower for keyword in github_keywords)
    
    async def _fetch_github_data(
        self,
        question: str,
        repository_url: str,
        repository_name: str,
        default_branch: str = "main"
    ) -> Dict[str, Any]:
        """
        ‚úÖ OPTIMIS√â: R√©cup√®re les donn√©es GitHub r√©elles selon la question.
        
        Args:
            question: Question pour cibler les donn√©es n√©cessaires
            repository_url: URL du repository
            repository_name: Nom du repository (owner/repo)
            default_branch: Branche par d√©faut
            
        Returns:
            Dict avec les donn√©es GitHub pertinentes
        """
        try:
            question_lower = question.lower()
            
            collectors_needed = []
            
            if any(word in question_lower for word in ["commit", "dernier", "last", "historique"]):
                collectors_needed.append("commits")
                
            if any(word in question_lower for word in ["pull request", "pr", "merge"]):
                collectors_needed.append("pull_requests")
                
            if any(word in question_lower for word in ["structure", "fichiers", "files", "arborescence", "organisation"]):
                collectors_needed.append("repository")
            
            if not collectors_needed:
                collectors_needed = ["repository", "commits", "pull_requests"]
            
            logger.info(f"üì• Collecte GitHub: {collectors_needed}")
            
            result = await self.github_orchestrator.collect_all(
                repository_url=repository_url,
                config=self.github_config
            )
            
            if not result.get("success"):
                logger.warning(f"‚ö†Ô∏è Collecte GitHub √©chou√©e: {result.get('error')}")
                return {}
            
            github_data = result.get("data", {})
            
            formatted_data = {}
            
            if "commits" in github_data:
                commits_dict = github_data["commits"]
                commits_list = commits_dict.get("data", []) if isinstance(commits_dict, dict) else commits_dict
                if commits_list:
                    formatted_data["commits"] = commits_list[:10]
                    formatted_data["last_commit"] = commits_list[0]
            
            if "pull_requests" in github_data:
                prs_dict = github_data["pull_requests"]
                prs_list = prs_dict.get("data", []) if isinstance(prs_dict, dict) else prs_dict
                if prs_list:
                    formatted_data["pull_requests"] = prs_list[:5]
                    formatted_data["last_pr"] = prs_list[0]
            
            if "repository" in github_data:
                formatted_data["repository"] = github_data["repository"]
            
            logger.info(f"‚úÖ Donn√©es GitHub r√©cup√©r√©es: {list(formatted_data.keys())}")
            return formatted_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration GitHub: {e}", exc_info=True)
            return {}
    
    def _create_response_prompt(
        self,
        question: str,
        task_context: Dict[str, Any],
        project_context: Dict[str, Any],
        github_data: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Cr√©e le prompt pour g√©n√©rer la r√©ponse bas√© sur l'exploration compl√®te.
        
        Args:
            question: Question de l'utilisateur
            task_context: Contexte de la t√¢che
            project_context: Contexte du projet (avec exploration compl√®te si disponible)
            
        Returns:
            Prompt format√©
        """
        technologies_str = ", ".join(project_context.get("technologies", [])) or "Non sp√©cifi√©"
        exploration_successful = project_context.get("exploration_successful", False)
        
        has_github_data = github_data and len(github_data) > 0
        
        if exploration_successful or has_github_data:
            analysis_summary = project_context.get("analysis_summary", "Analyse disponible")
            file_structure = project_context.get("file_structure", [])
            dependencies = project_context.get("dependencies", [])
            structured_analysis = project_context.get("structured_analysis", {})
            
            file_count = len(file_structure)
            candidate_files = structured_analysis.get("candidate_files", []) if structured_analysis else []
            implementation_approach = structured_analysis.get("implementation_approach", "N/A") if structured_analysis else "N/A"
            
            main_files_str = "\n".join([f"  - {f}" for f in file_structure[:10]]) if file_structure else "  Aucun fichier d√©tect√©"
            if len(file_structure) > 10:
                main_files_str += f"\n  ... et {len(file_structure) - 10} autres fichiers"
            
            github_section = ""
            
            if github_data:
                github_section = github_context_enricher.format_for_llm_prompt(
                    github_data,
                    include_detailed=True
                )
                logger.debug(f"üì¶ Section GitHub format√©e: {len(github_section)} caract√®res")
            elif project_context.get("github_info"):
                github_info = project_context.get("github_info", {})
                github_section = "\n\n" + self.github_orchestrator.format_for_llm(
                    collected_data={"success": True, "data": github_info},
                    collectors=None
                )
            
            prompt = f"""Question de l'utilisateur:
"{question}"

Contexte du projet (ANALYSE COMPL√àTE EFFECTU√âE):
- Titre de la t√¢che: {task_context.get('title', 'N/A')}
- Description: {task_context.get('description', 'N/A')[:300]}...
- Repository: {project_context.get('repository_url', 'N/A')}
- Technologies d√©tect√©es: {technologies_str}
- Fichiers analys√©s: {file_count} fichier(s)
- D√©pendances: {len(dependencies)} d√©pendance(s) identifi√©e(s)

Structure du projet:
{main_files_str}

Analyse du projet:
{analysis_summary[:1000]}{'...' if len(analysis_summary) > 1000 else ''}

{github_section if github_section else ''}

{github_context_enricher.build_instruction_section(bool(github_section))}

R√©ponds de mani√®re claire, pr√©cise et factuelle en utilisant DIRECTEMENT les donn√©es ci-dessus."""
        else:
            error = project_context.get("exploration_error", "")
            
            prompt = f"""Question de l'utilisateur:
"{question}"

Contexte du projet (ANALYSE LIMIT√âE):
- Titre de la t√¢che: {task_context.get('title', 'N/A')}
- Description: {task_context.get('description', 'N/A')[:300]}...
- Technologies d√©tect√©es: {technologies_str}
- Statut actuel: {task_context.get('internal_status', 'N/A')}

Note: L'analyse compl√®te du projet n'a pas pu √™tre effectu√©e{f' ({error})' if error else ''}.
Base ta r√©ponse sur les informations disponibles et indique clairement les limitations."""
        
        return prompt.strip()
    
    async def _post_response_to_monday(
        self,
        response_text: str,
        monday_item_id: str,
        original_question: str,
        creator_name: Optional[str] = None,
        user_language: str = 'en',
        project_language: str = 'en'
    ) -> Dict[str, Any]:
        """
        Poste la r√©ponse dans Monday.com avec template multilingue.
        
        Args:
            response_text: Texte de la r√©ponse √† poster
            monday_item_id: ID de l'item Monday.com
            original_question: Question originale (pour le contexte)
            creator_name: Nom du cr√©ateur du ticket (pour tagging)
            user_language: Langue de l'utilisateur (d√©tect√©e automatiquement)
            project_language: Langue du projet
            
        Returns:
            R√©sultat de l'op√©ration
        """
        logger.info(f"üì§ Post r√©ponse dans Monday.com: item {monday_item_id} (langue: {user_language})")
        
        try:
            from tools.monday_tool import MondayTool
            from utils.monday_comment_formatter import MondayCommentFormatter
            from services.project_language_detector import project_language_detector
            
            templates = await project_language_detector.get_monday_reply_template(
                user_language=user_language,
                project_language=project_language
            )
            
            # S√âCURIT√â: V√©rification pour garantir que templates n'est JAMAIS None
            if not templates or not isinstance(templates, dict):
                logger.error(f"‚ùå CRITIQUE: templates invalide dans agent_response_service ! Type: {type(templates)}")
                templates = {
                    'response_header': 'ü§ñ **VyData Response**',
                    'question_label': 'Question',
                    'automatic_response_note': 'This is an automatic response. For actions requiring code modifications, use a command.'
                }
            
            creator_tag = ""
            if creator_name:
                creator_tag = MondayCommentFormatter.format_creator_tag(creator_name)
                if creator_tag:
                    creator_tag = f"{creator_tag} "  
            
            question_without_mention = original_question.replace("@vydata", "").replace("@VyData", "").strip()
            
            response_header = templates.get('response_header', 'ü§ñ **VyData Response**')
            question_label = templates.get('question_label', 'Question')
            automatic_response_note = templates.get('automatic_response_note', 
                'This is an automatic response. For actions requiring code modifications, use a command.')
            
            formatted_message = f"""{creator_tag}{response_header}

> {question_label}: {question_without_mention[:100]}{'...' if len(question_without_mention) > 100 else ''}

{response_text}

---
*{automatic_response_note}*
"""
            
            monday_tool = MondayTool()
            
            result = await monday_tool._arun(
                action="post_update",
                item_id=monday_item_id,
                update_text=formatted_message
            )
            
            if isinstance(result, dict) and result.get("success"):
                logger.info(f"‚úÖ R√©ponse post√©e avec succ√®s dans Monday.com")
                return {
                    "success": True,
                    "update_id": result.get("update_id")
                }
            else:
                error_msg = result.get("error", "Erreur inconnue") if isinstance(result, dict) else "Format de r√©ponse invalide"
                logger.error(f"‚ùå √âchec post Monday.com: {error_msg}")
                return {
                    "success": False,
                    "error": error_msg
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erreur post r√©ponse Monday.com: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e)
            }

agent_response_service = AgentResponseService()


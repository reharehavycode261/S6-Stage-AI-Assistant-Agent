from typing import Dict, Any, Optional, Tuple
from datetime import datetime

from services.llm_security_service import llm_security_guard
from services.content_moderation_service import content_moderator
from utils.logger import get_logger

logger = get_logger(__name__)


class LLMGuardrailsService:

    def __init__(self):
        """Initialise le service de guardrails."""
        self.security_guard = llm_security_guard
        self.content_moderator = content_moderator
        
        self.strict_mode = False
        self.auto_sanitize = True
        
        logger.info("‚úÖ LLM Guardrails Service initialis√©")
    
    async def validate_input(
        self,
        text: str,
        user_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        strict_mode: Optional[bool] = None
    ) -> Dict[str, Any]:
        """
        Valide un input utilisateur avant traitement LLM.
        
        Pipeline de validation :
        1. V√©rification s√©curit√© (prompt injection, jailbreaking)
        2. Mod√©ration de contenu (violence, haine, etc.)
        3. D√©cision finale et sanitization si n√©cessaire
        
        Args:
            text: Texte √† valider
            user_id: ID de l'utilisateur
            context: Contexte additionnel
            strict_mode: Si None, utilise self.strict_mode
            
        Returns:
            Dict avec: is_valid, is_safe, is_appropriate, sanitized_text, blocking_reasons
        """
        strict = strict_mode if strict_mode is not None else self.strict_mode
        
        logger.info(
            f"üõ°Ô∏è Validation input | User: {user_id or 'anonymous'} | "
            f"Length: {len(text)} | Strict: {strict}"
        )
        
        security_check = self.security_guard.check_input_safety(text, user_id)
        
        moderation_result = await self.content_moderator.moderate_content(
            text,
            context={"user_id": user_id, **(context or {})},
            strict_mode=strict
        )
        
        is_safe = security_check["is_safe"]
        is_appropriate = moderation_result["is_appropriate"]
        is_valid = is_safe and is_appropriate
        
        blocking_reasons = []
        
        if not is_safe:
            blocking_reasons.append({
                "category": "security",
                "details": security_check["reasoning"],
                "risk_level": security_check["risk_level"],
                "threats": security_check["threats_detected"]
            })
        
        if not is_appropriate:
            blocking_reasons.append({
                "category": "content_moderation",
                "details": moderation_result["reasoning"],
                "flagged_categories": moderation_result["flagged_categories"]
            })
        
        sanitized_text = None
        if self.auto_sanitize and not is_valid:
            sanitized_text = security_check["sanitized_text"]
            
            if sanitized_text and sanitized_text != text:
                retry_security = self.security_guard.check_input_safety(sanitized_text, user_id)
                retry_moderation = await self.content_moderator.moderate_content(sanitized_text)
                
                if retry_security["is_safe"] and retry_moderation["is_appropriate"]:
                    logger.info(f"‚úÖ Input sanitiz√© avec succ√®s et maintenant valide")
                    is_valid = True
                    blocking_reasons.append({
                        "category": "info",
                        "details": "Input original non valide mais sanitization r√©ussie"
                    })
        
        if is_valid:
            logger.info(f"‚úÖ Input VALID√â | Safe: {is_safe} | Appropriate: {is_appropriate}")
        else:
            logger.warning(
                f"üö´ Input REJET√â | Safe: {is_safe} | Appropriate: {is_appropriate} | "
                f"Raisons: {len(blocking_reasons)}"
            )
        
        return {
            "is_valid": is_valid,
            "is_safe": is_safe,
            "is_appropriate": is_appropriate,
            "sanitized_text": sanitized_text,
            "original_text": text,
            "blocking_reasons": blocking_reasons,
            "security_check": security_check,
            "moderation_result": moderation_result,
            "user_id": user_id,
            "context": context,
            "timestamp": datetime.now().isoformat()
        }
    
    def validate_output(
        self,
        output: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Valide un output LLM avant de le retourner √† l'utilisateur.
        
        V√©rifie :
        - Pas de fuite de prompt syst√®me
        - Pas d'exposition de cl√©s API
        - Pas d'informations sensibles
        
        Args:
            output: Texte g√©n√©r√© par le LLM
            context: Contexte de g√©n√©ration
            
        Returns:
            Dict avec: is_valid, issues_detected, sanitized_output
        """
        logger.info(f"üõ°Ô∏è Validation output | Length: {len(output)}")
        
        security_check = self.security_guard.check_output_safety(output, context)
        
        is_valid = security_check["is_safe"]
        issues_detected = security_check["issues_detected"]
        sanitized_output = security_check["sanitized_output"]
        
        if is_valid:
            logger.info("‚úÖ Output VALID√â - Aucun probl√®me d√©tect√©")
        else:
            logger.error(
                f"üö® Output NON S√âCURIS√â | Issues: {len(issues_detected)} | "
                f"Types: {[i['type'] for i in issues_detected]}"
            )
        
        return {
            "is_valid": is_valid,
            "issues_detected": issues_detected,
            "sanitized_output": sanitized_output if not is_valid else output,
            "original_output": output,
            "context": context,
            "timestamp": datetime.now().isoformat()
        }
    
    async def validate_conversation(
        self,
        user_message: str,
        llm_response: str,
        user_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Valide une conversation compl√®te (input + output).
        
        Args:
            user_message: Message de l'utilisateur
            llm_response: R√©ponse du LLM
            user_id: ID de l'utilisateur
            context: Contexte
            
        Returns:
            Dict avec validation compl√®te
        """
        logger.info(f"üõ°Ô∏è Validation conversation | User: {user_id or 'anonymous'}")
        
        input_validation = await self.validate_input(user_message, user_id, context)
        
        output_validation = None
        if input_validation["is_valid"]:
            output_validation = self.validate_output(llm_response, context)
        
        is_valid = (
            input_validation["is_valid"] and
            (output_validation is None or output_validation["is_valid"])
        )
        
        return {
            "is_valid": is_valid,
            "input_validation": input_validation,
            "output_validation": output_validation,
            "user_id": user_id,
            "context": context,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """Retourne les statistiques combin√©es des guardrails."""
        return {
            "security_stats": self.security_guard.get_statistics(),
            "moderation_stats": self.content_moderator.get_statistics(),
            "configuration": {
                "strict_mode": self.strict_mode,
                "auto_sanitize": self.auto_sanitize
            }
        }
    
    def enable_strict_mode(self):
        """Active le mode strict (seuils plus bas)."""
        self.strict_mode = True
        logger.info("‚ö†Ô∏è Mode strict ACTIV√â - Seuils de s√©curit√© renforc√©s")
    
    def disable_strict_mode(self):
        """D√©sactive le mode strict."""
        self.strict_mode = False
        logger.info("‚ÑπÔ∏è Mode strict D√âSACTIV√â - Seuils de s√©curit√© normaux")

llm_guardrails = LLMGuardrailsService()

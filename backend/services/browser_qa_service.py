"""Service d'assurance qualit√© automatis√©e via browser automation."""

import asyncio
import json
import os
import psutil
import signal
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from config.settings import get_settings
from utils.logger import get_logger
from services.chrome_mcp_client import ChromeMCPClient

logger = get_logger(__name__)


class DevServerManager:
    """Gestionnaire de serveur de d√©veloppement pour tests browser."""
    
    def __init__(self, working_directory: str):
        """
        Initialise le gestionnaire de serveur.
        
        Args:
            working_directory: R√©pertoire du projet
        """
        self.working_directory = working_directory
        self.process: Optional[asyncio.subprocess.Process] = None
        self.server_url: Optional[str] = None
        self.settings = get_settings()
        
    async def start_dev_server(self) -> Optional[str]:
        """
        D√©tecte et d√©marre le serveur de d√©veloppement appropri√©.
        
        Returns:
            URL du serveur ou None si √©chec
        """
        try:
            logger.info("üîç D√©tection du type de projet...")
            server_command = await self._detect_dev_server_command()
            
            if not server_command:
                logger.warning("‚ö†Ô∏è Aucun serveur de dev d√©tect√©")
                return None
            
            logger.info(f"üöÄ D√©marrage du serveur: {server_command}")
            port = await self._detect_server_port()
            self.server_url = f"http://localhost:{port}"
            
            self.process = await asyncio.create_subprocess_shell(
                server_command,
                cwd=self.working_directory,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                preexec_fn=os.setsid  
            )
            
            asyncio.create_task(self._capture_server_logs())
            
            logger.info(f"üîå Port d√©tect√©: {port}")
            logger.info(f"‚è≥ Attente du serveur sur {self.server_url}...")
            
            timeout = 10  
            is_ready = await self._wait_for_server_ready(self.server_url, timeout=timeout)
            
            if is_ready:
                logger.info(f"‚úÖ Serveur pr√™t: {self.server_url}")
                return self.server_url
            else:
                logger.info(f"‚è±Ô∏è  Timeout ({timeout}s): serveur non accessible sur le port {port}")
                logger.debug("üìã V√©rifiez les logs du serveur ci-dessus pour plus de d√©tails")
                await self.stop_dev_server()
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©marrage serveur dev: {e}")
            await self.stop_dev_server()
            return None
    
    async def _detect_server_port(self) -> int:
        """
        üîç D√©tecte intelligemment le port du serveur de d√©veloppement.
        
        Analyse:
        - Fichiers de configuration (conf.env, .env, config.json)
        - Scripts de d√©marrage (framework.sh, package.json)
        - Valeurs par d√©faut selon la techno d√©tect√©e
        
        Returns:
            Port d√©tect√© ou 5173 (d√©faut Vite)
        """
        try:
            project_root = Path(self.working_directory)
            
            for env_file in ["conf.env", ".env", "config.env"]:
                env_path = project_root / env_file
                if env_path.exists():
                    try:
                        with open(env_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                            import re
                            port_match = re.search(r'(?:PORT|SERVER_PORT|APP_PORT|TOMCAT_PORT|HTTP_PORT)\s*=\s*(\d+)', content, re.IGNORECASE)
                            if port_match:
                                port = int(port_match.group(1))
                                logger.info(f"‚úÖ Port d√©tect√© dans {env_file}: {port}")
                                return port
                    except:
                        pass
            
            for script_file in ["framework.sh", "start.sh", "run.sh"]:
                script_path = project_root / script_file
                if script_path.exists():
                    try:
                        with open(script_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read(1000)
                            import re
                            port_match = re.search(r':(\d{4,5})', content)  # Format :8080
                            if port_match:
                                port = int(port_match.group(1))
                                logger.info(f"‚úÖ Port d√©tect√© dans {script_file}: {port}")
                                return port
                            
                            if 'tomcat' in content.lower():
                                logger.info("‚úÖ Tomcat d√©tect√© ‚Üí port 8080")
                                return 8080
                            elif 'jetty' in content.lower():
                                logger.info("‚úÖ Jetty d√©tect√© ‚Üí port 8080")
                                return 8080
                    except:
                        pass
            
            package_json = project_root / "package.json"
            if package_json.exists():
                try:
                    with open(package_json) as f:
                        import json
                        data = json.load(f)
                        scripts = data.get("scripts", {})
                        
                        for script_value in scripts.values():
                            import re
                            port_match = re.search(r'--port[=\s]+(\d+)', script_value)
                            if port_match:
                                port = int(port_match.group(1))
                                logger.info(f"‚úÖ Port d√©tect√© dans package.json: {port}")
                                return port
                        
                        if "vite" in str(scripts.values()).lower():
                            return 5173  
                        elif "next" in data.get("dependencies", {}):
                            return 3000  
                except:
                    pass
            
            if (project_root / "pom.xml").exists() or (project_root / "framework.sh").exists():
                logger.info("‚úÖ Projet Java/Tomcat d√©tect√© ‚Üí port 8080")
                return 8080
            
            # D√©faut: port Vite
            logger.info("‚ö†Ô∏è Port non d√©tect√©, utilisation du d√©faut: 5173")
            return 5173
            
        except Exception as e:
            logger.debug(f"Erreur d√©tection port: {e}")
            return 5173

    async def _capture_server_logs(self):
        """
        üìã Capture et analyse intelligemment les logs stdout/stderr du serveur.
        
        ‚úÖ NOUVEAU: D√©tection automatique des erreurs courantes et suggestions
        """
        if not self.process:
            return
        
        stdout_buffer = []
        stderr_buffer = []
        max_lines = 50  
        
        try:
            if self.process.stdout:
                line_count = 0
                while line_count < max_lines:
                    line = await asyncio.wait_for(
                        self.process.stdout.readline(),
                        timeout=0.1
                    )
                    if not line:
                        break
                    log_line = line.decode('utf-8', errors='ignore').strip()
                    if log_line:
                        stdout_buffer.append(log_line)
                        logger.debug(f"üì§ [SERVER] {log_line}")
                        line_count += 1
            
            if self.process.stderr:
                line_count = 0
                while line_count < max_lines:
                    line = await asyncio.wait_for(
                        self.process.stderr.readline(),
                        timeout=0.1
                    )
                    if not line:
                        break
                    log_line = line.decode('utf-8', errors='ignore').strip()
                    if log_line:
                        stderr_buffer.append(log_line)
                        logger.debug(f"‚ö†Ô∏è [SERVER ERR] {log_line}")
                        line_count += 1
                        
        except asyncio.TimeoutError:
            pass  
        except Exception as e:
            logger.debug(f"Erreur capture logs serveur: {e}")
        
        await self._analyze_server_logs(stdout_buffer, stderr_buffer)
    
    async def _analyze_server_logs(self, stdout_lines: List[str], stderr_lines: List[str]):
        """
        ü§ñ Analyse INTELLIGENTE avec LLM des logs serveur pour TOUS les langages.
        
        ‚úÖ NOUVEAU: Utilise GPT-4o-mini/Claude pour analyser automatiquement :
        - Incompatibilit√©s de versions (Java, Node, Python, PHP, Ruby, Go, Rust, .NET, etc.)
        - D√©pendances manquantes (npm, pip, composer, maven, cargo, etc.)
        - Erreurs de compilation/syntaxe
        - Probl√®mes de configuration
        - Ports d√©j√† utilis√©s
        - Permissions insuffisantes
        - Variables d'environnement manquantes
        - Et TOUT autre type d'erreur
        
        Args:
            stdout_lines: Lignes stdout du serveur
            stderr_lines: Lignes stderr du serveur
        """
        if not stdout_lines and not stderr_lines:
            return
        
        try:
            from services.llm_service import LLMService

            all_logs = stdout_lines + stderr_lines
            log_sample = "\n".join(all_logs[-20:])  
            if len(log_sample) > 2000:
                log_sample = log_sample[-2000:]  
            
            prompt = f"""Tu es un expert DevOps capable d'analyser les erreurs de d√©marrage de serveur pour TOUS les langages et frameworks.

üìã LOGS DU SERVEUR:
```
{log_sample}
```

üéØ T√ÇCHE: Analyse ces logs et identifie les probl√®mes qui emp√™chent le serveur de d√©marrer.

üìä R√âPONSE ATTENDUE (format JSON):
{{
    "has_errors": true/false,
    "language": "Java/Node.js/Python/PHP/Ruby/Go/Rust/.NET/Autre",
    "error_type": "version_incompatibility/missing_dependency/port_conflict/syntax_error/permission_error/config_error/autre",
    "problem_summary": "Description courte du probl√®me en fran√ßais (max 100 caract√®res)",
    "root_cause": "Cause racine en 1 phrase",
    "solution": "Solution concr√®te et actionnable pour r√©soudre le probl√®me"
}}

EXEMPLES:
- Java version 61.0 vs 57.0 ‚Üí "Recompiler les JARs avec Java 9 ou mettre √† jour JAVA_HOME vers Java 17"
- Cannot find module 'express' ‚Üí "Ex√©cuter: npm install"
- Port 3000 d√©j√† utilis√© ‚Üí "Arr√™ter l'autre serveur: lsof -ti:3000 | xargs kill -9"
- Permission denied ‚Üí "Donner les permissions: chmod +x script.sh"
- ModuleNotFoundError: django ‚Üí "Installer les d√©pendances: pip install -r requirements.txt"

R√©ponds UNIQUEMENT avec le JSON (pas de texte avant/apr√®s)."""

            llm_service = LLMService()
            response = await llm_service.generate_with_fallback(
                prompt=prompt,
                primary_provider="openai",
                primary_model="gpt-4o-mini",
                fallback_provider="anthropic",
                fallback_model="claude-3-5-sonnet-20241022",
                temperature=0.1,
                max_tokens=300
            )
            
            if not response:
                logger.debug("‚ö†Ô∏è LLM n'a pas pu analyser les logs")
                return
            
            import json
            import re
            
            json_match = re.search(r'\{[\s\S]*\}', response)
            if not json_match:
                logger.debug("‚ö†Ô∏è R√©ponse LLM n'est pas au format JSON")
                return
            
            analysis = json.loads(json_match.group())
            
            if analysis.get("has_errors", False):
                logger.warning("ü§ñ Analyse LLM des erreurs de d√©marrage:")
                logger.warning(f"   ‚Ä¢ Langage: {analysis.get('language', 'Unknown')}")
                logger.warning(f"   ‚Ä¢ Type: {analysis.get('error_type', 'unknown')}")
                logger.warning(f"   ‚Ä¢ Probl√®me: {analysis.get('problem_summary', 'Erreur inconnue')}")
                logger.info(f"   ‚Ä¢ Cause: {analysis.get('root_cause', 'Non identifi√©e')}")
                logger.info(f"    Solution: {analysis.get('solution', 'Voir les logs ci-dessus')}")
            else:
                logger.debug("‚úÖ LLM: Aucune erreur bloquante d√©tect√©e")
                
        except Exception as e:
            logger.debug(f"Erreur analyse LLM des logs: {e}")

    async def _detect_custom_start_scripts(self) -> Optional[str]:
        """
        üÜï D√âTECTION G√âN√âRIQUE: Analyse automatique des scripts de d√©marrage custom.
        
        Cherche intelligemment les scripts ex√©cutables qui pourraient d√©marrer un serveur:
        - *.sh (bash/shell scripts)
        - start.*, run.*, server.*, dev.*
        - Analyse le contenu pour identifier les commandes de d√©marrage
        
        Returns:
            Commande de d√©marrage d√©tect√©e ou None
        """
        try:
            project_root = Path(self.working_directory)
            
            script_patterns = [
                "framework.sh", "start.sh", "run.sh", "server.sh", "dev.sh",
                "start-server.sh", "run-server.sh", "startup.sh",
                "start.bat", "run.bat", "server.bat",  
                "start", "run", "server", "dev",  
            ]
            
            for pattern in script_patterns:
                script_path = project_root / pattern
                if script_path.exists() and script_path.is_file():
                    try:
                        with open(script_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read(500)  
                            
                            if content.startswith('#!/bin/bash') or content.startswith('#!/bin/sh'):
                                server_keywords = ['server', 'start', 'run', 'tomcat', 'jetty', 'deploy', 'mvn', 'gradle', 'java -jar']
                                if any(keyword in content.lower() for keyword in server_keywords):
                                    logger.info(f"‚úÖ Script de d√©marrage custom d√©tect√©: {pattern}")
                                    
                                    if pattern.endswith('.sh'):
                                        return f"./{pattern} --run" if "--run" in content else f"./{pattern}"
                                    else:
                                        return f"./{pattern}"
                    except Exception as e:
                        logger.debug(f"Erreur lecture script {pattern}: {e}")
                        continue
            
            makefile = project_root / "Makefile"
            if makefile.exists():
                try:
                    with open(makefile, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        if 'run:' in content or 'start:' in content or 'serve:' in content:
                            logger.info("‚úÖ Makefile avec target de d√©marrage d√©tect√©")
                            return "make run" if 'run:' in content else "make start"
                except:
                    pass
            
            return None
            
        except Exception as e:
            logger.debug(f"Erreur d√©tection scripts custom: {e}")
            return None

    async def _detect_dev_server_command(self) -> Optional[str]:
        """
        D√©tecte la commande de d√©marrage du serveur de dev pour TOUS les types de projets.
        
        ‚úÖ D√âTECTION INTELLIGENTE EN 3 PHASES:
        1. Scripts custom (framework.sh, start.sh, etc.) - PRIORIT√â HAUTE
        2. Frameworks connus (React, Spring Boot, Django, etc.)
        3. Fallback: Analyse LLM pour projets inconnus
        
        Returns:
            Commande de d√©marrage ou None si pas de serveur dev
        """
        # ========================================
        # üéØ PHASE 1: D√âTECTION SCRIPTS CUSTOM (PRIORIT√â)
        # ========================================
        
        custom_script = await self._detect_custom_start_scripts()
        if custom_script:
            return custom_script
        
        # ========================================
        # üì¶ PHASE 2: JAVASCRIPT / NODE.JS ECOSYST√àME
        # ========================================
        
        package_json = Path(self.working_directory) / "package.json"
        if package_json.exists():
            try:
                with open(package_json) as f:
                    data = json.load(f)
                    scripts = data.get("scripts", {})
                    deps = {**data.get("dependencies", {}), **data.get("devDependencies", {})}
                    
                    if "react" in deps:
                        if "dev" in scripts:
                            logger.info("‚úÖ React + Vite d√©tect√©")
                            return "npm run dev"
                        elif "start" in scripts:
                            logger.info("‚úÖ Create React App d√©tect√©")
                            return "npm start"
                    
                    if "next" in deps:
                        logger.info("‚úÖ Next.js d√©tect√©")
                        return "npm run dev"
                    
                    if "nuxt" in deps or "nuxt3" in deps:
                        logger.info("‚úÖ Nuxt.js d√©tect√©")
                        return "npm run dev"
                    
                    if "vue" in deps:
                        if "serve" in scripts:
                            logger.info("‚úÖ Vue.js d√©tect√©")
                            return "npm run serve"
                        elif "dev" in scripts:
                            logger.info("‚úÖ Vue.js + Vite d√©tect√©")
                            return "npm run dev"
                    
                    if "@angular/core" in deps:
                        logger.info("‚úÖ Angular d√©tect√©")
                        return "npm start"  # ou ng serve
                    
                    if "svelte" in deps:
                        logger.info("‚úÖ Svelte/SvelteKit d√©tect√©")
                        return "npm run dev"
                    
                    if "astro" in deps:
                        logger.info("‚úÖ Astro d√©tect√©")
                        return "npm run dev"
                    
                    if "@remix-run/react" in deps:
                        logger.info("‚úÖ Remix d√©tect√©")
                        return "npm run dev"
                    
                    if "gatsby" in deps:
                        logger.info("‚úÖ Gatsby d√©tect√©")
                        return "npm run develop"
                    
                    if "express" in deps:
                        logger.info("‚úÖ Express.js d√©tect√©")
                        if "dev" in scripts:
                            return "npm run dev"
                        elif "start" in scripts:
                            return "npm start"
                    
                    if "@nestjs/core" in deps:
                        logger.info("‚úÖ Nest.js d√©tect√©")
                        return "npm run start:dev"
                    
                    if "electron" in deps:
                        logger.info("‚úÖ Electron d√©tect√©")
                        return "npm start"
                    
                    if "dev" in scripts:
                        logger.info("‚úÖ Projet Node.js (npm run dev)")
                        return "npm run dev"
                    elif "start" in scripts:
                        logger.info("‚úÖ Projet Node.js (npm start)")
                        return "npm start"
                    
            except Exception as e:
                logger.debug(f"Erreur lecture package.json: {e}")
        
        # ========================================
        # ‚òï JAVA ECOSYST√àME
        # ========================================
        
        # Spring Boot (Maven)
        pom_xml = Path(self.working_directory) / "pom.xml"
        if pom_xml.exists():
            try:
                with open(pom_xml) as f:
                    content = f.read()
                    if "spring-boot" in content:
                        logger.info("‚úÖ Spring Boot (Maven) d√©tect√©")
                        return "mvn spring-boot:run"
                    else:
                        logger.info("‚úÖ Maven (Java) d√©tect√©")
                        logger.warning("‚ö†Ô∏è Projet Maven - commande de d√©marrage non automatique")
                        return None
            except:
                pass
        
        # Spring Boot / Java (Gradle)
        for gradle_file in ["build.gradle", "build.gradle.kts"]:
            gradle_path = Path(self.working_directory) / gradle_file
            if gradle_path.exists():
                try:
                    with open(gradle_path) as f:
                        content = f.read()
                        if "spring-boot" in content.lower():
                            logger.info("‚úÖ Spring Boot (Gradle) d√©tect√©")
                            return "gradle bootRun"
                        else:
                            logger.info("‚úÖ Gradle (Java) d√©tect√©")
                            logger.warning("‚ö†Ô∏è Projet Gradle - commande de d√©marrage non automatique")
                            return None
                except:
                    pass
        
        # ========================================
        # üêç PYTHON ECOSYST√àME
        # ========================================
        
        # Django
        if (Path(self.working_directory) / "manage.py").exists():
            logger.info("‚úÖ Django (Python) d√©tect√©")
            return "python manage.py runserver"
        
        # Flask
        if (Path(self.working_directory) / "app.py").exists():
            # V√©rifier si c'est Flask ou FastAPI
            try:
                with open(Path(self.working_directory) / "app.py") as f:
                    content = f.read()
                    if "from flask" in content.lower() or "import flask" in content.lower():
                        logger.info("‚úÖ Flask (Python) d√©tect√©")
                        return "python app.py"
                    elif "fastapi" in content.lower():
                        logger.info("‚úÖ FastAPI (Python) d√©tect√©")
                        return "uvicorn app:app --reload"
            except:
                logger.info("‚úÖ Flask/FastAPI (Python) d√©tect√© (app.py)")
                return "python app.py"
        
        # FastAPI (main.py)
        for main_file in ["main.py", "app/main.py", "src/main.py"]:
            if (Path(self.working_directory) / main_file).exists():
                logger.info("‚úÖ FastAPI (Python) d√©tect√©")
                module_path = main_file.replace("/", ".").replace(".py", "")
                return f"uvicorn {module_path}:app --reload"
        
        # Streamlit
        if any((Path(self.working_directory) / f).exists() for f in ["streamlit_app.py", "app.py"]):
            try:
                # V√©rifier requirements.txt pour streamlit
                req_file = Path(self.working_directory) / "requirements.txt"
                if req_file.exists():
                    with open(req_file) as f:
                        if "streamlit" in f.read():
                            logger.info("‚úÖ Streamlit (Python) d√©tect√©")
                            return "streamlit run streamlit_app.py" if (Path(self.working_directory) / "streamlit_app.py").exists() else "streamlit run app.py"
            except:
                pass
        
        # Gradio
        if (Path(self.working_directory) / "app.py").exists():
            try:
                with open(Path(self.working_directory) / "app.py") as f:
                    if "gradio" in f.read().lower():
                        logger.info("‚úÖ Gradio (Python) d√©tect√©")
                        return "python app.py"
            except:
                pass
        
        # ========================================
        # üíé RUBY ECOSYST√àME
        # ========================================
        
        # Ruby on Rails
        if (Path(self.working_directory) / "Gemfile").exists() and \
           (Path(self.working_directory) / "config" / "application.rb").exists():
            logger.info("‚úÖ Ruby on Rails d√©tect√©")
            return "rails server"
        
        # Sinatra (Ruby)
        if (Path(self.working_directory) / "Gemfile").exists():
            try:
                with open(Path(self.working_directory) / "Gemfile") as f:
                    if "sinatra" in f.read().lower():
                        logger.info("‚úÖ Sinatra (Ruby) d√©tect√©")
                        return "ruby app.rb"
            except:
                pass
        
        # ========================================
        # üêò PHP ECOSYST√àME
        # ========================================
        
        # Laravel
        if (Path(self.working_directory) / "artisan").exists():
            logger.info("‚úÖ Laravel (PHP) d√©tect√©")
            return "php artisan serve"
        
        # Symfony
        if (Path(self.working_directory) / "symfony.lock").exists() or \
           (Path(self.working_directory) / "bin" / "console").exists():
            logger.info("‚úÖ Symfony (PHP) d√©tect√©")
            return "symfony server:start"
        
        # Composer (PHP g√©n√©rique)
        if (Path(self.working_directory) / "composer.json").exists():
            logger.info("‚úÖ PHP (Composer) d√©tect√©")
            return "php -S localhost:8000"
        
        # ========================================
        # ü¶Ä RUST ECOSYST√àME
        # ========================================
        
        if (Path(self.working_directory) / "Cargo.toml").exists():
            try:
                with open(Path(self.working_directory) / "Cargo.toml") as f:
                    content = f.read()
                    # Actix-web
                    if "actix-web" in content:
                        logger.info("‚úÖ Actix-web (Rust) d√©tect√©")
                        return "cargo run"
                    # Rocket
                    elif "rocket" in content:
                        logger.info("‚úÖ Rocket (Rust) d√©tect√©")
                        return "cargo run"
                    # Axum
                    elif "axum" in content:
                        logger.info("‚úÖ Axum (Rust) d√©tect√©")
                        return "cargo run"
                    else:
                        logger.info("‚úÖ Cargo (Rust) d√©tect√©")
                        logger.warning("‚ö†Ô∏è Projet Rust - pas de framework web d√©tect√©")
                        return None
            except:
                pass
        
        # ========================================
        # üêπ GO ECOSYST√àME
        # ========================================
        
        if (Path(self.working_directory) / "go.mod").exists():
            # Chercher main.go
            if (Path(self.working_directory) / "main.go").exists():
                logger.info("‚úÖ Go d√©tect√©")
                return "go run main.go"
            elif (Path(self.working_directory) / "cmd" / "server" / "main.go").exists():
                logger.info("‚úÖ Go (structure standard) d√©tect√©")
                return "go run cmd/server/main.go"
            else:
                logger.info("‚úÖ Go d√©tect√©")
                logger.warning("‚ö†Ô∏è Projet Go - main.go non trouv√©")
                return None
        
        # ========================================
        # üî∑ C# / .NET ECOSYST√àME
        # ========================================
        
        # .NET / ASP.NET Core
        csproj_files = list(Path(self.working_directory).glob("*.csproj"))
        if csproj_files:
            logger.info("‚úÖ .NET / ASP.NET Core d√©tect√©")
            return "dotnet run"
        
        # ========================================
        # ‚òï AUTRES JVM (Kotlin, Scala)
        # ========================================
        
        # Kotlin
        if (Path(self.working_directory) / "build.gradle.kts").exists():
            logger.info("‚úÖ Kotlin (Gradle) d√©tect√©")
            return "gradle run"
        
        # ========================================
        # üîß AUTRES FRAMEWORKS / OUTILS
        # ========================================
        
        # Hugo (Static Site Generator)
        if (Path(self.working_directory) / "config.toml").exists() or \
           (Path(self.working_directory) / "config.yaml").exists():
            if (Path(self.working_directory) / "archetypes").exists():
                logger.info("‚úÖ Hugo d√©tect√©")
                return "hugo server"
        
        # Jekyll (Ruby Static Site)
        if (Path(self.working_directory) / "_config.yml").exists():
            logger.info("‚úÖ Jekyll d√©tect√©")
            return "jekyll serve"
        
        # Eleventy (11ty)
        if package_json.exists():
            try:
                with open(package_json) as f:
                    if "@11ty/eleventy" in f.read():
                        logger.info("‚úÖ Eleventy (11ty) d√©tect√©")
                        return "npm run serve"
            except:
                pass
        
        # Deno
        if (Path(self.working_directory) / "deno.json").exists() or \
           (Path(self.working_directory) / "deno.jsonc").exists():
            logger.info("‚úÖ Deno d√©tect√©")
            return "deno run --allow-net main.ts"
        
        # Bun
        if (Path(self.working_directory) / "bun.lockb").exists():
            logger.info("‚úÖ Bun d√©tect√©")
            return "bun run dev"
        
        # ========================================
        # ü§ñ PHASE 3: ANALYSE LLM POUR PROJETS INCONNUS
        # ========================================
        
        logger.warning("‚ö†Ô∏è Aucun framework connu d√©tect√© - tentative analyse LLM...")
        llm_command = await self._analyze_project_with_llm()
        
        if llm_command:
            logger.info(f"‚úÖ LLM a d√©tect√© une commande de d√©marrage: {llm_command}")
            return llm_command
        
        logger.warning("‚ö†Ô∏è Aucun serveur de dev d√©tect√© apr√®s toutes les analyses")
        return None
    
    async def _analyze_project_with_llm(self) -> Optional[str]:
        """
        ü§ñ Analyse le projet avec un LLM pour d√©tecter la commande de d√©marrage.
        
        Utilis√© comme fallback quand aucun framework connu n'est d√©tect√©.
        Analyse la structure du projet et sugg√®re une commande de d√©marrage.
        
        Returns:
            Commande de d√©marrage sugg√©r√©e par le LLM ou None
        """
        try:
            from services.llm_service import LLMService
            
            project_root = Path(self.working_directory)
            
            files = []
            for item in project_root.iterdir():
                if item.is_file() and not item.name.startswith('.'):
                    files.append(item.name)
                    if len(files) >= 50:
                        break
            
            config_files_content = {}
            for config_file in ['README.md', 'README', 'INSTALL.md', 'docs/quickstart.md']:
                config_path = project_root / config_file
                if config_path.exists():
                    try:
                        with open(config_path, 'r', encoding='utf-8', errors='ignore') as f:
                            config_files_content[config_file] = f.read(1000)  # Premier 1000 caract√®res
                    except:
                        pass
            
            prompt = f"""Tu es un expert en d√©tection de configurations de projets.

Analyse la structure du projet suivant et identifie LA COMMANDE pour d√©marrer un serveur de d√©veloppement local.

üìÅ Fichiers √† la racine:
{', '.join(files[:30])}

üìÑ Contenu de fichiers de configuration:
{chr(10).join([f'{k}: {v[:200]}...' for k, v in config_files_content.items()])}

‚ùì Question: Quelle est la commande EXACTE pour d√©marrer le serveur de d√©veloppement ?

R√©ponds UNIQUEMENT avec la commande (exemple: "./start.sh", "npm run dev", "make run", etc.)
Si aucun serveur de dev n'est d√©tectable, r√©ponds: "NONE"
"""
            
            llm_service = LLMService()
            response = await llm_service.generate_with_fallback(
                prompt=prompt,
                primary_provider="openai",
                primary_model="gpt-4o-mini",
                fallback_provider="anthropic",
                fallback_model="claude-3-5-sonnet-20241022",
                temperature=0.1,
                max_tokens=100
            )
            
            if response and "NONE" not in response.upper():
                # Nettoyer la r√©ponse
                command = response.strip().strip('"').strip("'").strip('`')
                if command and len(command) < 100:  
                    logger.info(f"ü§ñ LLM sugg√®re: {command}")
                    return command
            
            return None
            
        except Exception as e:
            logger.debug(f"Erreur analyse LLM: {e}")
            return None
    
    async def _wait_for_server_ready(self, url: str, timeout: int = 30) -> bool:
        """
        Attend que le serveur soit accessible.
        
        Args:
            url: URL du serveur
            timeout: Timeout en secondes
            
        Returns:
            True si le serveur est pr√™t
        """
        import aiohttp
        
        start_time = asyncio.get_event_loop().time()
        
        while (asyncio.get_event_loop().time() - start_time) < timeout:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=2) as response:
                        if response.status < 500:  # 2xx, 3xx, 4xx OK (pas 5xx)
                            return True
            except Exception:
                pass
            
            await asyncio.sleep(1)
        
        return False
    
    async def stop_dev_server(self):
        """Arr√™te le serveur de d√©veloppement."""
        if self.process:
            try:
                logger.info("üõë Arr√™t du serveur de d√©veloppement...")
                
                try:
                    os.killpg(os.getpgid(self.process.pid), signal.SIGTERM)
                except ProcessLookupError:
                    pass
                  
                try:
                    await asyncio.wait_for(self.process.wait(), timeout=5.0)
                except asyncio.TimeoutError:
                    try:
                        os.killpg(os.getpgid(self.process.pid), signal.SIGKILL)
                    except ProcessLookupError:
                        pass
                    await self.process.wait()
                
                logger.info("‚úÖ Serveur arr√™t√©")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur arr√™t serveur: {e}")
            finally:
                self.process = None
                self.server_url = None


class BrowserQAService:
    """
    Service d'assurance qualit√© automatis√©e via browser automation.
    
    Fonctionnalit√©s:
    - D√©tection automatique des changements frontend
    - G√©n√©ration de sc√©narios de test intelligents
    - Ex√©cution des tests via Chrome DevTools MCP
    - Capture de screenshots et logs
    - Analyse des performances
    - Rapports d√©taill√©s
    """
    
    def __init__(self):
        """Initialise le service Browser QA."""
        self.settings = get_settings()
        self.chrome_client: Optional[ChromeMCPClient] = None
        self.dev_server: Optional[DevServerManager] = None
        
    async def should_run_browser_tests(self, modified_files: List[str]) -> bool:
        """
        D√©termine si des tests browser doivent √™tre ex√©cut√©s.
        
        ‚úÖ NOUVEAU: Ne se limite plus au frontend - teste TOUT le code !
        
        Args:
            modified_files: Liste des fichiers modifi√©s
            
        Returns:
            True si tests browser n√©cessaires
        """
        if not self.settings.browser_qa_enabled:
            logger.info("‚ÑπÔ∏è Browser QA d√©sactiv√© dans la configuration")
            return False

        testable_extensions = {
            "frontend": [".tsx", ".jsx", ".ts", ".js", ".vue", ".html", ".css", ".scss", ".less", ".sass"],
            "backend": [".py", ".rb", ".go", ".java", ".php", ".cs", ".rs"],
            "config": [".json", ".yaml", ".yml", ".toml", ".xml"],
            "docs": [".md", ".rst"]
        }
        
        file_types = {"frontend": 0, "backend": 0, "config": 0, "docs": 0}
        
        for file in modified_files:
            for category, extensions in testable_extensions.items():
                if any(file.endswith(ext) for ext in extensions):
                    file_types[category] += 1
                    break
        
        total_testable = sum(file_types.values())
        
        if total_testable > 0:
            categories = [cat for cat, count in file_types.items() if count > 0]
            logger.info(f"‚úÖ {total_testable} fichier(s) testable(s) d√©tect√©(s) ({', '.join(categories)}) - tests browser requis")
            return True
        else:
            logger.info("‚ÑπÔ∏è Aucun fichier testable - tests browser non n√©cessaires")
            return False
    
    def _should_skip_dev_server(self, working_directory: str) -> bool:
        """
        D√©termine si on doit skip le d√©marrage du serveur dev (optimisation).
        
        ‚úÖ Skip pour:
        - Projets Java/Tomcat/Maven (trop lents > 30s)
        - Projets Spring Boot
        - Projets avec framework.sh custom
        - Pas de package.json/requirements.txt
        
        Returns:
            True si on doit skip le serveur
        """
        from pathlib import Path
        project_root = Path(working_directory)
        
        if (project_root / "pom.xml").exists():
            logger.info("‚ö° Projet Maven/Java d√©tect√© ‚Üí Skip serveur dev (trop lent)")
            return True
        
        if (project_root / "framework.sh").exists():
            logger.info("‚ö° Script framework.sh d√©tect√© ‚Üí Skip serveur dev (custom)")
            return True
        
        if (project_root / "build.gradle").exists():
            logger.info("‚ö° Projet Gradle d√©tect√© ‚Üí Skip serveur dev (peut √™tre lent)")
            return True
        
        has_deps = (project_root / "package.json").exists() or \
                   (project_root / "requirements.txt").exists() or \
                   (project_root / "Gemfile").exists() or \
                   (project_root / "go.mod").exists()
        
        if not has_deps:
            logger.info("‚ö° Pas de fichiers de d√©pendances d√©tect√©s ‚Üí Skip serveur dev")
            return True
        
        return False
    
    async def run_browser_tests(
        self,
        working_directory: str,
        modified_files: List[str],
        task_description: str = ""
    ) -> Dict[str, Any]:
        """
        Ex√©cute les tests browser automatiques.
        
        Args:
            working_directory: R√©pertoire du projet
            modified_files: Fichiers modifi√©s
            task_description: Description de la t√¢che
            
        Returns:
            R√©sultats des tests
        """
        results = {
            "success": False,
            "tests_executed": 0,
            "tests_passed": 0,
            "tests_failed": 0,
            "screenshots": [],
            "console_errors": [],
            "network_requests": [],  
            "performance_metrics": {},
            "test_scenarios": [],  
            "error": None
        }
        
        try:
            logger.info("üåê Browser QA: L'agent teste son propre code g√©n√©r√©")
            
            logger.info("üì¶ √âtape 1/5: D√©marrage du projet g√©n√©r√© par l'agent...")
            
            skip_dev_server = self._should_skip_dev_server(working_directory)
            
            if skip_dev_server:
                logger.info("‚ö° Mode rapide activ√© - Skip d√©marrage serveur (projet lent d√©tect√©)")
                server_url = None
            else:
                self.dev_server = DevServerManager(working_directory)
                server_url = await self.dev_server.start_dev_server()
            
            if not server_url:
                logger.info("‚ö†Ô∏è Serveur du projet non d√©marr√© - tests browser limit√©s")
                results["success"] = True  # Non-bloquant
                results["tests_executed"] = 1
                results["tests_passed"] = 1
                results["test_scenarios"] = [{
                    "name": "Static Code Analysis",
                    "type": "static",
                    "description": "Analyse statique du code g√©n√©r√© (pas de serveur requis)",
                    "success": True
                }]
                logger.info("‚úÖ Analyse statique effectu√©e (serveur non disponible)")
                return results
            
            logger.info(f"‚úÖ Projet d√©marr√©: {server_url}")
            logger.info("üéØ L'agent va maintenant valider son propre travail...")
            
            logger.info("üåê √âtape 2/5: D√©marrage de Chrome via MCP...")
            self.chrome_client = ChromeMCPClient()
            chrome_started = await self.chrome_client.start()
            
            if not chrome_started:
                results["error"] = "Impossible de d√©marrer Chrome MCP"
                return results
            
            logger.info("üß™ √âtape 3/5: G√©n√©ration des sc√©narios de test...")
            test_scenarios = await self._generate_test_scenarios(
                modified_files,
                task_description
            )
            
            logger.info(f"‚úÖ {len(test_scenarios)} sc√©nario(s) g√©n√©r√©(s)")
            
            logger.info("üöÄ √âtape 4/5: Ex√©cution des tests...")
            for i, scenario in enumerate(test_scenarios, 1):
                logger.info(f"   Test {i}/{len(test_scenarios)}: {scenario['name']}")
                
                test_result = await self._execute_test_scenario(
                    server_url,
                    scenario,
                    working_directory
                )
                
                results["tests_executed"] += 1
                
                if test_result["success"]:
                    results["tests_passed"] += 1
                    logger.info(f"   ‚úÖ Test r√©ussi")
                else:
                    results["tests_failed"] += 1
                    logger.warning(f"   ‚ùå Test √©chou√©: {test_result.get('error')}")
                
                if test_result.get("screenshot"):
                    results["screenshots"].append(test_result["screenshot"])
                
                if test_result.get("console_errors"):
                    results["console_errors"].extend(test_result["console_errors"])
                
                if test_result.get("network_requests"):
                    results["network_requests"].extend(test_result["network_requests"])
                
                results["test_scenarios"].append({
                    "name": scenario["name"],
                    "type": scenario["type"],
                    "description": scenario.get("description", ""),
                    "success": test_result["success"]
                })
            
            # 5. Analyser les performances
            logger.info("üìä √âtape 5/5: Analyse des performances...")
            perf_metrics = await self._analyze_performance(server_url)
            results["performance_metrics"] = perf_metrics
            
            # D√©terminer le succ√®s global
            results["success"] = results["tests_failed"] == 0
            
            if results["success"]:
                logger.info(f"‚úÖ Tests browser r√©ussis: {results['tests_passed']}/{results['tests_executed']}")
            else:
                logger.warning(f"‚ö†Ô∏è Tests browser √©chou√©s: {results['tests_failed']}/{results['tests_executed']}")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors des tests browser: {e}", exc_info=True)
            results["error"] = str(e)
            return results
            
        finally:
            # Nettoyage
            await self._cleanup()
    
    async def _generate_test_scenarios(
        self,
        modified_files: List[str],
        task_description: str
    ) -> List[Dict[str, Any]]:
        """
        G√©n√®re les sc√©narios de test intelligents pour TOUT le code g√©n√©r√©.
        
        ‚úÖ NOUVEAU: Tests adapt√©s au type de code (backend, frontend, API, config, docs)
        
        Args:
            modified_files: Fichiers modifi√©s par l'agent
            task_description: Description de la t√¢che
            
        Returns:
            Liste des sc√©narios de test intelligents
        """
        scenarios = []
        
        file_categories = self._classify_files(modified_files)
        
        if file_categories["backend"]:
            scenarios.extend(await self._generate_backend_tests(file_categories["backend"]))
        if file_categories["frontend"]:
            scenarios.extend(await self._generate_frontend_tests(file_categories["frontend"]))
        
        if file_categories["backend"] and file_categories["frontend"]:
            scenarios.extend(await self._generate_integration_tests(file_categories))
        
        if file_categories["docs"] or file_categories["config"]:
            scenarios.extend(await self._generate_documentation_tests())
        
        generated_code_scenarios = [
            {
                "name": "Generated Code - Application Load",
                "type": "generated_smoke",
                "description": "L'agent v√©rifie que son code g√©n√©r√© se charge sans erreur",
                "steps": [
                    {"action": "navigate", "url": "/"},
                    {"action": "wait", "duration": 3},
                    {"action": "screenshot", "name": "generated_code_homepage"},
                    {"action": "check_console"},
                    {"action": "check_network"}
                ]
            },
            {
                "name": "Generated Code - Functionality Check",
                "type": "generated_functionality",
                "description": "L'agent teste les fonctionnalit√©s du code qu'il a g√©n√©r√©",
                "steps": [
                    {"action": "navigate", "url": "/"},
                    {"action": "wait", "duration": 2},
                    {"action": "evaluate", "script": """
                        // Test automatique des fonctionnalit√©s de base
                        const results = {
                            dom_loaded: document.readyState === 'complete',
                            has_content: document.body.innerText.length > 50,
                            links_count: document.querySelectorAll('a').length,
                            forms_count: document.querySelectorAll('form').length,
                            buttons_count: document.querySelectorAll('button').length,
                            images_count: document.querySelectorAll('img').length,
                            scripts_count: document.querySelectorAll('script').length
                        };
                        return results;
                    """},
                    {"action": "screenshot", "name": "functionality_check"},
                    {"action": "check_console"}
                ]
            },
            {
                "name": "Generated Code - Error Detection",
                "type": "generated_errors",
                "description": "L'agent d√©tecte les erreurs dans son propre code",
                "steps": [
                    {"action": "navigate", "url": "/"},
                    {"action": "wait", "duration": 2},
                    {"action": "check_console"},
                    {"action": "evaluate", "script": """
                        // V√©rifier les erreurs JS globales
                        return {
                            has_errors: window.onerror !== null,
                            console_error_count: 0,  // Sera rempli par check_console
                            page_title: document.title,
                            url: window.location.href
                        };
                    """},
                    {"action": "screenshot", "name": "error_detection"}
                ]
            },
            {
                "name": "Generated Code - Performance Check",
                "type": "generated_performance",
                "description": "L'agent mesure les performances de son code",
                "steps": [
                    {"action": "navigate", "url": "/"},
                    {"action": "wait", "duration": 2},
                    {"action": "check_performance"},
                    {"action": "screenshot", "name": "performance_check"}
                ]
            }
        ]
        
        scenarios = generated_code_scenarios + scenarios
        
        max_tests = self.settings.browser_qa_max_tests_per_file * 3
        if len(scenarios) > max_tests:
            logger.info(f"‚ö†Ô∏è Limitation √† {max_tests} tests (sur {len(scenarios)} g√©n√©r√©s)")
            scenarios = scenarios[:max_tests]
        
        logger.info(f"‚úÖ {len(scenarios)} sc√©nario(s) de test g√©n√©r√©(s)")
        return scenarios
    
    def _classify_files(self, modified_files: List[str]) -> Dict[str, List[str]]:
        """
        Classifie les fichiers modifi√©s par cat√©gorie.
        
        Args:
            modified_files: Liste des fichiers modifi√©s
            
        Returns:
            Dictionnaire des fichiers par cat√©gorie
        """
        categories = {
            "frontend": [],
            "backend": [],
            "config": [],
            "docs": []
        }
        
        for file in modified_files:
            if any(file.endswith(ext) for ext in [".tsx", ".jsx", ".ts", ".js", ".vue", ".html", ".css", ".scss"]):
                categories["frontend"].append(file)
            elif any(file.endswith(ext) for ext in [".py", ".rb", ".go", ".java", ".php"]):
                categories["backend"].append(file)
            elif any(file.endswith(ext) for ext in [".json", ".yaml", ".yml", ".toml", ".xml"]):
                categories["config"].append(file)
            elif any(file.endswith(ext) for ext in [".md", ".rst"]):
                categories["docs"].append(file)
        
        return categories
    
    async def _generate_backend_tests(self, backend_files: List[str]) -> List[Dict[str, Any]]:
        """
        G√©n√®re des tests pour le code backend (API endpoints).
        
        Args:
            backend_files: Fichiers backend modifi√©s
            
        Returns:
            Sc√©narios de test backend
        """
        scenarios = []
        
        scenarios.append({
            "name": "Backend API - Health Check",
            "type": "backend_api",
            "description": "Teste les endpoints API via le browser",
            "steps": [
                {"action": "navigate", "url": "/"},
                {"action": "evaluate", "script": """
                    async function testAPI() {
                        const results = [];
                        
                        // Test endpoint de base
                        try {
                            const response = await fetch('/api/health');
                            results.push({
                                endpoint: '/api/health',
                                status: response.status,
                                ok: response.ok
                            });
                        } catch (e) {
                            results.push({endpoint: '/api/health', error: e.message});
                        }
                        
                        return results;
                    }
                    return await testAPI();
                """},
                {"action": "screenshot", "name": "api_test"},
                {"action": "check_network"}
            ]
        })
        
        scenarios.append({
            "name": "Backend API - Documentation",
            "type": "backend_docs",
            "description": "V√©rifie que la documentation API est accessible",
            "steps": [
                {"action": "navigate", "url": "/docs"},
                {"action": "wait", "duration": 2},
                {"action": "screenshot", "name": "api_docs"},
                {"action": "check_console"}
            ]
        })
        
        return scenarios
    
    async def _generate_frontend_tests(self, frontend_files: List[str]) -> List[Dict[str, Any]]:
        """
        G√©n√®re des tests pour le code frontend.
        
        Args:
            frontend_files: Fichiers frontend modifi√©s
            
        Returns:
            Sc√©narios de test frontend
        """
        scenarios = []
        
        for file in frontend_files[:self.settings.browser_qa_max_tests_per_file]:
            component_name = Path(file).stem
            
            scenarios.append({
                "name": f"Frontend Component - {component_name}",
                "type": "frontend_component",
                "file": file,
                "description": f"Teste le composant {component_name}",
                "steps": [
                    {"action": "navigate", "url": "/"},
                    {"action": "wait", "duration": 1},
                    {"action": "screenshot", "name": f"component_{component_name}"},
                    {"action": "check_console"}
                ]
            })
        
        # Test responsive
        scenarios.append({
            "name": "Frontend Responsive - Multiple Viewports",
            "type": "frontend_responsive",
            "description": "Teste le responsive design",
            "steps": [
                {"action": "navigate", "url": "/"},
                {"action": "resize", "viewport": "375x667"},  # Mobile
                {"action": "screenshot", "name": "mobile"},
                {"action": "resize", "viewport": "768x1024"},  # Tablet
                {"action": "screenshot", "name": "tablet"},
                {"action": "resize", "viewport": "1920x1080"},  # Desktop
                {"action": "screenshot", "name": "desktop"},
                {"action": "check_console"}
            ]
        })
        
        return scenarios
    
    async def _generate_integration_tests(self, file_categories: Dict[str, List[str]]) -> List[Dict[str, Any]]:
        """
        G√©n√®re des tests d'int√©gration E2E (frontend + backend).
        
        Args:
            file_categories: Cat√©gories de fichiers
            
        Returns:
            Sc√©narios de test E2E
        """
        scenarios = []
        
        scenarios.append({
            "name": "Integration E2E - Full Flow",
            "type": "integration_e2e",
            "description": "Teste le flux complet frontend ‚Üí backend",
            "steps": [
                {"action": "navigate", "url": "/"},
                {"action": "wait", "duration": 2},
                {"action": "evaluate", "script": """
                    async function testIntegration() {
                        // Tester une requ√™te API depuis le frontend
                        try {
                            const response = await fetch('/api/test');
                            return {
                                success: response.ok,
                                status: response.status,
                                data: await response.json().catch(() => null)
                            };
                        } catch (e) {
                            return {success: false, error: e.message};
                        }
                    }
                    return await testIntegration();
                """},
                {"action": "screenshot", "name": "integration"},
                {"action": "check_console"},
                {"action": "check_network"},
                {"action": "check_performance"}
            ]
        })
        
        return scenarios
    
    async def _generate_documentation_tests(self) -> List[Dict[str, Any]]:
        """
        G√©n√®re des tests pour la documentation.
        
        Returns:
            Sc√©narios de test documentation
        """
        scenarios = []
        
        scenarios.append({
            "name": "Documentation - Accessibility",
            "type": "documentation",
            "description": "V√©rifie que la documentation est accessible",
            "steps": [
                {"action": "navigate", "url": "/admin"},
                {"action": "wait", "duration": 2},
                {"action": "screenshot", "name": "admin_interface"},
                {"action": "check_console"}
            ]
        })
        
        return scenarios
    
    async def _execute_test_scenario(
        self,
        server_url: str,
        scenario: Dict[str, Any],
        working_directory: str
    ) -> Dict[str, Any]:
        """
        Ex√©cute un sc√©nario de test avec support complet des outils Chrome MCP.
        
        ‚úÖ NOUVEAU: Support de evaluate, check_network, check_performance
        
        Args:
            server_url: URL du serveur
            scenario: Sc√©nario √† ex√©cuter
            working_directory: R√©pertoire de travail
            
        Returns:
            R√©sultat du test avec toutes les m√©triques
        """
        result = {
            "success": True,
            "scenario_name": scenario["name"],
            "scenario_type": scenario.get("type", "unknown"),
            "screenshot": None,
            "console_errors": [],
            "network_requests": [],
            "performance_metrics": {},
            "evaluation_results": None,
            "error": None
        }
        
        try:
            for step in scenario["steps"]:
                action = step["action"]
                
                if action == "navigate":
                    url = server_url + step.get("url", "/")
                    logger.debug(f"   ‚Üí Navigation: {url}")
                    await self.chrome_client.navigate_page(url)
                
                elif action == "wait":
                    duration = step.get("duration", 1)
                    await asyncio.sleep(duration)
                
                elif action == "screenshot":
                    screenshot_name = step.get("name", "screenshot")
                    screenshot_path = os.path.join(
                        working_directory,
                        f"browser_qa_{screenshot_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                    )
                    await self.chrome_client.take_screenshot(screenshot_path)
                    result["screenshot"] = screenshot_path
                    logger.debug(f"   ‚Üí Screenshot: {screenshot_path}")
                
                elif action == "check_console":
                    console_data = await self.chrome_client.get_console_messages()
                    if console_data.get("errors"):
                        result["console_errors"] = console_data["errors"]
                        if len(console_data["errors"]) > 0:
                            result["success"] = False
                            logger.warning(f"   ‚ö†Ô∏è {len(console_data['errors'])} erreur(s) console d√©tect√©e(s)")
                
                elif action == "check_network":
                    network_data = await self.chrome_client.execute_command(
                        "list_network_requests",
                        {}
                    )
                    if network_data and not network_data.get("error"):
                        result["network_requests"] = network_data.get("requests", [])
                        logger.debug(f"   ‚Üí Network: {len(result['network_requests'])} requ√™te(s)")
                
                elif action == "check_performance":
                    perf_data = await self.chrome_client.get_performance_metrics()
                    if perf_data and not perf_data.get("error"):
                        result["performance_metrics"] = perf_data
                        logger.debug(f"   ‚Üí Performance: {perf_data.get('load_time_ms', 'N/A')}ms")
                
                elif action == "evaluate":
                    script = step.get("script", "")
                    if script:
                        eval_result = await self.chrome_client.execute_command(
                            "evaluate_script",
                            {"script": script}
                        )
                        result["evaluation_results"] = eval_result
                        logger.debug(f"   ‚Üí Evaluation: {eval_result.get('success', 'N/A')}")
                
                elif action == "resize":
                    viewport = step.get("viewport", "1920x1080")
                    width, height = map(int, viewport.split("x"))
                    await self.chrome_client.execute_command(
                        "resize_page",
                        {"width": width, "height": height}
                    )
                    logger.debug(f"   ‚Üí Resize: {viewport}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur ex√©cution sc√©nario {scenario['name']}: {e}")
            result["success"] = False
            result["error"] = str(e)
        
        return result
    
    async def _analyze_performance(self, server_url: str) -> Dict[str, Any]:
        """
        Analyse les performances de la page.
        
        Args:
            server_url: URL du serveur
            
        Returns:
            M√©triques de performance
        """
        try:
            return {
                "load_time_ms": 0,
                "dom_content_loaded_ms": 0,
                "first_contentful_paint_ms": 0,
                "analyzed": False
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse performance: {e}")
            return {"error": str(e)}
    
    async def _cleanup(self):
        """Nettoyage des ressources."""
        try:
            if self.chrome_client:
                await self.chrome_client.stop()
                self.chrome_client = None
            
            if self.dev_server:
                await self.dev_server.stop_dev_server()
                self.dev_server = None
        except Exception as e:
            logger.error(f"‚ùå Erreur nettoyage: {e}")


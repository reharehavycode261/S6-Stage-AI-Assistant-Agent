"""
VyData Evaluator - LLM as Judge pour Ã©valuer les rÃ©ponses de l'agent VyData.

ImplÃ©mentation du systÃ¨me d'Ã©valuation automatique selon l'architecture:
Update Monday â†’ Agent Coding â†’ Output â†’ LLM Judge â†’ Score /100

CritÃ¨res d'Ã©valuation:
1. Accuracy - La rÃ©ponse rÃ©pond-elle correctement Ã  la question ?
2. Completeness - Traite-t-elle tous les aspects demandÃ©s ?
3. Clarity - Est-elle claire et bien structurÃ©e ?
4. Data Quality - Les donnÃ©es fournies sont-elles exactes et pertinentes ?
5. Actionability - Fournit-elle des informations utiles ou des Ã©tapes suivantes ?
"""

from typing import Dict, Any, Optional, List
from datetime import datetime
import time
from pydantic import BaseModel, Field
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from utils.logger import get_logger

logger = get_logger(__name__)


class EvaluationScore(BaseModel):
    """ModÃ¨le structurÃ© pour le score d'Ã©valuation."""
    score: int = Field(..., ge=0, le=100, description="Score global entre 0 et 100")
    reasoning: str = Field(..., description="Explication dÃ©taillÃ©e du score")
    criteria_scores: Dict[str, int] = Field(
        default_factory=dict,
        description="Scores par critÃ¨re (accuracy, completeness, clarity, data_quality, actionability)"
    )


class VyDataEvaluator:
    """
    Ã‰valuateur LLM pour l'agent VyData.
    
    Compare la sortie de l'agent (agent_response) avec la sortie attendue (reference_output)
    et attribue un score selon 5 critÃ¨res.
    """
    
    def __init__(
        self,
        model_name: str = "claude-3-5-sonnet-20241022",
        provider: str = "anthropic",
        temperature: float = 0.0
    ):
        """
        Initialise l'Ã©valuateur.
        
        Args:
            model_name: Nom du modÃ¨le LLM Ã  utiliser.
            provider: Provider ('anthropic' ou 'openai').
            temperature: TempÃ©rature du modÃ¨le (0.0 = dÃ©terministe).
        """
        self.model_name = model_name
        self.provider = provider
        
        llm_initialized = False
        
        if provider == "anthropic":
            try:
                self.llm = ChatAnthropic(
                    model=model_name,
                    temperature=temperature
                )
                llm_initialized = True
                logger.info(f"âœ… LLM Anthropic initialisÃ©: {model_name}")
            except Exception as e:
                logger.warning(f"âš ï¸ Ã‰chec Anthropic: {e}")
                logger.info("ğŸ”„ Fallback vers OpenAI...")
        
        if not llm_initialized or provider == "openai":
            try:
                self.llm = ChatOpenAI(
                    model="gpt-4" if provider == "anthropic" else model_name,
                    temperature=temperature
                )
                self.provider = "openai"  
                self.model_name = "gpt-4" if provider == "anthropic" else model_name
                llm_initialized = True
                logger.info(f"âœ… LLM OpenAI initialisÃ©: {self.model_name}")
            except Exception as e:
                logger.error(f"âŒ Ã‰chec OpenAI: {e}")
                raise ValueError(f"Impossible d'initialiser un LLM provider: {e}")
        
        if not llm_initialized:
            raise ValueError(f"Provider non supportÃ©: {provider}")
        
        self.prompt = self._create_prompt()
        
        self.chain = self.prompt | self.llm.with_structured_output(EvaluationScore)
        
        logger.info(f"âœ… VyDataEvaluator initialisÃ© (model={self.model_name}, provider={self.provider})")
    
    def _create_prompt(self) -> ChatPromptTemplate:
        """CrÃ©e le prompt systÃ¨me pour l'Ã©valuation."""
        
        system_prompt = """Tu es un Ã©valuateur expert pour un agent IA de coding appelÃ© VyData.

**ğŸ¯ Ta mission:**
Ã‰valuer la qualitÃ© des outputs de l'agent VyData en les comparant Ã  des outputs de rÃ©fÃ©rence attendus.

**ğŸ“Š CritÃ¨res d'Ã©valuation (5 critÃ¨res):**

1. **Accuracy** (Exactitude) - 25%
   - La rÃ©ponse rÃ©pond-elle correctement Ã  la question posÃ©e ?
   - Les informations fournies sont-elles exactes ?
   - L'agent a-t-il compris la demande ?

2. **Completeness** (ComplÃ©tude) - 25%
   - Tous les aspects demandÃ©s sont-ils traitÃ©s ?
   - La rÃ©ponse couvre-t-elle tous les points importants ?
   - Manque-t-il des Ã©lÃ©ments essentiels ?

3. **Clarity** (ClartÃ©) - 20%
   - La rÃ©ponse est-elle claire et bien structurÃ©e ?
   - Le format est-il lisible et professionnel ?
   - La communication est-elle efficace ?

4. **Data Quality** (QualitÃ© des donnÃ©es) - 15%
   - Les donnÃ©es/code fournis sont-ils exacts et pertinents ?
   - Les exemples sont-ils appropriÃ©s ?
   - Les rÃ©fÃ©rences sont-elles correctes ?

5. **Actionability** (CaractÃ¨re actionnable) - 15%
   - La rÃ©ponse fournit-elle des informations utiles ?
   - Contient-elle des Ã©tapes suivantes claires si nÃ©cessaire ?
   - L'utilisateur peut-il agir sur la base de cette rÃ©ponse ?

**ğŸ¯ BarÃ¨me de notation (0-100):**

- **90-100**: Excellent - Respecte tous les critÃ¨res, dÃ©passe les attentes
- **80-89**: Bon - Quelques problÃ¨mes mineurs, rÃ©pond globalement aux attentes
- **70-79**: AdÃ©quat - Des manques notables mais rÃ©ponse acceptable
- **50-69**: Pauvre - Erreurs significatives ou donnÃ©es manquantes
- **0-49**: TrÃ¨s mauvais - Ne rÃ©pond pas correctement Ã  la demande

**ğŸ“‹ Format de sortie attendu:**

Tu dois retourner un JSON avec:
- **score**: Score global (0-100)
- **reasoning**: Explication dÃ©taillÃ©e justifiant le score
- **criteria_scores**: Score pour chaque critÃ¨re individuel

**âš ï¸ Important:**
- Sois objectif et constructif
- Compare toujours avec l'output de rÃ©fÃ©rence attendu
- Justifie prÃ©cisÃ©ment ton score
- Mentionne les points forts ET les points Ã  amÃ©liorer
"""

        user_prompt = """**ğŸ“ Input (Question/Commande de l'utilisateur):**
{reference_input}

**âœ… Output Attendu (RÃ©fÃ©rence du Golden Set):**
{reference_output}

**ğŸ¤– Output GÃ©nÃ©rÃ© par l'Agent VyData:**
{agent_response}

---

**Instructions:**
Ã‰value l'output de l'agent en le comparant Ã  l'output attendu. Attribue un score global et des scores par critÃ¨re."""

        return ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", user_prompt)
        ])
    
    def evaluate_response(
        self,
        reference_input: str,
        reference_output: str,
        agent_response: str,
        test_id: Optional[str] = None,
        monday_update_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Ã‰value la rÃ©ponse de l'agent.
        
        Args:
            reference_input: La question/commande initiale (input Monday).
            reference_output: La rÃ©ponse attendue (Golden Set).
            agent_response: La rÃ©ponse gÃ©nÃ©rÃ©e par l'agent.
            test_id: ID du test (optionnel).
            monday_update_id: ID de l'update Monday (optionnel).
            
        Returns:
            Dictionnaire avec les rÃ©sultats de l'Ã©valuation complets.
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ” Ã‰valuation en cours{f' pour {test_id}' if test_id else ''}...")
            result: EvaluationScore = self.chain.invoke({
                "reference_input": reference_input,
                "reference_output": reference_output,
                "agent_response": agent_response
            })
            
            duration = time.time() - start_time
            
            status = "PASS" if result.score >= 70 else "FAIL"
            
            evaluation_result = {
                "eval_id": f"EVAL_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "timestamp": datetime.now().isoformat(),
                "test_id": test_id or "N/A",
                "monday_update_id": monday_update_id or "N/A",
                "agent_output": agent_response,
                "llm_score": result.score,
                "llm_reasoning": result.reasoning,
                "criteria_scores": result.criteria_scores,
                "human_validation_status": "pending",
                "human_score": None,
                "human_feedback": None,
                "final_score": result.score,  
                "status": status,
                "duration_seconds": round(duration, 2),
                "error_message": None,
                "evaluator_model": self.model_name,
                "evaluator_provider": self.provider
            }
            
            logger.info(
                f"âœ… Ã‰valuation terminÃ©e: {result.score}/100 ({status}) "
                f"en {duration:.2f}s"
            )
            
            return evaluation_result
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'Ã©valuation: {e}", exc_info=True)
            
            duration = time.time() - start_time
            
            return {
                "eval_id": f"EVAL_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "timestamp": datetime.now().isoformat(),
                "test_id": test_id or "N/A",
                "monday_update_id": monday_update_id or "N/A",
                "agent_output": agent_response,
                "llm_score": 0,
                "llm_reasoning": f"Error during evaluation: {str(e)}",
                "criteria_scores": {},
                "human_validation_status": "pending",
                "human_score": None,
                "human_feedback": None,
                "final_score": 0,
                "status": "FAIL",
                "duration_seconds": round(duration, 2),
                "error_message": str(e),
                "evaluator_model": self.model_name,
                "evaluator_provider": self.provider
            }
    
    def batch_evaluate(
        self,
        evaluations: List[Dict[str, str]]
    ) -> List[Dict[str, Any]]:
        """
        Ã‰value plusieurs rÃ©ponses en batch.
        
        Args:
            evaluations: Liste de dict avec keys: reference_input, reference_output, agent_response
            
        Returns:
            Liste des rÃ©sultats d'Ã©valuation.
        """
        logger.info(f"ğŸ“Š Ã‰valuation batch de {len(evaluations)} rÃ©ponses...")
        
        results = []
        for i, eval_data in enumerate(evaluations, 1):
            logger.info(f"   Ã‰valuation {i}/{len(evaluations)}...")
            result = self.evaluate_response(**eval_data)
            results.append(result)
        
        avg_score = sum(r['llm_score'] for r in results) / len(results)
        passed = sum(1 for r in results if r['status'] == 'PASS')
        
        logger.info(
            f"âœ… Batch terminÃ©: {passed}/{len(results)} PASS "
            f"(score moyen: {avg_score:.1f}/100)"
        )
        
        return results
    
    def get_evaluation_summary(self, result: Dict[str, Any]) -> str:
        """
        GÃ©nÃ¨re un rÃ©sumÃ© textuel de l'Ã©valuation pour affichage.
        
        Args:
            result: RÃ©sultat d'Ã©valuation.
            
        Returns:
            Texte formatÃ© pour affichage.
        """
        summary = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“Š RÃ‰SULTAT D'Ã‰VALUATION - {result['test_id']}
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ¯ Score Global: {result['llm_score']}/100 ({result['status']})
â•‘  â±ï¸  DurÃ©e: {result['duration_seconds']}s
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“‹ Scores par CritÃ¨re:
"""
        
        if result.get('criteria_scores'):
            for criterion, score in result['criteria_scores'].items():
                summary += f"â•‘    â€¢ {criterion}: {score}/100\n"
        
        summary += f"""â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ’¬ Justification:
â•‘  {result['llm_reasoning'][:80]}...
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        
        return summary


"""
Explorateur approfondi de repository pour comprendre le code avant g√©n√©ration.

‚úÖ AM√âLIORATION MAJEURE: Phase d'exploration obligatoire avant toute g√©n√©ration de code.
"""

import os
import re
from typing import Dict, Any, List, Optional, Set
from pathlib import Path
from utils.logger import get_logger

logger = get_logger(__name__)


class RepositoryExplorer:
    """
    Explore un repository en profondeur pour construire un contexte riche.
    
    Cette classe est CRITIQUE pour la qualit√© du code g√©n√©r√©:
    - Lit le code existant
    - Identifie les patterns et conventions
    - Comprend l'architecture
    - Construit un contexte pour l'IA
    """
    
    def __init__(self, working_directory: str):
        self.working_directory = Path(working_directory)
        self.logger = logger
    
    async def explore_for_task(
        self,
        task_description: str,
        files_mentioned: Optional[List[str]] = None,
        max_files_to_read: int = 15
    ) -> Dict[str, Any]:
        """
        Explore le repository de mani√®re cibl√©e pour une t√¢che sp√©cifique.
        
        ‚úÖ STRAT√âGIE D'EXPLORATION:
        1. Identifier les fichiers pertinents √† la t√¢che
        2. Lire leur contenu complet
        3. Analyser les patterns et conventions
        4. Construire un contexte riche
        
        Args:
            task_description: Description de la t√¢che
            files_mentioned: Fichiers mentionn√©s dans la t√¢che
            max_files_to_read: Nombre max de fichiers √† lire (15 par d√©faut)
            
        Returns:
            Dict avec le contexte complet du repository
        """
        self.logger.info(f"üîç Exploration approfondie du repository pour: {task_description[:100]}...")
        
        context = {
            "files_read": [],
            "code_samples": {},
            "patterns_detected": [],
            "conventions": {},
            "architecture_insights": [],
            "dependencies": [],
            "related_files": []
        }
        
        relevant_files = await self._identify_relevant_files(
            task_description,
            files_mentioned,
            max_files_to_read
        )
        
        self.logger.info(f"üìã {len(relevant_files)} fichiers identifi√©s comme pertinents")
        
        for file_path in relevant_files:
            try:
                content = await self._read_file_safely(file_path)
                if content:
                    context["files_read"].append(str(file_path))
                    context["code_samples"][str(file_path)] = content
                    self.logger.info(f"‚úÖ Lu: {file_path} ({len(content)} caract√®res)")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Impossible de lire {file_path}: {e}")
        
        if context["code_samples"]:
            patterns = self._analyze_code_patterns(context["code_samples"])
            context["patterns_detected"] = patterns["patterns"]
            context["conventions"] = patterns["conventions"]
            
            self.logger.info(f"üéØ {len(patterns['patterns'])} patterns d√©tect√©s")
        
        architecture = self._identify_architecture(context["code_samples"])
        context["architecture_insights"] = architecture
        
        dependencies = self._extract_dependencies(context["code_samples"])
        context["dependencies"] = dependencies
        
        related = await self._find_related_files(relevant_files)
        context["related_files"] = related
        
        self.logger.info(f"‚úÖ Exploration termin√©e: {len(context['files_read'])} fichiers analys√©s")
        
        return context
    
    async def _identify_relevant_files(
        self,
        task_description: str,
        files_mentioned: Optional[List[str]],
        max_files: int
    ) -> List[Path]:
        """
        Identifie les fichiers pertinents pour la t√¢che.
        
        ‚úÖ STRAT√âGIE:
        1. Fichiers mentionn√©s explicitement
        2. Fichiers dans les m√™mes r√©pertoires
        3. Fichiers avec noms similaires
        4. Fichiers de configuration/build
        """
        relevant = set()
        
        if files_mentioned:
            for file_path in files_mentioned:
                full_path = self.working_directory / file_path
                if full_path.exists():
                    relevant.add(full_path)
                    self.logger.info(f"üìå Fichier mentionn√© ajout√©: {file_path}")
        
        keywords = self._extract_keywords_from_task(task_description)
        self.logger.info(f"üîë Mots-cl√©s extraits: {', '.join(keywords[:5])}")
        
        all_files = []
        for ext in ['.py', '.js', '.ts', '.java', '.go', '.rb', '.php', '.jsx', '.tsx']:
            try:
                all_files.extend(self.working_directory.rglob(f"*{ext}"))
            except Exception as e:
                self.logger.warning(f"Erreur scan {ext}: {e}")
        
        for file_path in all_files[:200]:  # Limiter la recherche
            if self._is_relevant_file(file_path, keywords):
                relevant.add(file_path)
                if len(relevant) >= max_files:
                    break
        
        if len(relevant) == 0 and all_files:
            self.logger.info("üìù Aucun fichier trouv√© par mots-cl√©s, ajout des fichiers source principaux")
            for file_path in all_files[:max_files]:
                if not self._is_test_or_excluded(file_path):
                    relevant.add(file_path)
                    if len(relevant) >= max_files:
                        break
        
        config_files = [
            'package.json', 'requirements.txt', 'pom.xml', 'build.gradle',
            'Cargo.toml', 'go.mod', 'composer.json', 'Gemfile'
        ]
        
        for config_file in config_files:
            config_path = self.working_directory / config_file
            if config_path.exists():
                relevant.add(config_path)
        
        return list(relevant)[:max_files]
    
    def _extract_keywords_from_task(self, task_description: str) -> List[str]:
        """Extrait les mots-cl√©s importants de la description de t√¢che."""
        # Nettoyer et tokenizer
        words = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]{2,}\b', task_description.lower())
        
        # Filtrer les mots communs
        stop_words = {
            'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her',
            'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how',
            'add', 'create', 'update', 'delete', 'implement', 'modify', 'change'
        }
        
        keywords = [w for w in words if w not in stop_words and len(w) > 3]
        
        return list(dict.fromkeys(keywords))[:20]
    
    def _is_test_or_excluded(self, file_path: Path) -> bool:
        """V√©rifie si un fichier est un test ou doit √™tre exclu."""
        path_str = str(file_path).lower()
        
        exclusions = [
            'node_modules', 'venv', '__pycache__', '.git', 'dist', 'build',
            'test_', '_test', '.spec.', '.test.', 'tests/'
        ]
        
        return any(exclude in path_str for exclude in exclusions)
    
    def _is_relevant_file(self, file_path: Path, keywords: List[str]) -> bool:
        """D√©termine si un fichier est pertinent pour la t√¢che."""
        if self._is_test_or_excluded(file_path):
            return False
        
        path_str = str(file_path).lower()
        file_name = file_path.stem.lower()
        
        for keyword in keywords:
            if keyword in file_name or keyword in path_str:
                return True
        
        return False
    
    async def _read_file_safely(self, file_path: Path, max_size: int = 50000) -> Optional[str]:
        """
        Lit un fichier de mani√®re s√©curis√©e avec limite de taille.
        
        Args:
            file_path: Chemin du fichier
            max_size: Taille maximale en caract√®res (50KB par d√©faut)
            
        Returns:
            Contenu du fichier ou None si erreur
        """
        try:
            if not file_path.exists():
                return None
            
            size = file_path.stat().st_size
            if size > max_size:
                self.logger.warning(f"‚ö†Ô∏è Fichier {file_path} trop grand ({size} bytes), lecture partielle")
                # Lire seulement le d√©but
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read(max_size)
            
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        
        except Exception as e:
            self.logger.warning(f"Erreur lecture {file_path}: {e}")
            return None
    
    def _analyze_code_patterns(self, code_samples: Dict[str, str]) -> Dict[str, Any]:
        """
        Analyse les patterns et conventions dans le code existant.
        
        ‚úÖ D√âTECTION:
        - Conventions de nommage
        - Patterns architecturaux
        - Style de code
        - Imports courants
        """
        patterns = []
        conventions = {}
        
        # Analyser chaque fichier
        all_code = "\n".join(code_samples.values())
        
        # 1. Convention de nommage
        if re.search(r'function\s+[a-z][a-zA-Z0-9]*', all_code):
            conventions["naming"] = "camelCase"
            patterns.append("Convention camelCase pour les fonctions")
        elif re.search(r'def\s+[a-z][a-z0-9_]*', all_code):
            conventions["naming"] = "snake_case"
            patterns.append("Convention snake_case pour les fonctions")
        
        # 2. Style de classes
        if 'class ' in all_code:
            if re.search(r'class\s+[A-Z][a-zA-Z0-9]*', all_code):
                patterns.append("Classes en PascalCase")
        
        # 3. Patterns d'imports
        imports = re.findall(r'^import\s+(\w+)', all_code, re.MULTILINE)
        imports.extend(re.findall(r'^from\s+(\w+)', all_code, re.MULTILINE))
        
        if imports:
            conventions["common_imports"] = list(set(imports))[:10]
            patterns.append(f"Imports courants: {', '.join(list(set(imports))[:5])}")
        
        # 4. Patterns d'architecture
        if 'async def' in all_code or 'await ' in all_code:
            patterns.append("Architecture asynchrone d√©tect√©e")
            conventions["async"] = True
        
        if 'interface ' in all_code or 'implements ' in all_code:
            patterns.append("Utilisation d'interfaces")
        
        if '@' in all_code and ('decorator' in all_code or 'annotation' in all_code):
            patterns.append("Utilisation de d√©corateurs/annotations")
        
        return {
            "patterns": patterns,
            "conventions": conventions
        }
    
    def _identify_architecture(self, code_samples: Dict[str, str]) -> List[str]:
        """Identifie l'architecture du projet."""
        insights = []
        
        all_code = "\n".join(code_samples.values())
        file_paths = list(code_samples.keys())
        
        # D√©tecter MVC
        if any('controller' in path.lower() for path in file_paths):
            insights.append("Architecture MVC d√©tect√©e (contr√¥leurs pr√©sents)")
        
        if any('model' in path.lower() for path in file_paths):
            insights.append("Couche Model pr√©sente")
        
        if any('view' in path.lower() or 'template' in path.lower() for path in file_paths):
            insights.append("Couche View pr√©sente")
        
        # D√©tecter patterns
        if 'Repository' in all_code or 'repository' in all_code.lower():
            insights.append("Pattern Repository d√©tect√©")
        
        if 'Service' in all_code and 'service' in all_code.lower():
            insights.append("Architecture en couches (Services)")
        
        return insights
    
    def _extract_dependencies(self, code_samples: Dict[str, str]) -> List[str]:
        """Extrait les d√©pendances du code."""
        dependencies = set()
        
        for content in code_samples.values():
            # Python imports
            imports = re.findall(r'^(?:from|import)\s+([\w.]+)', content, re.MULTILINE)
            dependencies.update(imports)
        
        return list(dependencies)[:20]
    
    async def _find_related_files(self, files: List[Path]) -> List[str]:
        """Trouve les fichiers li√©s (m√™me r√©pertoire, imports, etc.)."""
        related = set()
        
        for file_path in files:
            # Fichiers dans le m√™me r√©pertoire
            try:
                siblings = list(file_path.parent.glob(f"*{file_path.suffix}"))
                related.update(str(s) for s in siblings[:5])
            except Exception:
                pass
        
        return list(related)[:10]
    
    def build_context_summary(self, exploration_result: Dict[str, Any]) -> str:
        """
        Construit un r√©sum√© textuel du contexte pour l'IA.
        
        ‚úÖ FORMAT OPTIMIS√â pour la g√©n√©ration de code de qualit√©.
        """
        summary = "## üìö CONTEXTE DU REPOSITORY (CODE R√âEL ANALYS√â)\n\n"
        
        # 1. Fichiers analys√©s
        files_count = len(exploration_result.get("files_read", []))
        summary += f"### Fichiers analys√©s: {files_count}\n\n"
        
        if exploration_result.get("files_read"):
            summary += "**Fichiers pertinents lus:**\n"
            for f in exploration_result["files_read"][:10]:
                summary += f"- `{f}`\n"
            summary += "\n"
        
        # 2. Patterns d√©tect√©s
        if exploration_result.get("patterns_detected"):
            summary += "### ‚úÖ Patterns et conventions d√©tect√©s:\n\n"
            for pattern in exploration_result["patterns_detected"]:
                summary += f"- {pattern}\n"
            summary += "\n"
        
        # 3. Conventions de code
        if exploration_result.get("conventions"):
            summary += "### üìè Conventions de code √† respecter:\n\n"
            conv = exploration_result["conventions"]
            
            if "naming" in conv:
                summary += f"- **Nommage**: {conv['naming']}\n"
            
            if "common_imports" in conv:
                summary += f"- **Imports courants**: {', '.join(conv['common_imports'][:5])}\n"
            
            if "async" in conv:
                summary += "- **Asynchrone**: Utiliser async/await\n"
            
            summary += "\n"
        
        # 4. Architecture
        if exploration_result.get("architecture_insights"):
            summary += "### üèóÔ∏è Architecture du projet:\n\n"
            for insight in exploration_result["architecture_insights"]:
                summary += f"- {insight}\n"
            summary += "\n"
        
        # 5. Exemples de code
        if exploration_result.get("code_samples"):
            summary += "### üíª EXEMPLES DE CODE EXISTANT (√† respecter):\n\n"
            
            for file_path, content in list(exploration_result["code_samples"].items())[:3]:
                summary += f"**Fichier: `{file_path}`** (extrait):\n```\n"
                # Prendre les 500 premiers caract√®res
                extract = content[:500].strip()
                summary += extract
                if len(content) > 500:
                    summary += "\n... (tronqu√©)"
                summary += "\n```\n\n"
        
        summary += "‚ö†Ô∏è **IMPORTANT**: Le code que tu g√©n√®res DOIT respecter ces patterns et conventions!\n"
        
        return summary


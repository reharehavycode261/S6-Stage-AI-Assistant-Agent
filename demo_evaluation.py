#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DÃ©monstration rapide du systÃ¨me d'Ã©valuation Golden Dataset.

Ce script montre comment le systÃ¨me fonctionne avec un exemple simple.
"""

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                   â•‘
â•‘        ğŸ¯ DÃ‰MONSTRATION: SYSTÃˆME D'Ã‰VALUATION GOLDEN DATASET     â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Le systÃ¨me EST DÃ‰JÃ€ IMPLÃ‰MENTÃ‰ dans votre projet !

ğŸ“Š STATISTIQUES ACTUELLES:
   â€¢ Score de fiabilitÃ©: 66.24/100 (needs_improvement)
   â€¢ Tests rÃ©ussis: 3/5 (60%)
   â€¢ Score moyen: 75.6/100

ğŸ“ DATASETS EXISTANTS:

1ï¸âƒ£  Dataset Questions (data/golden_datasets/questions_dataset.json)
   â€¢ 5 questions de test
   â€¢ Type: Questions â†’ RÃ©sultats d'analyses
   â€¢ Exemples:
     âœ“ "hello" â†’ Salutation et capacitÃ©s
     âœ“ "Pourquoi Java ?" â†’ Analyse technique
     âœ“ "Dernier commit ?" â†’ MÃ©tadonnÃ©es GitHub
     âœ“ "Structure projet ?" â†’ Architecture
     âœ“ "Dernier PR ?" â†’ Pull Requests GitHub

2ï¸âƒ£  Dataset Commandes (data/golden_datasets/commands_dataset.json)
   â€¢ 1 commande de test
   â€¢ Type: Commandes â†’ Pull Requests
   â€¢ Exemple:
     âœ“ "CrÃ©e un formulaire de login" â†’ PR complet

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ COMMENT FONCTIONNE L'Ã‰VALUATION:

1. CRÃ‰ATION DU TEST
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Input: "Quel est le dernier commit?"â”‚
   â”‚ Expected: "Le message est X par Y..." â”‚
   â”‚ Criteria: [Pertinence, PrÃ©cision...]â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
2. EXÃ‰CUTION PAR L'AGENT
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ DÃ©tection: question GitHub        â”‚
   â”‚ â€¢ Appel API GitHub (13 collecteurs) â”‚
   â”‚ â€¢ Extraction donnÃ©es structurÃ©es    â”‚
   â”‚ â€¢ GÃ©nÃ©ration rÃ©ponse (GPT-4)        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
3. Ã‰VALUATION PAR LLM JUDGE
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Compare: attendu vs rÃ©el            â”‚
   â”‚ Scores:                             â”‚
   â”‚  â€¢ Pertinence: 90/100               â”‚
   â”‚  â€¢ PrÃ©cision: 85/100                â”‚
   â”‚  â€¢ ClartÃ©: 80/100                   â”‚
   â”‚  â€¢ Contexte: 85/100                 â”‚
   â”‚ â†’ Moyenne: 85/100 âœ… PASSED         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
4. RAPPORT FINAL
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Score global: 66.24/100             â”‚
   â”‚ Tests rÃ©ussis: 3/5                  â”‚
   â”‚ Statut: Ã€ AMÃ‰LIORER                 â”‚
   â”‚ Rapport JSON sauvegardÃ©             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ COMMENT UTILISER:

OPTION 1: Tester avec les datasets existants
    curl -X POST "http://localhost:8000/evaluation/run?dataset_type=questions&run_in_background=false"

OPTION 2: Interface interactive (VOS 5 questions)
    python3 custom_evaluation_interactive.py

OPTION 3: Voir les derniers rÃ©sultats
    python3 show_evaluation_results.py

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“š VOTRE CAS D'USAGE EXACT:

Vous voulez:
âœ… Poser 5 questions Ã  l'agent
âœ… L'agent y rÃ©pond
âœ… Un LLM Judge Ã©value (comme un prof)
âœ… Obtenir un score de fiabilitÃ©

C'EST EXACTEMENT CE QUI EST IMPLÃ‰MENTÃ‰ ! ğŸ‰

Lancez simplement:
    python3 custom_evaluation_interactive.py

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š RÃ‰SULTATS DÃ‰TAILLÃ‰S ACTUELS:

Test 1: q001_hello                    âœ… 95/100 PASSED
   Salutation et prÃ©sentation des capacitÃ©s
   
Test 2: q002_technologies             âœ… 85/100 PASSED
   Analyse des choix technologiques (Java vs Python)
   
Test 3: q003_last_commit              âŒ 55/100 FAILED
   RÃ©cupÃ©ration du dernier commit GitHub
   
Test 4: q004_project_structure        âœ… 78/100 PASSED
   Analyse de la structure du projet
   
Test 5: q005_last_pr                  âŒ 65/100 FAILED
   Informations sur le dernier Pull Request

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¡ WORKFLOW INDUSTRIALISATION:

1. Ã‰VALUATION (oÃ¹ vous Ãªtes maintenant)
   â””â”€> Score < 70% ? â†’ AmÃ©liorer l'agent
   â””â”€> Score â‰¥ 70% ? â†’ Tests supplÃ©mentaires
   â””â”€> Score â‰¥ 80% ? â†’ PrÃªt pour production

2. AMÃ‰LIORATION
   â€¢ Analyser les tests Ã©chouÃ©s
   â€¢ Corriger les problÃ¨mes identifiÃ©s
   â€¢ RÃ©-Ã©valuer

3. PRODUCTION
   â€¢ Agent validÃ© et fiable
   â€¢ DÃ©ployÃ© pour utilisation rÃ©elle
   â€¢ Monitoring continu

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ FICHIERS IMPORTANTS:

â€¢ custom_evaluation_interactive.py   â† Interface pour vos questions
â€¢ GUIDE_EVALUATION.md                â† Documentation complÃ¨te
â€¢ data/golden_datasets/              â† Datasets existants
â€¢ data/evaluation_reports/           â† Rapports gÃ©nÃ©rÃ©s
â€¢ services/evaluation/               â† Code du systÃ¨me

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ PROCHAINES Ã‰TAPES SUGGÃ‰RÃ‰ES:

1. Lire le guide complet:
   cat GUIDE_EVALUATION.md

2. Tester avec VOS questions:
   python3 custom_evaluation_interactive.py

3. Analyser les rÃ©sultats:
   python3 show_evaluation_results.py

4. AmÃ©liorer l'agent si nÃ©cessaire

5. RÃ©-Ã©valuer jusqu'Ã  score â‰¥ 80%

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ¨ Le systÃ¨me est COMPLET et FONCTIONNEL ! âœ¨

Vous pouvez commencer Ã  l'utiliser immÃ©diatement.

""")

print("ğŸ’¡ Lancez: python3 custom_evaluation_interactive.py")
print()


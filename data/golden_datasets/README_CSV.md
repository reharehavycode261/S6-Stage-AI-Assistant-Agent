# üìä Guide d'utilisation des fichiers CSV pour Golden Datasets

## üìÅ Fichiers cr√©√©s

Trois fichiers CSV ont √©t√© cr√©√©s dans ce dossier :

### 1. `golden_sets.csv` - Tests de r√©f√©rence (Golden Set)
**Colonnes :**
- `test_id` : Identifiant unique du test (ex: GS_A001, GS_P001)
- `test_type` : Type de test (`analysis` ou `pr`)
- `input_monday_update` : L'input qui d√©clenche l'agent (simulation d'un update Monday)
- `expected_output` : Le r√©sultat attendu de l'agent
- `expected_output_type` : Type de sortie (`resultats_analyses` ou `pr`)
- `evaluation_criteria` : Crit√®res d'√©valuation s√©par√©s par `;`
- `priority` : Priorit√© du test (`high`, `medium`, `low`)
- `active` : Test actif ou non (`TRUE` ou `FALSE`)

**üìù Contenu actuel : 16 tests**
- 10 tests d'analyse (GS_A001 √† GS_A010)
- 6 tests de PR (GS_P001 √† GS_P006)

---

### 2. `evaluation_results.csv` - R√©sultats d'√©valuation
**Colonnes :**
- `eval_id` : ID unique de l'√©valuation (format: EVAL_YYYYMMDD_HHMMSS)
- `timestamp` : Date et heure de l'√©valuation (ISO 8601)
- `test_id` : R√©f√©rence au test du Golden Set
- `monday_update_id` : ID de l'update Monday qui a d√©clench√© l'agent
- `agent_output` : La r√©ponse g√©n√©r√©e par l'agent
- `llm_score` : Score attribu√© par le LLM judge (0-100)
- `llm_reasoning` : Justification du score LLM
- `human_validation_status` : Statut validation humaine (`validated`, `pending`, `rejected`, `to_review`)
- `human_score` : Score donn√© par l'humain (0-100, NULL si pas valid√©)
- `human_feedback` : Commentaire de l'humain
- `final_score` : Score final (moyenne pond√©r√©e LLM 60% + Human 40%)
- `status` : `PASS` (‚â•70) ou `FAIL` (<70)
- `duration_seconds` : Dur√©e d'ex√©cution de l'agent
- `error_message` : Message d'erreur (NULL si pas d'erreur)

**üìù Contenu actuel : 15 √©valuations exemple**

---

### 3. `performance_metrics.csv` - M√©triques de performance
**Colonnes :**
- `metric_date` : Date de la m√©trique (YYYY-MM-DD)
- `total_tests_run` : Nombre total de tests ex√©cut√©s
- `tests_analysis` : Nombre de tests d'analyse
- `tests_pr` : Nombre de tests PR
- `pass_rate_percent` : Taux de r√©ussite en %
- `avg_llm_score` : Score moyen du LLM judge
- `avg_human_score` : Score moyen des validations humaines
- `avg_final_score` : Score final moyen
- `avg_duration_s` : Dur√©e moyenne d'ex√©cution en secondes
- `tests_pending_validation` : Nombre de tests en attente de validation
- `reliability_status` : Statut (`excellent` ‚â•85, `good` ‚â•70, `needs_improvement` <70)
- `notes` : Notes et commentaires

**üìù Contenu actuel : 15 jours de m√©triques (23-06 octobre 2025)**

---

## üöÄ Utilisation dans Excel

### Option 1 : Import manuel (recommand√© pour visualisation)

1. **Ouvrir Excel** et cr√©er un nouveau classeur

2. **Importer chaque CSV dans une feuille s√©par√©e :**
   
   **Pour Excel (Windows/Mac) :**
   - Onglet "Donn√©es" ‚Üí "Obtenir des donn√©es" ‚Üí "√Ä partir d'un fichier" ‚Üí "CSV"
   - S√©lectionner `golden_sets.csv`
   - Cliquer sur "Charger"
   - Renommer la feuille en `Golden_Sets`
   - R√©p√©ter pour `evaluation_results.csv` ‚Üí feuille `Evaluation_Results`
   - R√©p√©ter pour `performance_metrics.csv` ‚Üí feuille `Performance_Metrics`

3. **Sauvegarder le classeur** : `golden_datasets.xlsx`

### Option 2 : Conversion automatique avec Python

Si vous avez Python et pandas install√©s :

```bash
# Depuis la racine du projet
python scripts/csv_to_excel.py
```

Cela cr√©era automatiquement `golden_datasets.xlsx` avec les 3 feuilles.

---

## üìä Structure Excel finale

```
golden_datasets.xlsx
‚îú‚îÄ‚îÄ Feuille 1: Golden_Sets (16 tests de r√©f√©rence)
‚îú‚îÄ‚îÄ Feuille 2: Evaluation_Results (15 √©valuations)
‚îî‚îÄ‚îÄ Feuille 3: Performance_Metrics (15 jours de m√©triques)
```

---

## ‚úèÔ∏è Modification des donn√©es

### Ajouter un nouveau test (Golden_Sets)

1. Ouvrir `golden_sets.csv` (ou la feuille Excel)
2. Ajouter une nouvelle ligne :
   ```csv
   GS_A011,analysis,Nouvelle question test,R√©ponse attendue,resultats_analyses,accuracy;completeness,high,TRUE
   ```
3. Sauvegarder

### Enregistrer une nouvelle √©valuation (Evaluation_Results)

Le code Python se charge d'ajouter automatiquement les r√©sultats via :
```python
from services.evaluation.excel_golden_dataset_service import ExcelGoldenDatasetService

service = ExcelGoldenDatasetService()
service.save_evaluation_result({
    "eval_id": "EVAL_20251107_100000",
    "timestamp": "2025-11-07T10:00:00",
    "test_id": "GS_A001",
    # ... autres champs
})
```

---

## üéØ Crit√®res d'√©valuation disponibles

- `accuracy` : Exactitude de la r√©ponse
- `completeness` : Compl√©tude (tous les aspects trait√©s)
- `clarity` : Clart√© et structure
- `data_quality` : Qualit√© des donn√©es/code
- `code_quality` : Qualit√© du code (pour les PRs)
- `actionability` : Caract√®re actionnable

**Format dans le CSV :** S√©parer par des points-virgules `;`
```
accuracy;completeness;clarity;data_quality;actionability
```

---

## üìà Interpr√©tation des scores

| Score | Statut | Signification |
|-------|--------|---------------|
| 90-100 | Excellent | R√©pond √† tous les crit√®res |
| 80-89 | Bon | Quelques probl√®mes mineurs |
| 70-79 | Ad√©quat | Manques notables |
| 50-69 | Pauvre | Erreurs ou donn√©es manquantes |
| 0-49 | Tr√®s pauvre | Ne r√©pond pas correctement |

**Seuil de r√©ussite par d√©faut : 70/100**

---

## üîÑ Workflow complet

```
1. Update Monday d√©clenche l'agent
        ‚Üì
2. Agent g√©n√®re une r√©ponse (agent_output)
        ‚Üì
3. LLM Judge √©value vs expected_output ‚Üí llm_score
        ‚Üì
4. Validation humaine (optionnelle) ‚Üí human_score
        ‚Üì
5. Calcul final_score (60% LLM + 40% Human)
        ‚Üì
6. Enregistrement dans Evaluation_Results
        ‚Üì
7. Mise √† jour des Performance_Metrics quotidiennes
```

---

## üõ†Ô∏è D√©pendances Python

Pour utiliser le script de conversion :

```bash
pip install pandas openpyxl
```

---

## üìû Support

Pour toute question sur l'utilisation de ces fichiers CSV, consulter :
- `services/evaluation/excel_golden_dataset_service.py` : Service de gestion Excel
- `services/evaluation/llm_excel_evaluator.py` : √âvaluateur LLM
- `services/evaluation/excel_evaluation_orchestrator.py` : Orchestrateur

---

## ‚úÖ Checklist d'utilisation

- [ ] Fichiers CSV cr√©√©s dans `data/golden_datasets/`
- [ ] Import dans Excel r√©ussi (3 feuilles)
- [ ] Tests Golden Set ajout√©s/modifi√©s selon vos besoins
- [ ] Script Python test√© (optionnel)
- [ ] Workflow d'√©valuation compris
- [ ] Pr√™t √† lancer les √©valuations !

**Bon courage avec l'√©valuation de votre agent ! üöÄ**












"""
Service de recherche s√©mantique RAG (Retrieval-Augmented Generation).

Ce service:
- Combine embeddings et vector store pour la recherche s√©mantique
- Fournit un contexte pertinent multilingue pour le LLM
- √âvite les hallucinations en basant les r√©ponses sur les donn√©es r√©elles
- G√®re l'historique des conversations
"""

import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import re

from openai import AsyncOpenAI
from config.settings import get_settings
from utils.logger import get_logger
from services.embedding_service import embedding_service
from services.vector_store_service import (
    vector_store_service,
    SimilaritySearchResult,
    ContextSearchResult
)

logger = get_logger(__name__)
settings = get_settings()


@dataclass
class EnrichedContext:
    """Contexte enrichi pour le LLM avec sources."""
    query: str
    similar_messages: List[SimilaritySearchResult] = field(default_factory=list)
    project_context: Optional[List[ContextSearchResult]] = None
    formatted_context: str = ""
    total_sources: int = 0
    relevance_score: float = 0.0


@dataclass
class SemanticSearchConfig:
    """Configuration pour la recherche s√©mantique."""
    message_match_threshold: float = 0.7
    message_match_count: int = 5
    context_match_threshold: float = 0.6
    context_match_count: int = 3
    min_relevance_score: float = 0.5
    include_project_context: bool = True
    include_similar_messages: bool = True


class SemanticSearchService:
    """
    Service de recherche s√©mantique RAG pour enrichir les requ√™tes LLM.
    
    Fonctionnalit√©s principales:
    - Recherche multilingue par similarit√©
    - Enrichissement du contexte avec sources
    - Historique des conversations
    - Anti-hallucination via RAG
    """
    
    def __init__(self):
        """Initialise le service de recherche s√©mantique."""
        self.default_config = SemanticSearchConfig()
        self._openai_client: Optional[AsyncOpenAI] = None
    
    def _get_openai_client(self) -> AsyncOpenAI:
        """R√©cup√®re ou cr√©e le client OpenAI."""
        if self._openai_client is None:
            if not settings.openai_api_key:
                raise ValueError("OPENAI_API_KEY non configur√©e")
            self._openai_client = AsyncOpenAI(api_key=settings.openai_api_key)
        return self._openai_client
    
    async def initialize(self):
        """Initialise les services d√©pendants."""
        await vector_store_service.initialize()
        logger.info("‚úÖ Service de recherche s√©mantique initialis√©")
    
    async def _detect_language(self, text: str) -> str:
        """
        D√©tecte la langue du texte en utilisant le LLM OpenAI.
        
        Support de toutes les langues automatiquement (pas limit√© √† FR/EN/ES).
        Analyse TOUT le texte pour d√©tecter la langue MAJORITAIRE (pas juste le d√©but).
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Code langue ISO 639-1 ('fr', 'en', 'es', 'de', 'it', 'pt', 'ar', 'zh', 'ja', 'ru', etc.)
        """
        try:
            client = self._get_openai_client()
            
            # Analyser plus de texte pour avoir une vue globale (jusqu'√† 1000 caract√®res)
            text_sample = text[:1000] if len(text) > 500 else text
            
            logger.info(f"üåç D√©tection langue pour texte ({len(text)} caract√®res):")
            logger.info(f"   √âchantillon (100 car.): '{text_sample[:100]}...'")
            
            system_prompt = """Tu es un expert en d√©tection de langues. 
D√©tecte la langue PRINCIPALE du texte et retourne UNIQUEMENT le code ISO 639-1 (2 lettres).

R√àGLES CRITIQUES:
1. Analyse TOUT le texte fourni (pas seulement le d√©but)
2. Si le texte contient plusieurs langues, d√©tecte la langue MAJORITAIRE
3. Ignore les noms propres, mentions (@), URLs et mots techniques anglais
4. Focus sur les mots de fonction (articles, verbes, connecteurs) pour d√©terminer la langue

Exemples:
- "@vydata F√ºge bitte... mais ajoute aussi une fonctionnalit√©" ‚Üí 'de' (allemand majoritaire au d√©but)
- "Ajoute cette feature avec README" ‚Üí 'fr' (fran√ßais majoritaire, ignore les anglicismes)
- "Please add this fonctionnalit√©" ‚Üí 'en' (anglais majoritaire)

Codes ISO 639-1:
- fr (fran√ßais)
- en (anglais / english)
- es (espagnol / espa√±ol)
- de (allemand / deutsch)
- it (italien / italiano)
- pt (portugais / portugu√™s)
- ar (arabe / arabic)
- zh (chinois / chinese)
- ja (japonais / japanese)
- ru (russe / russian)
- nl (n√©erlandais / dutch)
- pl (polonais / polish)
- tr (turc / turkish)
- ko (cor√©en / korean)
- hi (hindi)
- sv (su√©dois / swedish)
- no (norv√©gien / norwegian)
- da (danois / danish)
- fi (finnois / finnish)

R√©ponds UNIQUEMENT avec les 2 lettres du code langue, rien d'autre."""

            response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Analyse TOUT ce texte et d√©tecte la langue MAJORITAIRE:\n\n{text_sample}"}
                ],
                temperature=0.0,
                max_tokens=5
            )
            
            detected_lang = response.choices[0].message.content.strip().lower()
            
            if len(detected_lang) == 2 and detected_lang.isalpha():
                logger.info(f"‚úÖ Langue MAJORITAIRE d√©tect√©e par LLM: {detected_lang}")
                logger.info(f"   Texte analys√© ({len(text_sample)} car.): '{text_sample[:80]}...'")
                return detected_lang
            else:
                logger.warning(f"‚ö†Ô∏è R√©ponse LLM invalide: '{detected_lang}' - fallback sur 'en'")
                logger.warning(f"   Texte qui a caus√© l'erreur: '{text_sample[:80]}...'")
                return 'en'
                
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection langue par LLM: {e}")
            logger.error(f"   Texte qui a caus√© l'erreur: '{text[:80]}...'")
            return 'en'
    
    async def store_user_message(
        self,
        message_text: str,
        monday_item_id: Optional[str] = None,
        monday_update_id: Optional[str] = None,
        task_id: Optional[int] = None,
        intent_type: Optional[str] = None,
        user_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> int:
        """
        Stocke un message utilisateur avec son embedding.
        
        Args:
            message_text: Texte du message
            monday_item_id: ID de l'item Monday.com
            monday_update_id: ID de l'update Monday.com
            task_id: ID de la t√¢che
            intent_type: Type d'intention ('question', 'command', etc.)
            user_id: ID de l'utilisateur
            metadata: M√©tadonn√©es additionnelles
            
        Returns:
            ID de l'enregistrement cr√©√©
        """
        language = await self._detect_language(message_text)
        
        cleaned_text = re.sub(r'<[^>]+>', '', message_text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        embedding_result = await embedding_service.generate_embedding(message_text)
        
        record_id = await vector_store_service.store_message_embedding(
            message_text=message_text,
            embedding=embedding_result.embedding,
            monday_item_id=monday_item_id,
            monday_update_id=monday_update_id,
            task_id=task_id,
            message_language=language,
            cleaned_text=cleaned_text,
            message_type="user_message",
            intent_type=intent_type,
            user_id=user_id,
            metadata=metadata or {}
        )
        
        logger.info(f"‚úÖ Message utilisateur stock√©: ID={record_id}, langue={language}")
        return record_id
    
    async def enrich_query_with_context(
        self,
        query: str,
        repository_url: Optional[str] = None,
        monday_item_id: Optional[str] = None,
        config: Optional[SemanticSearchConfig] = None
    ) -> EnrichedContext:
        """
        Enrichit une requ√™te avec du contexte pertinent (RAG).
        
        Args:
            query: Question ou commande de l'utilisateur
            repository_url: URL du repository pour filtrer le contexte
            monday_item_id: ID de l'item Monday pour filtrer les messages
            config: Configuration personnalis√©e
            
        Returns:
            EnrichedContext avec le contexte format√© et les sources
        """
        config = config or self.default_config
        
        similar_messages: List[SimilaritySearchResult] = []
        project_context: List[ContextSearchResult] = []
        
        tasks = []
        
        if config.include_similar_messages:
            tasks.append(
                vector_store_service.search_similar_messages(
                    query_text=query,
                    match_threshold=config.message_match_threshold,
                    match_count=config.message_match_count,
                    filter_item_id=monday_item_id
                )
            )
        else:
            tasks.append(asyncio.sleep(0))
        
        if config.include_project_context and repository_url:
            tasks.append(
                vector_store_service.search_project_context(
                    query_text=query,
                    repository_url=repository_url,
                    match_threshold=config.context_match_threshold,
                    match_count=config.context_match_count
                )
            )
        else:
            tasks.append(asyncio.sleep(0))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        if config.include_similar_messages and not isinstance(results[0], Exception):
            similar_messages = results[0]
        
        if config.include_project_context and len(results) > 1 and not isinstance(results[1], Exception):
            project_context = results[1]
        
        formatted_context = self._format_context(
            query=query,
            similar_messages=similar_messages,
            project_context=project_context
        )
        
        relevance_score = self._calculate_relevance_score(similar_messages, project_context)
        
        total_sources = len(similar_messages) + (len(project_context) if project_context else 0)
        
        logger.info(f"üîç Contexte enrichi: {total_sources} sources (score: {relevance_score:.2f})")
        logger.info(f"   ‚Ä¢ Messages similaires: {len(similar_messages)}")
        logger.info(f"   ‚Ä¢ Contexte projet: {len(project_context) if project_context else 0}")
        
        return EnrichedContext(
            query=query,
            similar_messages=similar_messages,
            project_context=project_context,
            formatted_context=formatted_context,
            total_sources=total_sources,
            relevance_score=relevance_score
        )
    
    def _format_context(
        self,
        query: str,
        similar_messages: List[SimilaritySearchResult],
        project_context: List[ContextSearchResult]
    ) -> str:
        """
        Formate le contexte pour le LLM.
        
        Args:
            query: Requ√™te de l'utilisateur
            similar_messages: Messages similaires trouv√©s
            project_context: Contexte du projet trouv√©
            
        Returns:
            Contexte format√© en markdown
        """
        context_parts = [
            "# CONTEXTE PERTINENT (RAG - Retrieval-Augmented Generation)",
            "",
            f"**Requ√™te:** {query}",
            ""
        ]
        
        if similar_messages:
            context_parts.append("## üìù Conversations Pr√©c√©dentes Similaires")
            context_parts.append("")
            
            for idx, result in enumerate(similar_messages[:3], 1):
                similarity_pct = result.similarity * 100
                context_parts.append(f"### Message {idx} (Similarit√©: {similarity_pct:.1f}%)")
                
                if result.record.message_language:
                    context_parts.append(f"**Langue:** {result.record.message_language}")
                if result.record.intent_type:
                    context_parts.append(f"**Type:** {result.record.intent_type}")
                if result.record.created_at:
                    context_parts.append(f"**Date:** {result.record.created_at.strftime('%Y-%m-%d %H:%M')}")

                text = result.record.cleaned_text or result.record.message_text
                context_parts.append(f"**Contenu:** {text[:500]}")
                context_parts.append("")
        
        if project_context:
            context_parts.append("## üìö Contexte du Projet")
            context_parts.append("")
            
            for idx, result in enumerate(project_context, 1):
                similarity_pct = result.similarity * 100
                context_parts.append(f"### Source {idx}: {result.context_type} (Similarit√©: {similarity_pct:.1f}%)")
                
                if result.file_path:
                    context_parts.append(f"**Fichier:** `{result.file_path}`")
                
                context_parts.append(f"**Contenu:** {result.context_text[:800]}")
                context_parts.append("")
        
        context_parts.extend([
            "---",
            "",
            "**INSTRUCTIONS:**",
            "- Utilise UNIQUEMENT les informations ci-dessus pour r√©pondre",
            "- Si les informations ne sont pas suffisantes, dis-le clairement",
            "- Ne fais PAS d'hallucinations ou d'inventions",
            "- Cite les sources quand tu utilises les informations",
            "- Adapte ta r√©ponse √† la langue de la requ√™te",
            ""
        ])
        
        return "\n".join(context_parts)
    
    def _calculate_relevance_score(
        self,
        similar_messages: List[SimilaritySearchResult],
        project_context: List[ContextSearchResult]
    ) -> float:
        """
        Calcule un score de pertinence global du contexte.
        
        Args:
            similar_messages: Messages similaires
            project_context: Contexte projet
            
        Returns:
            Score de 0.0 √† 1.0
        """
        if not similar_messages and not project_context:
            return 0.0
        
        total_score = 0.0
        total_weight = 0.0
        
        for result in similar_messages:
            total_score += result.similarity * 0.6
            total_weight += 0.6
        
        if project_context:
            for result in project_context:
                total_score += result.similarity * 0.4
                total_weight += 0.4
        
        if total_weight == 0:
            return 0.0
        
        return total_score / total_weight
    
    async def get_conversation_history(
        self,
        monday_item_id: str,
        limit: int = 10
    ) -> List[SimilaritySearchResult]:
        """
        R√©cup√®re l'historique des conversations pour un item Monday.
        
        Args:
            monday_item_id: ID de l'item Monday.com
            limit: Nombre maximum de messages
            
        Returns:
            Liste des messages de l'historique
        """
        results = await vector_store_service.search_similar_messages(
            query_text="historique conversation",
            match_threshold=0.0,
            match_count=limit,
            filter_item_id=monday_item_id
        )
        
        logger.info(f"üìã Historique r√©cup√©r√©: {len(results)} messages pour item {monday_item_id}")
        return results
    
    async def close(self):
        """Ferme les connexions."""
        await vector_store_service.close()
        logger.info("üîí Service de recherche s√©mantique ferm√©")

semantic_search_service = SemanticSearchService()
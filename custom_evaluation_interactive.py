#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Interface d'Ã©valuation interactive pour tester l'agent avec vos propres questions.

Utilisation:
    python3 custom_evaluation_interactive.py

Vous pourrez poser 5 questions et l'agent sera Ã©valuÃ© automatiquement.
"""

import asyncio
import json
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path

from services.evaluation.agent_evaluation_service import AgentEvaluationService
from models.evaluation_models import GoldenDatasetItem, EvaluationReport
from utils.logger import get_logger

logger = get_logger(__name__)


class InteractiveEvaluator:
    """
    Ã‰valuateur interactif pour tester l'agent avec vos propres questions.
    """
    
    def __init__(self):
        self.evaluation_service = AgentEvaluationService()
        self.questions: List[Dict[str, Any]] = []
        
    def collect_questions(self) -> None:
        """
        Collecte 5 questions de l'utilisateur de maniÃ¨re interactive.
        """
        print("\n" + "="*70)
        print("ğŸ¯ Ã‰VALUATION INTERACTIVE DE L'AGENT IA")
        print("="*70)
        print("\nğŸ“ Vous allez poser 5 questions Ã  l'agent.")
        print("ğŸ’¡ L'agent sera Ã©valuÃ© sur ses rÃ©ponses par un LLM Judge.")
        print("\n")
        
        # Configuration du repository par dÃ©faut
        print("ğŸ”§ CONFIGURATION")
        print("-" * 70)
        default_repo = "https://github.com/reharehavycode261/S2-GenericDAO"
        repo = input(f"Repository Ã  analyser [{default_repo}]: ").strip() or default_repo
        
        repo_name = repo.replace("https://github.com/", "")
        
        print(f"\nâœ… Repository: {repo}")
        print("\n" + "="*70)
        
        # Collecter les 5 questions
        for i in range(1, 6):
            print(f"\nğŸ“Œ QUESTION {i}/5")
            print("-" * 70)
            
            question = input(f"â“ Votre question #{i}: ").strip()
            
            if not question:
                print("âš ï¸  Question vide ignorÃ©e")
                continue
            
            # Demander le type de rÃ©ponse attendue (optionnel)
            print("\nğŸ’­ Que devrait rÃ©pondre l'agent idÃ©alement ?")
            print("   (Appuyez sur EntrÃ©e pour laisser le juge dÃ©cider)")
            expected = input("ğŸ“‹ RÃ©ponse attendue: ").strip()
            
            # DÃ©terminer le type de question
            question_lower = question.lower()
            if any(word in question_lower for word in ["commit", "pr", "pull request", "structure", "branch"]):
                q_type = "github_metadata"
            elif any(word in question_lower for word in ["crÃ©er", "ajouter", "implÃ©menter", "create", "add"]):
                q_type = "command"
            else:
                q_type = "question"
            
            self.questions.append({
                "id": f"custom_q{i:03d}",
                "type": "questions",
                "input_text": question,
                "input_context": {
                    "repository_url": repo,
                    "repository_name": repo_name,
                    "default_branch": "main"
                },
                "expected_output": expected or "RÃ©ponse claire et prÃ©cise basÃ©e sur le contexte du projet",
                "expected_output_metadata": {
                    "type": q_type,
                    "custom": True
                },
                "evaluation_criteria": [
                    "Pertinence de la rÃ©ponse",
                    "PrÃ©cision technique",
                    "ClartÃ© de l'explication",
                    "Utilisation du contexte du projet"
                ],
                "description": f"Question personnalisÃ©e #{i}: {question[:50]}..."
            })
            
            print(f"âœ… Question #{i} enregistrÃ©e")
        
        print("\n" + "="*70)
        print(f"âœ… {len(self.questions)} questions collectÃ©es")
        print("="*70)
    
    def save_custom_dataset(self) -> str:
        """
        Sauvegarde les questions dans un dataset personnalisÃ©.
        
        Returns:
            Chemin du fichier crÃ©Ã©
        """
        dataset = {
            "name": "Custom Interactive Dataset",
            "type": "questions",
            "description": "Dataset personnalisÃ© crÃ©Ã© via l'interface interactive",
            "version": "1.0.0",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "items": self.questions
        }
        
        # CrÃ©er le dossier si nÃ©cessaire
        custom_dir = Path("data/golden_datasets/custom")
        custom_dir.mkdir(parents=True, exist_ok=True)
        
        # Nom de fichier avec timestamp
        filename = f"custom_interactive_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = custom_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Dataset sauvegardÃ©: {filepath}")
        return str(filepath)
    
    async def run_evaluation(self, dataset_path: str) -> EvaluationReport:
        """
        Lance l'Ã©valuation avec le dataset personnalisÃ©.
        
        Args:
            dataset_path: Chemin du dataset Ã  Ã©valuer
            
        Returns:
            Rapport d'Ã©valuation complet
        """
        print("\n" + "="*70)
        print("ğŸš€ LANCEMENT DE L'Ã‰VALUATION")
        print("="*70)
        print("\nâ³ L'agent traite vos questions...")
        print("ğŸ’¡ Cela peut prendre 2-3 minutes\n")
        
        # Charger le dataset
        from models.evaluation_models import GoldenDataset
        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset_data = json.load(f)
        
        dataset = GoldenDataset(**dataset_data)
        
        # Lancer l'Ã©valuation
        report = await self.evaluation_service.evaluate_dataset(
            dataset_type="custom",
            save_report=True
        )
        
        return report
    
    def display_results(self, report: EvaluationReport) -> None:
        """
        Affiche les rÃ©sultats de l'Ã©valuation de maniÃ¨re claire.
        
        Args:
            report: Rapport d'Ã©valuation
        """
        print("\n" + "="*70)
        print("ğŸ“Š RÃ‰SULTATS DE L'Ã‰VALUATION")
        print("="*70)
        
        # Score global
        reliability = report.global_metrics.reliability_score
        status_emoji = "âœ…" if reliability >= 70 else "âš ï¸" if reliability >= 60 else "âŒ"
        
        print(f"\n{status_emoji} Score de fiabilitÃ©: {reliability:.1f}/100")
        print(f"ğŸ“ˆ Score moyen: {report.global_metrics.average_score:.1f}/100")
        print(f"âœ… Tests rÃ©ussis: {report.global_metrics.tests_passed}/{report.global_metrics.total_tests}")
        
        # CatÃ©gorisation
        if reliability >= 80:
            status = "ğŸŒŸ EXCELLENT"
        elif reliability >= 70:
            status = "âœ… BON"
        elif reliability >= 60:
            status = "âš ï¸  Ã€ AMÃ‰LIORER"
        else:
            status = "âŒ NON FIABLE"
        
        print(f"\nğŸ† Statut: {status}")
        
        # DÃ©tail par question
        print("\n" + "="*70)
        print("ğŸ“‹ DÃ‰TAIL DES RÃ‰PONSES")
        print("="*70)
        
        for i, result in enumerate(report.results, 1):
            passed_emoji = "âœ…" if result.passed else "âŒ"
            print(f"\n{passed_emoji} Question {i}: {result.item_id}")
            print(f"   Score: {result.score:.1f}/100")
            
            # Afficher la question
            for question in self.questions:
                if question["id"] == result.item_id:
                    print(f"\n   â“ Question: {question['input_text']}")
                    break
            
            # Afficher la rÃ©ponse de l'agent (tronquÃ©e)
            print(f"\n   ğŸ¤– RÃ©ponse de l'agent:")
            output_preview = result.agent_output[:200]
            if len(result.agent_output) > 200:
                output_preview += "..."
            print(f"   {output_preview}")
            
            # Jugement du LLM
            print(f"\n   ğŸ¯ Jugement:")
            reasoning_preview = result.reasoning[:150]
            if len(result.reasoning) > 150:
                reasoning_preview += "..."
            print(f"   {reasoning_preview}")
            
            # Scores par critÃ¨re
            print(f"\n   ğŸ“Š Scores dÃ©taillÃ©s:")
            for criterion, score in result.criteria_scores.items():
                bar = "â–ˆ" * int(score / 10) + "â–‘" * (10 - int(score / 10))
                print(f"      {criterion:20s}: {bar} {score:.0f}/100")
        
        print("\n" + "="*70)
        print("ğŸ’¡ RECOMMANDATIONS")
        print("="*70)
        
        if reliability < 60:
            print("âŒ L'agent nÃ©cessite des amÃ©liorations majeures")
            print("   â€¢ VÃ©rifier la rÃ©cupÃ©ration des donnÃ©es GitHub")
            print("   â€¢ AmÃ©liorer la comprÃ©hension du contexte")
            print("   â€¢ Renforcer la prÃ©cision technique")
        elif reliability < 70:
            print("âš ï¸  L'agent est fonctionnel mais peut Ãªtre amÃ©liorÃ©")
            print("   â€¢ Analyser les questions Ã©chouÃ©es")
            print("   â€¢ AmÃ©liorer la clartÃ© des rÃ©ponses")
        elif reliability < 80:
            print("âœ… L'agent fonctionne bien")
            print("   â€¢ Quelques ajustements mineurs possibles")
        else:
            print("ğŸŒŸ L'agent performe excellemment !")
            print("   â€¢ Maintenir la qualitÃ© actuelle")
        
        # Rapport dÃ©taillÃ© sauvegardÃ©
        print(f"\nğŸ“„ Rapport complet sauvegardÃ©:")
        print(f"   {report.report_id}")
        print("="*70 + "\n")
    
    async def run(self) -> None:
        """
        Lance le processus complet d'Ã©valuation interactive.
        """
        try:
            # 1. Collecter les questions
            self.collect_questions()
            
            if not self.questions:
                print("âŒ Aucune question fournie. Abandon.")
                return
            
            # 2. Sauvegarder le dataset
            dataset_path = self.save_custom_dataset()
            
            # 3. Demander confirmation
            print("\nğŸ”„ Voulez-vous lancer l'Ã©valuation maintenant ? (o/n)")
            confirm = input("ğŸ‘‰ ").strip().lower()
            
            if confirm not in ['o', 'oui', 'y', 'yes']:
                print("\nâœ‹ Ã‰valuation annulÃ©e.")
                print(f"ğŸ’¾ Vos questions sont sauvegardÃ©es dans: {dataset_path}")
                print("ğŸ’¡ Vous pouvez les Ã©valuer plus tard avec:")
                print(f"   curl -X POST http://localhost:8000/evaluation/run?dataset_path={dataset_path}")
                return
            
            # 4. Lancer l'Ã©valuation
            report = await self.run_evaluation(dataset_path)
            
            # 5. Afficher les rÃ©sultats
            self.display_results(report)
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  Ã‰valuation interrompue par l'utilisateur")
        except Exception as e:
            print(f"\nâŒ Erreur: {e}")
            logger.error(f"Erreur Ã©valuation interactive: {e}", exc_info=True)


async def main():
    """Point d'entrÃ©e principal."""
    evaluator = InteractiveEvaluator()
    await evaluator.run()


if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘          ğŸ¯ Ã‰VALUATION INTERACTIVE DE L'AGENT IA            â•‘
    â•‘                                                              â•‘
    â•‘  Ce systÃ¨me utilise les Golden Datasets pour Ã©valuer        â•‘
    â•‘  la fiabilitÃ© de l'agent sur VOS propres questions.         â•‘
    â•‘                                                              â•‘
    â•‘  ğŸ“ Posez 5 questions                                       â•‘
    â•‘  ğŸ¤– L'agent y rÃ©pond                                        â•‘
    â•‘  ğŸ‘¨â€âš–ï¸  Un LLM Judge Ã©value les rÃ©ponses                       â•‘
    â•‘  ğŸ“Š Vous obtenez un score de fiabilitÃ©                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    asyncio.run(main())


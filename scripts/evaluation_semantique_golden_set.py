#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
√âvaluation s√©mantique intelligente avec le Golden Set

Le LLM Judge compare chaque r√©ponse de l'agent avec TOUTES les r√©ponses
du Golden Set pour trouver les correspondances s√©mantiques.

Approche:
1. Pour chaque r√©ponse agent, chercher les r√©ponses Golden similaires
2. √âvaluer la correspondance s√©mantique
3. Donner un score global de qualit√©
"""

import sys
from pathlib import Path
import pandas as pd
import asyncio
from datetime import datetime
import json

sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.golden_dataset_manager import GoldenDatasetManager
from services.evaluation.llm_judge_service_simplified import LLMJudgeServiceSimplified
from utils.logger import get_logger

logger = get_logger(__name__)


async def charger_golden_set_reference():
    """Charge le Golden Set de r√©f√©rence (91 exemples parfaits)"""
    print("\nüìÇ Chargement du Golden Set de r√©f√©rence...")
    
    csv_path = Path(__file__).parent.parent / "data/golden_datasets/golden_sets.csv"
    
    if not csv_path.exists():
        print(f"‚ùå Fichier introuvable: {csv_path}")
        return None
    
    df = pd.read_csv(csv_path)
    print(f"‚úÖ {len(df)} exemples de r√©f√©rence charg√©s")
    
    return df


async def charger_reponses_agent():
    """Charge les vraies r√©ponses de l'agent depuis Monday"""
    print("\nüìÇ Chargement des r√©ponses de l'agent...")
    
    csv_path = Path(__file__).parent.parent / "data/golden_datasets/agent_interactions_log.csv"
    
    if not csv_path.exists():
        print(f"‚ùå Fichier introuvable: {csv_path}")
        return None
    
    df = pd.read_csv(csv_path)
    df_success = df[df['success'] == True].copy()
    
    # Nettoyer
    df_success['input_text'] = df_success['input_text'].fillna("")
    df_success['agent_output'] = df_success['agent_output'].fillna("")
    
    # Filtrer les vides
    df_success = df_success[
        (df_success['input_text'].str.len() > 10) & 
        (df_success['agent_output'].str.len() > 10)
    ]
    
    print(f"‚úÖ {len(df_success)} r√©ponses agent charg√©es")
    
    return df_success


async def evaluer_avec_correspondance_semantique(
    judge: LLMJudgeServiceSimplified,
    reponses_agent: pd.DataFrame,
    golden_set: pd.DataFrame
):
    """
    √âvalue les r√©ponses de l'agent en les comparant s√©mantiquement
    avec TOUTES les r√©ponses du Golden Set
    
    Args:
        judge: Service LLM Judge
        reponses_agent: DataFrame avec les r√©ponses de l'agent
        golden_set: DataFrame avec les 91 exemples de r√©f√©rence
        
    Returns:
        R√©sultat d'√©valuation avec score global
    """
    print(f"\n‚öñÔ∏è  √âvaluation s√©mantique intelligente...")
    print(f"   {len(reponses_agent)} r√©ponses agent vs {len(golden_set)} exemples Golden Set\n")
    
    # Construire le contexte pour le LLM
    evaluation_text = f"""Tu es un √©valuateur expert qui doit comparer les r√©ponses d'un agent IA avec une base de r√©f√©rence (Golden Set).

üìö GOLDEN SET (Base de r√©f√©rence - {len(golden_set)} exemples de r√©ponses PARFAITES):

"""
    
    # Ajouter un √©chantillon du Golden Set (limiter pour ne pas d√©passer le contexte)
    golden_sample_size = min(30, len(golden_set))  # Max 30 exemples pour le contexte
    for idx, row in golden_set.head(golden_sample_size).iterrows():
        evaluation_text += f"\n[Exemple Golden #{idx+1}]\n"
        evaluation_text += f"Question: {row['input']}\n"
        evaluation_text += f"R√©ponse parfaite: {row['output'][:200]}...\n"
    
    if len(golden_set) > golden_sample_size:
        evaluation_text += f"\n... et {len(golden_set) - golden_sample_size} autres exemples\n"
    
    evaluation_text += f"\n\n{'='*70}\n\n"
    evaluation_text += f"ü§ñ R√âPONSES DE L'AGENT √Ä √âVALUER ({len(reponses_agent)} r√©ponses):\n\n"
    
    # Ajouter les r√©ponses de l'agent
    for idx, row in reponses_agent.iterrows():
        evaluation_text += f"\n[R√©ponse Agent #{idx+1}]\n"
        evaluation_text += f"Question: {row['input_text'][:150]}\n"
        evaluation_text += f"R√©ponse: {row['agent_output'][:300]}...\n"
    
    # Instruction pour le LLM
    reference_output = f"""√âvalue la qualit√© GLOBALE de l'agent en comparant ses {len(reponses_agent)} r√©ponses avec la base Golden Set de {len(golden_set)} exemples.

M√âTHODOLOGIE D'√âVALUATION :

1. **Correspondance S√©mantique** (40 pts):
   - Pour chaque r√©ponse agent, cherche si une r√©ponse similaire existe dans le Golden Set
   - √âvalue si le contenu et la structure correspondent
   - Une r√©ponse agent peut correspondre √† plusieurs exemples Golden
   - Compte le nombre de correspondances trouv√©es

2. **Qualit√© du Contenu** (30 pts):
   - Les r√©ponses sont-elles compl√®tes et pr√©cises ?
   - Le niveau de d√©tail est-il comparable au Golden Set ?
   - Les informations sont-elles correctes ?

3. **Style et Format** (20 pts):
   - Le style de r√©ponse est-il coh√©rent avec le Golden Set ?
   - La structure est-elle claire et professionnelle ?
   - Le format (listes, √©tapes, exemples) est-il appropri√© ?

4. **Couverture des Connaissances** (10 pts):
   - L'agent d√©montre-t-il une connaissance comparable au Golden Set ?
   - Les r√©ponses couvrent-elles les domaines repr√©sent√©s dans le Golden Set ?

IMPORTANT:
- Ne cherche PAS de correspondance exacte (input ID)
- Cherche des correspondances S√âMANTIQUES (contenu similaire)
- Une r√©ponse agent peut correspondre partiellement √† plusieurs exemples Golden
- √âvalue la qualit√© globale, pas chaque r√©ponse individuellement

Donne UN SEUL score global /100."""
    
    try:
        result = await judge.evaluate_response(
            reference_input=evaluation_text,
            reference_output=reference_output,
            adam_response=f"Agent √©valu√© sur {len(reponses_agent)} r√©ponses vs {len(golden_set)} exemples Golden"
        )
        
        # Ajouter m√©tadonn√©es
        result['agent_responses_count'] = len(reponses_agent)
        result['golden_set_size'] = len(golden_set)
        result['evaluation_type'] = 'semantic_matching'
        
        return result
        
    except Exception as e:
        logger.error(f"Erreur lors de l'√©valuation: {e}", exc_info=True)
        return {
            "timestamp": datetime.now().isoformat(),
            "input_reference": f"{len(reponses_agent)} r√©ponses agent",
            "output_reference": reference_output,
            "agent_output": f"√âvaluation s√©mantique vs {len(golden_set)} exemples",
            "llm_score": 0.0,
            "llm_reasoning": f"Erreur: {str(e)}",
            "passed": False,
            "duration_seconds": None,
            "agent_responses_count": len(reponses_agent),
            "golden_set_size": len(golden_set),
            "evaluation_type": 'semantic_matching'
        }


async def main():
    """Fonction principale"""
    print("\n" + "="*70)
    print("üéØ √âVALUATION S√âMANTIQUE INTELLIGENTE")
    print("   Comparaison agent vs Golden Set (sans mapping IDs)")
    print("="*70)
    
    # 1. Charger le Golden Set de r√©f√©rence (91 exemples)
    golden_set = await charger_golden_set_reference()
    
    if golden_set is None or len(golden_set) == 0:
        print("‚ùå Golden Set vide ou introuvable")
        return
    
    # 2. Charger les r√©ponses r√©elles de l'agent
    reponses_agent = await charger_reponses_agent()
    
    if reponses_agent is None or len(reponses_agent) == 0:
        print("‚ùå Aucune r√©ponse agent disponible")
        return
    
    # 3. Initialiser les services
    print("\nüìÇ Initialisation du LLM Judge...")
    judge = LLMJudgeServiceSimplified(provider="anthropic")
    manager = GoldenDatasetManager()
    print("‚úÖ Services initialis√©s")
    
    # 4. √âvaluation s√©mantique
    result = await evaluer_avec_correspondance_semantique(
        judge, reponses_agent, golden_set
    )
    
    # 5. Afficher les r√©sultats
    print("\n" + "="*70)
    print("üìà R√âSULTAT DE L'√âVALUATION S√âMANTIQUE")
    print("="*70)
    
    print(f"\nüéØ Score Global: {result['llm_score']}/100")
    print(f"   Statut: {'‚úÖ PASS' if result['passed'] else '‚ùå FAIL'} (seuil: 70)")
    
    print(f"\nüìä D√©tails:")
    print(f"   ‚Ä¢ R√©ponses agent √©valu√©es: {result['agent_responses_count']}")
    print(f"   ‚Ä¢ Exemples Golden Set: {result['golden_set_size']}")
    print(f"   ‚Ä¢ Type d'√©valuation: {result['evaluation_type']}")
    
    print(f"\nüí° Raisonnement du LLM Judge:")
    reasoning_lines = result['llm_reasoning'].split('\n')
    for line in reasoning_lines[:15]:  # 15 premi√®res lignes
        if line.strip():
            print(f"   {line}")
    if len(reasoning_lines) > 15:
        print(f"   ... ({len(reasoning_lines) - 15} lignes suppl√©mentaires)")
    
    # 6. Sauvegarder
    print("\nüìÅ Sauvegarde du r√©sultat...")
    try:
        result['input_reference'] = f"√âvaluation s√©mantique: {result['agent_responses_count']} r√©ponses vs {result['golden_set_size']} exemples"
        result['agent_output'] = f"Correspondance s√©mantique intelligente"
        
        # Retirer m√©tadonn√©es non-CSV
        result_to_save = {k: v for k, v in result.items() if k not in ['agent_responses_count', 'golden_set_size', 'evaluation_type']}
        
        manager.save_evaluation_result(result_to_save)
        print("‚úÖ R√©sultat sauvegard√© dans evaluation_results.csv")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur sauvegarde: {e}")
    
    # 7. Interpr√©ter
    print("\n" + "="*70)
    print("üéØ INTERPR√âTATION")
    print("="*70)
    
    score = result['llm_score']
    
    if score >= 90:
        print("\nüåü EXCELLENT (90-100)")
        print("   L'agent produit des r√©ponses de qualit√© comparable au Golden Set.")
        print("   Les r√©ponses correspondent s√©mantiquement aux exemples de r√©f√©rence.")
    elif score >= 70:
        print("\n‚úÖ BIEN (70-89)")
        print("   L'agent produit de bonnes r√©ponses.")
        print("   Quelques diff√©rences avec le Golden Set mais qualit√© satisfaisante.")
    elif score >= 50:
        print("\n‚ö†Ô∏è  MOYEN (50-69)")
        print("   L'agent produit des r√©ponses correctes mais manque de profondeur.")
        print("   √âcart notable avec le niveau du Golden Set.")
    else:
        print("\n‚ùå INSUFFISANT (0-49)")
        print("   Les r√©ponses de l'agent ne correspondent pas au Golden Set.")
        print("   Am√©lioration majeure n√©cessaire.")
    
    print("\nüí° Avantages de cette approche:")
    print("   ‚Ä¢ Pas besoin de mapping exact des IDs")
    print("   ‚Ä¢ Correspondance s√©mantique intelligente")
    print("   ‚Ä¢ Une r√©ponse peut correspondre √† plusieurs exemples")
    print("   ‚Ä¢ √âvaluation de la qualit√© globale")
    
    print("\n" + "="*70)
    print("‚úÖ √âvaluation termin√©e !")
    print("="*70)


if __name__ == "__main__":
    asyncio.run(main())


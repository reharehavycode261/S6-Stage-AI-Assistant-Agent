#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test du Golden Dataset Multilingue avec RAG.

Ce script teste:
- Recherche s√©mantique multilingue
- √âvaluation enrichie avec contexte
- Comparaison des m√©thodes d'√©valuation
"""

import asyncio
import sys
from pathlib import Path

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.golden_dataset_rag_extension import golden_dataset_rag_extension
from utils.logger import get_logger

logger = get_logger(__name__)


async def test_multilingual_search():
    """Test de recherche multilingue."""
    print("\n" + "="*80)
    print("üåç TEST 1: RECHERCHE S√âMANTIQUE MULTILINGUE")
    print("="*80)
    
    test_cases = [
        {
            "query": "Comment fonctionne la validation humaine?",
            "language": "Fran√ßais",
            "expected_similarity": 0.7
        },
        {
            "query": "How does human validation work?",
            "language": "English",
            "expected_similarity": 0.7
        },
        {
            "query": "¬øC√≥mo funciona la validaci√≥n humana?",
            "language": "Espa√±ol",
            "expected_similarity": 0.65  # Peut-√™tre moins de correspondances
        },
        {
            "query": "‰∫∫Á±ªÈ™åËØÅÊòØÂ¶Ç‰ΩïÂ∑•‰ΩúÁöÑ?",
            "language": "Chinois",
            "expected_similarity": 0.60
        }
    ]
    
    for i, test in enumerate(test_cases, 1):
        print(f"\nüß™ Test Case {i}: {test['language']}")
        print(f"   Query: '{test['query']}'")
        print("-" * 80)
        
        try:
            similar_examples = await golden_dataset_rag_extension.find_similar_golden_examples(
                query=test['query'],
                top_k=3,
                match_threshold=0.5
            )
            
            if similar_examples:
                print(f"   ‚úÖ {len(similar_examples)} examples trouv√©s")
                for j, ex in enumerate(similar_examples, 1):
                    print(f"      {j}. Similarit√©: {ex['similarity_score']:.3f}")
                    print(f"         Langue: {ex['language']}")
                    print(f"         Input: {ex['input_reference'][:60]}...")
                    
                    if ex['similarity_score'] >= test['expected_similarity']:
                        print(f"         ‚úÖ Bonne similarit√© (>= {test['expected_similarity']})")
                    else:
                        print(f"         ‚ö†Ô∏è  Similarit√© faible (< {test['expected_similarity']})")
            else:
                print(f"   ‚ùå Aucun example trouv√©")
                
        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            return False
    
    return True


async def test_evaluation_with_context():
    """Test d'√©valuation enrichie avec contexte."""
    print("\n" + "="*80)
    print("üéØ TEST 2: √âVALUATION ENRICHIE AVEC CONTEXTE")
    print("="*80)
    
    # Simuler une r√©ponse d'agent
    agent_input = "Explique-moi comment fonctionne le workflow de l'agent"
    agent_output = """Le workflow de l'agent suit ces √©tapes:
1. R√©ception d'un webhook Monday.com
2. Classification de l'intention (question vs commande)
3. Exploration du repository si n√©cessaire
4. G√©n√©ration de la r√©ponse ou cr√©ation de PR
5. Validation humaine optionnelle
6. Mise √† jour de Monday.com"""
    
    print(f"\nüìù Agent Input: '{agent_input}'")
    print(f"üìù Agent Output: {agent_output[:100]}...")
    print("-" * 80)
    
    try:
        context = await golden_dataset_rag_extension.evaluate_with_similarity_context(
            agent_input=agent_input,
            agent_output=agent_output,
            find_similar=True,
            top_k=3
        )
        
        print(f"\n‚úÖ Contexte d'√©valuation g√©n√©r√©:")
        print(f"   ‚Ä¢ Langue d√©tect√©e: {context['input_language']}")
        print(f"   ‚Ä¢ Examples similaires trouv√©s: {context['similar_count']}")
        print(f"   ‚Ä¢ Similarit√© max: {context['max_similarity']:.3f}")
        
        if context['similar_golden_examples']:
            print(f"\nüìö Golden Examples Similaires:")
            for i, ex in enumerate(context['similar_golden_examples'], 1):
                print(f"   {i}. Similarit√©: {ex['similarity_score']:.3f}")
                print(f"      Input: {ex['input_reference'][:70]}...")
                print(f"      Expected Output: {ex['output_reference'][:70]}...")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_comparison_methods():
    """Test de comparaison des m√©thodes d'√©valuation."""
    print("\n" + "="*80)
    print("‚öñÔ∏è  TEST 3: COMPARAISON DES M√âTHODES D'√âVALUATION")
    print("="*80)
    
    agent_input = "Cr√©e un formulaire de login React"
    agent_output = "Voici un composant React avec formulaire de login incluant validation..."
    expected_output = "Un composant de login professionnel avec validation des champs..."
    
    print(f"\nüìù Input: '{agent_input}'")
    print("-" * 80)
    
    try:
        comparison = await golden_dataset_rag_extension.compare_evaluation_methods(
            agent_input=agent_input,
            agent_output=agent_output,
            expected_output=expected_output
        )
        
        print(f"\nüìä M√©thode Classique:")
        print(f"   ‚Ä¢ Input: {comparison['classic_evaluation']['input'][:60]}...")
        print(f"   ‚Ä¢ Output: {comparison['classic_evaluation']['output'][:60]}...")
        
        print(f"\nüìä M√©thode RAG Enrichie:")
        rag_eval = comparison['rag_enriched_evaluation']
        print(f"   ‚Ä¢ Input: {rag_eval['input'][:60]}...")
        print(f"   ‚Ä¢ Langue: {rag_eval['language']}")
        print(f"   ‚Ä¢ Examples similaires: {rag_eval['similar_examples_found']}")
        print(f"   ‚Ä¢ Similarit√© max: {rag_eval['max_similarity']:.3f}")
        
        print(f"\n‚úÖ Am√©liorations:")
        improvements = comparison['improvement']
        print(f"   ‚Ä¢ Contexte similaire disponible: {improvements['has_similar_context']}")
        print(f"   ‚Ä¢ Boost de similarit√© (>0.7): {improvements['similarity_boost']}")
        print(f"   ‚Ä¢ Langue d√©tect√©e: {improvements['language_detected']}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Fonction principale de test."""
    print("\n" + "="*80)
    print("üß™ TEST DU GOLDEN DATASET MULTILINGUE AVEC RAG")
    print("="*80)
    print("\nüí° Fonctionnalit√©s test√©es:")
    print("   ‚Ä¢ Recherche s√©mantique multilingue")
    print("   ‚Ä¢ √âvaluation enrichie avec contexte")
    print("   ‚Ä¢ Comparaison des m√©thodes d'√©valuation")
    
    results = {
        "multilingual_search": False,
        "evaluation_with_context": False,
        "comparison_methods": False
    }
    
    try:
        # Test 1
        results["multilingual_search"] = await test_multilingual_search()
        
        # Test 2
        results["evaluation_with_context"] = await test_evaluation_with_context()
        
        # Test 3
        results["comparison_methods"] = await test_comparison_methods()
        
        # R√©sum√©
        print("\n" + "="*80)
        print("üìã R√âSUM√â DES TESTS")
        print("="*80)
        
        for test_name, passed in results.items():
            status = "‚úÖ" if passed else "‚ùå"
            print(f"{status} {test_name.replace('_', ' ').title()}")
        
        all_passed = all(results.values())
        
        print("\n" + "="*80)
        if all_passed:
            print("üéâ TOUS LES TESTS R√âUSSIS !")
            print("="*80)
            print("\n‚úÖ Le syst√®me RAG multilingue pour golden dataset fonctionne correctement !")
            print("\nüìù Utilisation:")
            print("   from services.evaluation.golden_dataset_rag_extension import golden_dataset_rag_extension")
            print()
            print("   # Recherche d'examples similaires")
            print("   similar = await golden_dataset_rag_extension.find_similar_golden_examples(query)")
            print()
            print("   # √âvaluation enrichie")
            print("   context = await golden_dataset_rag_extension.evaluate_with_similarity_context(input, output)")
            print()
            return 0
        else:
            failed_count = sum(1 for r in results.values() if not r)
            print(f"‚ö†Ô∏è  {failed_count} TEST(S) √âCHOU√â(S)")
            print("="*80)
            return 1
            
    except Exception as e:
        print(f"\n‚ùå Erreur globale: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)


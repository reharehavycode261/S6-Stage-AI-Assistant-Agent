#!/usr/bin/env python3
"""
Analyse s√©mantique d√©taill√©e avec le LLM Judge.
Compare chaque interaction individuellement pour un verdict pr√©cis.
"""

import pandas as pd
import asyncio
from pathlib import Path
import sys

# Ajouter le chemin racine au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.llm_judge_service_simplified import LLMJudgeServiceSimplified
from utils.logger import get_logger

logger = get_logger(__name__)

async def evaluate_single_interaction(
    judge: LLMJudgeServiceSimplified,
    interaction_input: str,
    interaction_output: str,
    golden_set: pd.DataFrame,
    interaction_num: int
):
    """√âvalue une interaction contre le Golden Set."""
    
    print(f"\n{'='*80}")
    print(f"üîç √âVALUATION INTERACTION #{interaction_num}")
    print(f"{'='*80}")
    print(f"\nüìù Input: {interaction_input[:100]}...")
    print(f"‚úÖ Output: {interaction_output[:150]}...")
    
    # Pr√©parer le contexte pour le LLM
    golden_examples = "\n\n".join([
        f"Golden Example #{i+1}:\n"
        f"Input: {row['input_text_original']}\n"
        f"Output: {row['output'][:200]}..."
        for i, row in golden_set.head(20).iterrows()  # Top 20 exemples
    ])
    
    # Prompt pour √©valuation d√©taill√©e
    evaluation_prompt = f"""Tu es un √©valuateur expert. Analyse si cette interaction de l'agent correspond au Golden Set.

INTERACTION AGENT:
Input: {interaction_input}
Output: {interaction_output}

EXEMPLES DU GOLDEN SET (20 premiers):
{golden_examples}

√âVALUE:
1. Est-ce que l'input de l'agent correspond au PATTERN des inputs du Golden Set ?
2. Est-ce que l'output de l'agent correspond au STYLE des outputs du Golden Set ?
3. Est-ce que le DOMAINE de la question est le m√™me ?

R√©ponds en JSON avec:
{{
  "pattern_match": true/false,
  "style_match": true/false,
  "domain_match": true/false,
  "score": 0-100,
  "reasoning": "explication d√©taill√©e"
}}
"""

    reference_output = """√âvalue la COMPATIBILIT√â entre l'interaction agent et le Golden Set.
    
Score:
- 80-100: Parfaite correspondance (m√™me pattern, style, domaine)
- 60-79: Bonne correspondance (2/3 crit√®res)
- 40-59: Correspondance partielle (1/3 crit√®res)
- 0-39: Pas de correspondance (domaines diff√©rents)
"""
    
    try:
        result = await judge.evaluate_response(
            reference_input=evaluation_prompt,
            reference_output=reference_output,
            adam_response=f"Interaction: {interaction_input}\nR√©ponse: {interaction_output}"
        )
        
        print(f"\nüìä R√©sultat:")
        print(f"   Score: {result['llm_score']}/100")
        print(f"   Status: {'‚úÖ COMPATIBLE' if result['passed'] else '‚ùå INCOMPATIBLE'}")
        print(f"\nüí≠ Raisonnement:")
        reasoning_lines = result['llm_reasoning'].split('\n')[:5]
        for line in reasoning_lines:
            if line.strip():
                print(f"   {line.strip()[:100]}")
        
        return {
            'interaction_num': interaction_num,
            'score': result['llm_score'],
            'passed': result['passed'],
            'reasoning': result['llm_reasoning']
        }
        
    except Exception as e:
        logger.error(f"Erreur √©valuation interaction #{interaction_num}: {e}")
        return {
            'interaction_num': interaction_num,
            'score': 0,
            'passed': False,
            'reasoning': f"Erreur: {e}"
        }


async def main():
    print("\n" + "=" * 80)
    print("üéØ ANALYSE S√âMANTIQUE D√âTAILL√âE (LLM JUDGE)")
    print("   √âvaluation interaction par interaction")
    print("=" * 80)
    
    # Chemins
    base_path = Path(__file__).parent.parent / "data/golden_datasets"
    logs_path = base_path / "agent_interactions_log.csv"
    golden_path = base_path / "golden_sets_detailed.csv"
    
    # Charger les donn√©es
    print("\nüìÇ Chargement des donn√©es...")
    df_logs = pd.read_csv(logs_path)
    df_golden = pd.read_csv(golden_path)
    
    # Filtrer les interactions r√©elles
    df_real = df_logs.iloc[3:].copy()
    
    print(f"‚úÖ {len(df_real)} interactions r√©elles charg√©es")
    print(f"‚úÖ {len(df_golden)} exemples Golden Set charg√©s")
    
    # Initialiser le LLM Judge
    print("\nü§ñ Initialisation du LLM Judge...")
    judge = LLMJudgeServiceSimplified(provider="anthropic")
    print("‚úÖ LLM Judge pr√™t")
    
    # √âvaluer chaque interaction
    results = []
    
    for idx, row in df_real.iterrows():
        result = await evaluate_single_interaction(
            judge=judge,
            interaction_input=row['input_text'],
            interaction_output=row['agent_output'],
            golden_set=df_golden,
            interaction_num=idx - 2
        )
        results.append(result)
        
        # Pause pour √©viter rate limiting
        await asyncio.sleep(2)
    
    # Calculer les statistiques
    print("\n" + "=" * 80)
    print("üìä STATISTIQUES GLOBALES")
    print("=" * 80)
    
    total = len(results)
    compatible = sum(1 for r in results if r['passed'])
    incompatible = total - compatible
    avg_score = sum(r['score'] for r in results) / total if total > 0 else 0
    
    print(f"\nüìà R√©sum√©:")
    print(f"   ‚Ä¢ Total interactions: {total}")
    print(f"   ‚Ä¢ Compatible (‚â•70): {compatible} ({compatible/total*100:.1f}%)")
    print(f"   ‚Ä¢ Incompatible (<70): {incompatible} ({incompatible/total*100:.1f}%)")
    print(f"   ‚Ä¢ Score moyen: {avg_score:.1f}/100")
    
    # Verdict final
    print("\n" + "=" * 80)
    print("üéØ VERDICT FINAL")
    print("=" * 80)
    
    if incompatible >= total * 0.6:  # 60% ou plus incompatible
        print("\n‚ùå √âVALUATION GLOBALE: FAUSS√âE")
        print(f"   Raison: {incompatible}/{total} interactions ({incompatible/total*100:.0f}%)")
        print("   sont INCOMPATIBLES avec le Golden Set")
        print(f"\n   Score moyen: {avg_score:.1f}/100")
        print("   Score √©valuation r√©elle: 75/100")
        print("\n   ‚ö†Ô∏è  Le score de 75/100 est NON FIABLE car:")
        print("   ‚Ä¢ La majorit√© des questions ne correspondent pas au Golden Set")
        print("   ‚Ä¢ Les domaines sont diff√©rents")
        print("   ‚Ä¢ Pas de r√©f√©rence pour ces types de questions")
        verdict = "FAUSS√âE"
    elif incompatible >= total * 0.4:  # 40-60% incompatible
        print("\n‚ö†Ô∏è  √âVALUATION GLOBALE: PARTIELLEMENT FAUSS√âE")
        print(f"   Raison: {incompatible}/{total} interactions ({incompatible/total*100:.0f}%)")
        print("   ne correspondent pas au Golden Set")
        print(f"\n   Score moyen compatibilit√©: {avg_score:.1f}/100")
        print("   Score √©valuation r√©elle: 75/100")
        print("\n   ‚ö†Ô∏è  Le score de 75/100 est PARTIELLEMENT FIABLE:")
        print("   ‚Ä¢ Certaines questions correspondent")
        print("   ‚Ä¢ D'autres sont hors scope")
        print("   ‚Ä¢ Utiliser avec prudence")
        verdict = "PARTIELLEMENT FAUSS√âE"
    else:  # Moins de 40% incompatible
        print("\n‚úÖ √âVALUATION GLOBALE: VALIDE")
        print(f"   Raison: {compatible}/{total} interactions ({compatible/total*100:.0f}%)")
        print("   sont COMPATIBLES avec le Golden Set")
        print(f"\n   Score moyen compatibilit√©: {avg_score:.1f}/100")
        print("   Score √©valuation r√©elle: 75/100")
        print("\n   ‚úÖ Le score de 75/100 est FIABLE:")
        print("   ‚Ä¢ La majorit√© correspond au Golden Set")
        print("   ‚Ä¢ Les patterns sont similaires")
        print("   ‚Ä¢ L'√©valuation est repr√©sentative")
        verdict = "VALIDE"
    
    print("\n" + "=" * 80)
    print(f"üèÅ R√âSULTAT: {verdict}")
    print("=" * 80)
    
    # Recommandations
    print("\nüí° RECOMMANDATIONS:")
    
    if verdict == "FAUSS√âE":
        print("\n   ‚ùå Le syst√®me d'√©valuation actuel n'est PAS adapt√©")
        print("   ‚úÖ Actions recommand√©es:")
        print("      1. Cr√©er un nouveau Golden Set pour les analyses de projets")
        print("      2. Utiliser les logs r√©els comme base")
        print("      3. Ajouter 20-30 exemples de questions similaires")
        print("      4. S√©parer l'√©valuation en 2 cat√©gories distinctes")
    elif verdict == "PARTIELLEMENT FAUSS√âE":
        print("\n   ‚ö†Ô∏è  Le syst√®me d'√©valuation a des lacunes")
        print("   ‚úÖ Actions recommand√©es:")
        print("      1. Enrichir le Golden Set avec des exemples mixtes")
        print("      2. Documenter les limitations actuelles")
        print("      3. Ajouter au moins 10 exemples d'analyse de projets")
        print("      4. Suivre l'√©volution du score dans le temps")
    else:
        print("\n   ‚úÖ Le syst√®me d'√©valuation fonctionne correctement")
        print("   ‚úÖ Actions recommand√©es:")
        print("      1. Continuer avec le Golden Set actuel")
        print("      2. Monitorer les nouveaux types de questions")
        print("      3. Mettre √† jour r√©guli√®rement")
        print("      4. Maintenir la qualit√© des r√©ponses")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    asyncio.run(main())


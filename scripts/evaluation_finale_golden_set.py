#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
√âvaluation finale avec le Golden Set (structure Monday conforme)

√âvalue l'agent en comparant ses outputs avec le Golden Set de r√©f√©rence.
Donne UN SEUL SCORE GLOBAL /100 pour toutes les interactions.

Structure:
- input: updateMday_ITEM_ID
- output: Contenu de r√©f√©rence (texte ou JSON PR)
- type: "analysis" ou "pr"
"""

import sys
from pathlib import Path
import pandas as pd
import asyncio
from datetime import datetime
import json

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.golden_dataset_manager import GoldenDatasetManager
from services.evaluation.llm_judge_service_simplified import LLMJudgeServiceSimplified
from utils.logger import get_logger

logger = get_logger(__name__)


async def charger_golden_set():
    """Charge le Golden Set"""
    print("\nüìÇ Chargement du Golden Set...")
    
    csv_path = Path(__file__).parent.parent / "data/golden_datasets/golden_sets.csv"
    
    if not csv_path.exists():
        print(f"‚ùå Fichier introuvable: {csv_path}")
        print("üí° Ex√©cutez d'abord: python scripts/generer_golden_set_depuis_monday.py")
        return None
    
    df = pd.read_csv(csv_path)
    
    print(f"‚úÖ {len(df)} entr√©es charg√©es")
    print(f"   ‚Ä¢ Analyses: {len(df[df['type'] == 'analysis'])}")
    print(f"   ‚Ä¢ PR: {len(df[df['type'] == 'pr'])}")
    
    return df


async def charger_outputs_agent():
    """Charge les outputs r√©els de l'agent depuis agent_interactions_log.csv"""
    print("\nüìÇ Chargement des outputs de l'agent...")
    
    csv_path = Path(__file__).parent.parent / "data/golden_datasets/agent_interactions_log.csv"
    
    if not csv_path.exists():
        print(f"‚ùå Fichier introuvable: {csv_path}")
        return None
    
    df = pd.read_csv(csv_path)
    df_success = df[df['success'] == True].copy()
    
    # Cr√©er un mapping updateMday_ITEM_ID ‚Üí agent_output
    agent_outputs = {}
    for idx, row in df_success.iterrows():
        monday_item_id = row.get('monday_item_id', 'unknown')
        input_id = f"updateMday_{monday_item_id}"
        agent_outputs[input_id] = row.get('agent_output', '')
    
    print(f"‚úÖ {len(agent_outputs)} outputs r√©cup√©r√©s")
    
    return agent_outputs


async def evaluer_globalement(
    judge: LLMJudgeServiceSimplified,
    golden_set: pd.DataFrame,
    agent_outputs: dict
):
    """
    √âvalue GLOBALEMENT toutes les interactions
    
    Args:
        judge: Service LLM-as-judge
        golden_set: DataFrame du Golden Set
        agent_outputs: Dict mapping input_id ‚Üí agent_output
        
    Returns:
        Score global et reasoning
    """
    print(f"\n‚öñÔ∏è  √âvaluation globale de {len(golden_set)} interactions...")
    print("   (Le LLM compare les outputs agent vs golden set)\n")
    
    # Construire le texte pour le LLM Judge
    evaluation_text = """√âvalue la performance GLOBALE de l'agent IA en comparant ses outputs avec les outputs de r√©f√©rence (Golden Set).

Les outputs peuvent √™tre au format texte brut (analyses) ou contenir des informations sur des Pull Requests.

"""
    
    comparisons = []
    analyses_count = 0
    pr_count = 0
    
    for idx, row in golden_set.iterrows():
        input_id = row['input']
        golden_output = row['output']
        interaction_type = row['type']
        
        # R√©cup√©rer l'output de l'agent
        agent_output = agent_outputs.get(input_id, "NON TROUV√â")
        
        if interaction_type == 'analysis':
            analyses_count += 1
        else:
            pr_count += 1
        
        evaluation_text += f"\n{'='*70}\n"
        evaluation_text += f"Interaction #{idx+1} - Type: {interaction_type.upper()}\n"
        evaluation_text += f"{'='*70}\n"
        evaluation_text += f"Input ID: {input_id}\n\n"
        evaluation_text += f"üìã OUTPUT GOLDEN (R√©f√©rence attendue):\n{golden_output[:500]}...\n\n"
        evaluation_text += f"ü§ñ OUTPUT AGENT (Produit par l'IA):\n{agent_output[:500]}...\n\n"
        
        comparisons.append({
            'input_id': input_id,
            'type': interaction_type,
            'golden': golden_output,
            'agent': agent_output,
            'match': golden_output.strip().lower() == agent_output.strip().lower()
        })
    
    # Instruction pour le LLM Judge
    reference_output = f"""√âvalue la performance GLOBALE de l'agent sur ces {len(golden_set)} interactions ({analyses_count} analyses + {pr_count} PR).

Donne UN SEUL score global /100 qui √©value:

1. **Exactitude** (40 pts): Les outputs agent correspondent-ils aux golden outputs ?
   - Le contenu des r√©ponses est-il similaire ou identique ?
   - Les informations cl√©s sont-elles pr√©sentes (num√©ros de PR, branches, fichiers, etc.) ?

2. **Compl√©tude** (30 pts): Les outputs agent couvrent-ils tous les √©l√©ments du golden set ?

3. **Coh√©rence** (20 pts): Les outputs sont-ils coh√©rents entre eux ?

4. **Qualit√©** (10 pts): La pr√©sentation et la clart√© sont-elles bonnes ?

Le score doit refl√©ter la capacit√© de l'agent √† REPRODUIRE fid√®lement les outputs de r√©f√©rence."""
    
    try:
        result = await judge.evaluate_response(
            reference_input=evaluation_text,
            reference_output=reference_output,
            adam_response=f"Agent √©valu√© sur {len(golden_set)} interactions ({analyses_count} analyses, {pr_count} PR)"
        )
        
        # Ajouter les comparaisons dans le r√©sultat
        result['comparisons'] = comparisons
        result['analyses_count'] = analyses_count
        result['pr_count'] = pr_count
        
        return result
        
    except Exception as e:
        logger.error(f"Erreur lors de l'√©valuation globale: {e}", exc_info=True)
        return {
            "timestamp": datetime.now().isoformat(),
            "input_reference": f"{len(golden_set)} interactions",
            "output_reference": reference_output,
            "agent_output": "√âvaluation globale",
            "llm_score": 0.0,
            "llm_reasoning": f"Erreur: {str(e)}",
            "passed": False,
            "duration_seconds": None,
            "comparisons": comparisons,
            "analyses_count": analyses_count,
            "pr_count": pr_count
        }


async def main():
    """Fonction principale"""
    print("\n" + "="*70)
    print("üéØ √âVALUATION FINALE - Golden Set (Structure Monday)")
    print("="*70)
    
    # 1. Charger le Golden Set
    golden_set = await charger_golden_set()
    
    if golden_set is None or len(golden_set) == 0:
        print("‚ùå Golden Set vide ou introuvable")
        return
    
    # 2. Charger les outputs r√©els de l'agent
    agent_outputs = await charger_outputs_agent()
    
    if agent_outputs is None:
        print("‚ùå Outputs agent introuvables")
        return
    
    # 3. Afficher les interactions √† √©valuer
    print("\nüìù Interactions √† √©valuer:")
    for idx, row in golden_set.iterrows():
        type_icon = "üîç" if row['type'] == 'analysis' else "üîß"
        print(f"   {idx+1}. {type_icon} {row['input']} ({row['type']})")
    
    # 4. Initialiser le LLM Judge
    print("\nüìÇ Initialisation du LLM Judge...")
    judge = LLMJudgeServiceSimplified(provider="anthropic")
    manager = GoldenDatasetManager()
    print("‚úÖ Services initialis√©s")
    
    # 5. √âVALUATION GLOBALE
    result = await evaluer_globalement(judge, golden_set, agent_outputs)
    
    # 6. Afficher le r√©sultat
    print("\n" + "="*70)
    print("üìà R√âSULTAT DE L'√âVALUATION GLOBALE")
    print("="*70)
    
    print(f"\nüéØ Score Global: {result['llm_score']}/100")
    print(f"   Statut: {'‚úÖ PASS' if result['passed'] else '‚ùå FAIL'} (seuil: 70)")
    
    print(f"\nüìä D√©tails:")
    print(f"   ‚Ä¢ Analyses √©valu√©es: {result['analyses_count']}")
    print(f"   ‚Ä¢ PR √©valu√©es: {result['pr_count']}")
    print(f"   ‚Ä¢ Total: {len(golden_set)}")
    
    # Compter les correspondances exactes
    exact_matches = sum(1 for c in result['comparisons'] if c['match'])
    match_rate = (exact_matches / len(result['comparisons']) * 100) if result['comparisons'] else 0
    
    print(f"\nüîç Correspondances exactes: {exact_matches}/{len(result['comparisons'])} ({match_rate:.1f}%)")
    
    print(f"\nüí° Raisonnement du LLM Judge:")
    reasoning_lines = result['llm_reasoning'].split('\n')
    for line in reasoning_lines[:10]:  # 10 premi√®res lignes
        print(f"   {line}")
    if len(reasoning_lines) > 10:
        print(f"   ... ({len(reasoning_lines) - 10} lignes suppl√©mentaires)")
    
    # 7. Sauvegarder
    print("\nüìÅ Sauvegarde du r√©sultat...")
    try:
        result['input_reference'] = f"√âvaluation Golden Set: {len(golden_set)} interactions"
        result['agent_output'] = f"Performance globale ({result['analyses_count']} analyses, {result['pr_count']} PR)"
        
        # Retirer comparisons avant sauvegarde (trop volumineux pour CSV)
        result_to_save = {k: v for k, v in result.items() if k != 'comparisons'}
        
        manager.save_evaluation_result(result_to_save)
        print("‚úÖ R√©sultat sauvegard√© dans evaluation_results.csv")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur sauvegarde: {e}")
    
    # 8. Interpr√©ter le score
    print("\n" + "="*70)
    print("üéØ INTERPR√âTATION")
    print("="*70)
    
    score = result['llm_score']
    
    if score >= 90:
        print("\nüåü EXCELLENT (90-100)")
        print("   L'agent reproduit fid√®lement les outputs du Golden Set.")
        print("   Performance exceptionnelle !")
    elif score >= 70:
        print("\n‚úÖ BIEN (70-89)")
        print("   L'agent produit des outputs corrects et coh√©rents.")
        print("   Quelques ajustements mineurs possibles.")
    elif score >= 50:
        print("\n‚ö†Ô∏è  MOYEN (50-69)")
        print("   L'agent produit des r√©sultats partiellement corrects.")
        print("   Des am√©liorations sont n√©cessaires.")
    else:
        print("\n‚ùå INSUFFISANT (0-49)")
        print("   L'agent ne reproduit pas les outputs attendus.")
        print("   R√©vision majeure n√©cessaire.")
    
    print("\n" + "="*70)
    print("‚úÖ √âvaluation termin√©e !")
    print("="*70)
    
    print("\nüí° Prochaines √©tapes:")
    print("   1. Analyser le reasoning du LLM Judge")
    print("   2. Identifier les interactions mal √©valu√©es")
    print("   3. Am√©liorer les prompts ou le mod√®le")
    print("   4. Relancer l'√©valuation et comparer")


if __name__ == "__main__":
    asyncio.run(main())


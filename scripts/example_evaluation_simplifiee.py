#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script d'exemple : Utilisation du Golden Dataset simplifiÃ©

DÃ©montre comment:
1. Charger le Golden Dataset (input_reference + output_reference)
2. Simuler une rÃ©ponse de l'agent
3. Utiliser un LLM-as-judge pour Ã©valuer
4. Sauvegarder les rÃ©sultats
"""

import sys
from pathlib import Path

# Ajouter le rÃ©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.golden_dataset_manager import GoldenDatasetManager
from datetime import datetime
import random


def simulate_agent_response(input_text: str) -> str:
    """
    Simule une rÃ©ponse de l'agent (Ã  remplacer par votre vrai agent).
    
    Dans un vrai scÃ©nario, vous appelleriez votre agent ici.
    """
    # Pour la dÃ©mo, on retourne une rÃ©ponse simulÃ©e
    responses = [
        f"RÃ©ponse simulÃ©e pour: {input_text}",
        f"Traitement de: {input_text}",
        f"Analyse de: {input_text}"
    ]
    return random.choice(responses)


def simulate_llm_judge(agent_output: str, output_reference: str) -> dict:
    """
    Simule l'Ã©valuation par un LLM-as-judge.
    
    Dans un vrai scÃ©nario, vous appelleriez un LLM (Claude, GPT, etc.) ici
    avec un prompt demandant de comparer agent_output et output_reference.
    """
    # Pour la dÃ©mo, on retourne un score alÃ©atoire
    score = random.randint(60, 100)
    passed = score >= 70
    
    reasoning = f"L'agent a fourni une rÃ©ponse {'excellente' if score >= 90 else 'correcte' if score >= 70 else 'insuffisante'}. "
    reasoning += f"ComparÃ©e Ã  la rÃ©fÃ©rence attendue, la rÃ©ponse mÃ©rite un score de {score}/100."
    
    return {
        "score": float(score),
        "reasoning": reasoning,
        "passed": passed
    }


def main():
    """
    Fonction principale : DÃ©montre le workflow complet.
    """
    print("\n" + "="*70)
    print("ğŸ§ª DÃ‰MONSTRATION : Ã‰valuation avec Golden Dataset SimplifiÃ©")
    print("="*70)
    
    # 1. Initialiser le gestionnaire
    print("\nğŸ“‚ Ã‰tape 1: Initialisation du GoldenDatasetManager...")
    manager = GoldenDatasetManager()
    
    # 2. Charger le Golden Dataset
    print("\nğŸ“Š Ã‰tape 2: Chargement du Golden Dataset...")
    df_tests = manager.load_golden_sets()
    print(f"   âœ… {len(df_tests)} tests chargÃ©s")
    
    # 3. SÃ©lectionner un test (exemple: le premier)
    print("\nğŸ¯ Ã‰tape 3: SÃ©lection d'un test...")
    test_index = 0
    test = manager.get_test_by_index(test_index)
    
    print(f"\n   Input reference:")
    print(f"   â””â”€ {test['input_reference'][:100]}...")
    print(f"\n   Output reference (attendu):")
    print(f"   â””â”€ {test['output_reference'][:100]}...")
    
    # 4. Envoyer l'input au systÃ¨me (simulÃ©)
    print("\nğŸ¤– Ã‰tape 4: Envoi de l'input au systÃ¨me...")
    agent_output = simulate_agent_response(test['input_reference'])
    print(f"   âœ… RÃ©ponse de l'agent reÃ§ue")
    print(f"   â””â”€ {agent_output}")
    
    # 5. Ã‰valuation par LLM-as-judge
    print("\nâš–ï¸  Ã‰tape 5: Ã‰valuation par LLM-as-judge...")
    judge_result = simulate_llm_judge(agent_output, test['output_reference'])
    
    print(f"   ğŸ“Š Score: {judge_result['score']}/100")
    print(f"   âœ… Passed: {judge_result['passed']}")
    print(f"   ğŸ’­ Reasoning: {judge_result['reasoning']}")
    
    # 6. Sauvegarder le rÃ©sultat
    print("\nğŸ’¾ Ã‰tape 6: Sauvegarde du rÃ©sultat...")
    
    result = {
        "timestamp": datetime.now().isoformat(),
        "input_reference": test['input_reference'],
        "output_reference": test['output_reference'],
        "agent_output": agent_output,
        "llm_score": judge_result['score'],
        "llm_reasoning": judge_result['reasoning'],
        "passed": judge_result['passed'],
        "duration_seconds": 2.5  # SimulÃ©
    }
    
    manager.save_evaluation_result(result)
    print("   âœ… RÃ©sultat sauvegardÃ© dans evaluation_results.csv")
    
    # 7. Afficher les statistiques
    print("\nğŸ“ˆ Ã‰tape 7: Statistiques globales...")
    stats = manager.get_statistics_summary()
    
    if "message" in stats:
        print(f"   â„¹ï¸  {stats['message']}")
    else:
        print(f"   Total Ã©valuations: {stats['total_evaluations']}")
        print(f"   RÃ©ussis: {stats['passed']}")
        print(f"   Ã‰chouÃ©s: {stats['failed']}")
        print(f"   Taux de rÃ©ussite: {stats['pass_rate']}%")
        print(f"   Score moyen: {stats['avg_score']}/100")
    
    print("\n" + "="*70)
    print("âœ… DÃ©monstration terminÃ©e avec succÃ¨s!")
    print("="*70)
    print("\nğŸ“ Notes:")
    print("   â€¢ Ce script utilise des simulations pour la dÃ©mo")
    print("   â€¢ Dans un vrai scÃ©nario, remplacez les fonctions simulate_* par:")
    print("     - Votre agent VyData pour generate_response()")
    print("     - Un vrai LLM (Claude/GPT) pour llm_judge()")
    print("\nğŸ“š Documentation: data/golden_datasets/README_STRUCTURE_SIMPLIFIEE.md")
    print()


if __name__ == "__main__":
    main()


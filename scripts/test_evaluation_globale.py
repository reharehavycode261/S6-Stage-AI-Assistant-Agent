#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test d'√©valuation GLOBALE avec les VRAIES donn√©es Monday.com

Ce script √©value TOUTES les r√©ponses en une seule fois et donne UN SEUL SCORE GLOBAL.
"""

import sys
from pathlib import Path
import pandas as pd
import asyncio
from datetime import datetime

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.golden_dataset_manager import GoldenDatasetManager
from services.evaluation.llm_judge_service_simplified import LLMJudgeServiceSimplified
from utils.logger import get_logger

logger = get_logger(__name__)


async def charger_donnees_reelles():
    """Charge les vraies interactions depuis agent_interactions_log.csv"""
    print("\nüìÇ Chargement des vraies donn√©es Monday.com...")
    
    csv_path = Path(__file__).parent.parent / "data/golden_datasets/agent_interactions_log.csv"
    
    if not csv_path.exists():
        print(f"‚ùå Fichier introuvable: {csv_path}")
        return None
    
    df = pd.read_csv(csv_path)
    
    # Filtrer seulement les interactions r√©ussies
    df_success = df[df['success'] == True].copy()
    
    # Nettoyer les NaN
    df_success['input_text'] = df_success['input_text'].fillna("")
    df_success['agent_output'] = df_success['agent_output'].fillna("")
    
    # Filtrer les lignes vides
    df_success = df_success[
        (df_success['input_text'].str.len() > 10) & 
        (df_success['agent_output'].str.len() > 10)
    ]
    
    print(f"‚úÖ {len(df_success)} interactions r√©ussies charg√©es")
    
    return df_success


async def evaluer_globalement(
    judge: LLMJudgeServiceSimplified,
    interactions: pd.DataFrame
):
    """
    √âvalue TOUTES les interactions en une seule fois
    
    Args:
        judge: Service LLM-as-judge
        interactions: DataFrame avec toutes les interactions
        
    Returns:
        Score global et reasoning
    """
    print(f"\n‚öñÔ∏è  √âvaluation globale de {len(interactions)} interactions...")
    print("   (Le LLM analyse toutes les r√©ponses ensemble)\n")
    
    # Construire un texte avec toutes les interactions
    batch_input = "Voici plusieurs questions pos√©es par des utilisateurs et les r√©ponses g√©n√©r√©es par l'agent IA :\n\n"
    
    for i, row in interactions.iterrows():
        batch_input += f"=== Interaction {i+1} ===\n"
        batch_input += f"‚ùì Question: {row['input_text'][:200]}...\n"
        batch_input += f"ü§ñ R√©ponse: {row['agent_output'][:300]}...\n\n"
    
    # Instruction pour le LLM Judge
    reference_output = f"""√âvalue la qualit√© GLOBALE de l'agent sur ces {len(interactions)} interactions.

Donne UN SEUL score global /100 qui repr√©sente:
- La coh√©rence g√©n√©rale des r√©ponses
- Le niveau de d√©tail moyen
- La pr√©cision des informations
- L'utilit√© des r√©ponses
- La clart√© de communication

Le score doit refl√©ter la performance GLOBALE de l'agent, pas chaque r√©ponse individuellement."""
    
    try:
        result = await judge.evaluate_response(
            reference_input=batch_input,
            reference_output=reference_output,
            adam_response=f"Agent √©valu√© sur {len(interactions)} interactions"
        )
        
        return result
        
    except Exception as e:
        logger.error(f"Erreur lors de l'√©valuation globale: {e}", exc_info=True)
        return {
            "timestamp": datetime.now().isoformat(),
            "input_reference": f"{len(interactions)} interactions",
            "output_reference": reference_output,
            "agent_output": "√âvaluation globale",
            "llm_score": 0.0,
            "llm_reasoning": f"Erreur: {str(e)}",
            "passed": False,
            "duration_seconds": None
        }


async def main():
    """Fonction principale"""
    print("\n" + "="*70)
    print("üéØ √âVALUATION GLOBALE - Donn√©es r√©elles Monday.com")
    print("="*70)
    
    # 1. Charger les vraies donn√©es
    df_real = await charger_donnees_reelles()
    
    if df_real is None or len(df_real) == 0:
        print("‚ùå Aucune donn√©e r√©elle disponible")
        return
    
    # 2. Initialiser les services
    print("\nüìÇ Initialisation des services...")
    manager = GoldenDatasetManager()
    judge = LLMJudgeServiceSimplified(provider="anthropic")
    print("‚úÖ Services initialis√©s")
    
    # 3. Limiter le nombre d'interactions pour la d√©mo (optionnel)
    num_tests = min(5, len(df_real))
    df_to_evaluate = df_real.head(num_tests)
    
    print(f"\nüìä {num_tests} interactions √† √©valuer globalement")
    
    # Afficher la liste des questions
    print("\nüìù Questions √† √©valuer:")
    for i, row in df_to_evaluate.iterrows():
        print(f"   {i+1}. {row['input_text'][:80]}...")
    
    # 4. √âVALUATION GLOBALE (1 seul appel LLM)
    result = await evaluer_globalement(judge, df_to_evaluate)
    
    # 5. Afficher le r√©sultat global
    print("\n" + "="*70)
    print("üìà R√âSULTAT DE L'√âVALUATION GLOBALE")
    print("="*70)
    print(f"\nüéØ Score Global: {result['llm_score']}/100")
    print(f"   Statut: {'‚úÖ PASS' if result['passed'] else '‚ùå FAIL'} (seuil: 70)")
    print(f"\nüí° Raisonnement:")
    print(f"   {result['llm_reasoning']}")
    
    # 6. Sauvegarder le r√©sultat
    print("\nüìÅ Sauvegarde du r√©sultat...")
    try:
        # Ajouter des m√©tadonn√©es
        result['input_reference'] = f"√âvaluation globale de {num_tests} interactions"
        result['agent_output'] = f"Performance globale de l'agent"
        
        manager.save_evaluation_result(result)
        print("‚úÖ R√©sultat sauvegard√© dans evaluation_results.csv")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur sauvegarde: {e}")
    
    # 7. Statistiques
    print("\n" + "="*70)
    print("üìä STATISTIQUES")
    print("="*70)
    print(f"Interactions √©valu√©es: {num_tests}")
    print(f"Score global: {result['llm_score']}/100")
    print(f"Seuil de r√©ussite: 70/100")
    print(f"Performance: {'‚úÖ Satisfaisante' if result['passed'] else '‚ùå √Ä am√©liorer'}")
    
    if result['llm_score'] >= 90:
        print("\nüåü Excellent ! L'agent performe tr√®s bien.")
    elif result['llm_score'] >= 70:
        print("\n‚úÖ Bien ! L'agent r√©pond correctement aux attentes.")
    elif result['llm_score'] >= 50:
        print("\n‚ö†Ô∏è  Moyen. Des am√©liorations sont n√©cessaires.")
    else:
        print("\n‚ùå Insuffisant. L'agent n√©cessite des am√©liorations majeures.")
    
    print("\n" + "="*70)
    print("‚úÖ √âvaluation globale termin√©e !")
    print("="*70)


if __name__ == "__main__":
    asyncio.run(main())


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de test du systÃ¨me de scoring (LLM as Judge).
Teste avec les donnÃ©es rÃ©elles loggÃ©es.
"""

import sys
import asyncio
from pathlib import Path

# Ajouter le rÃ©pertoire racine au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.vydata_evaluator import VyDataEvaluator
from services.evaluation.golden_dataset_manager import GoldenDatasetManager
from services.evaluation.agent_output_logger import AgentOutputLogger
import pandas as pd
from utils.logger import get_logger

logger = get_logger(__name__)


async def test_scoring_with_real_data():
    """Teste le scoring avec les donnÃ©es rÃ©elles de l'agent."""
    
    print("\n" + "="*80)
    print("ğŸ§ª TEST DU SYSTÃˆME DE SCORING (LLM AS JUDGE)")
    print("="*80)
    
    # 1. Charger les donnÃ©es rÃ©elles loggÃ©es
    print("\nğŸ“Š Ã‰tape 1: Chargement des interactions rÃ©elles...")
    output_logger = AgentOutputLogger()
    
    try:
        df = pd.read_csv('data/golden_datasets/agent_interactions_log.csv')
        # Prendre seulement les interactions de type "analysis" (pas les tests)
        real_interactions = df[
            (df['interaction_type'] == 'analysis') & 
            (df['monday_item_id'].astype(str).str.len() > 8)  # IDs rÃ©els Monday
        ]
        
        if len(real_interactions) == 0:
            print("âš ï¸ Aucune interaction rÃ©elle trouvÃ©e")
            return
        
        print(f"âœ… {len(real_interactions)} interactions rÃ©elles trouvÃ©es")
        
    except Exception as e:
        print(f"âŒ Erreur chargement interactions: {e}")
        return
    
    # 2. Charger le golden dataset
    print("\nğŸ“š Ã‰tape 2: Chargement du golden dataset...")
    dataset_manager = GoldenDatasetManager()
    
    try:
        golden_df = pd.read_csv('data/golden_datasets/golden_sets.csv')
        print(f"âœ… {len(golden_df)} tests dans le golden dataset")
    except Exception as e:
        print(f"âŒ Erreur chargement golden dataset: {e}")
        return
    
    # 3. Initialiser l'Ã©valuateur (avec fallback OpenAI si Anthropic Ã©choue)
    print("\nğŸ¤– Ã‰tape 3: Initialisation de l'Ã©valuateur LLM...")
    
    evaluator = None
    try:
        # Essayer d'abord avec Anthropic
        evaluator = VyDataEvaluator(
            model_name="claude-3-5-sonnet-20241022",
            provider="anthropic"
        )
        print("âœ… Ã‰valuateur initialisÃ© avec Anthropic")
    except Exception as e:
        print(f"âš ï¸ Anthropic Ã©chouÃ©: {e}")
        print("ğŸ”„ Tentative avec OpenAI...")
        try:
            evaluator = VyDataEvaluator(
                model_name="gpt-4",
                provider="openai"
            )
            print("âœ… Ã‰valuateur initialisÃ© avec OpenAI (fallback)")
        except Exception as e2:
            print(f"âŒ OpenAI Ã©chouÃ© aussi: {e2}")
            return
    
    # 4. Tester avec la derniÃ¨re interaction rÃ©elle
    print("\nğŸ¯ Ã‰tape 4: Test de scoring sur la derniÃ¨re interaction...")
    
    last_interaction = real_interactions.iloc[-1]
    
    print(f"\nğŸ“ INTERACTION Ã€ Ã‰VALUER:")
    print(f"   â€¢ ID: {last_interaction['interaction_id']}")
    print(f"   â€¢ Date: {last_interaction['timestamp']}")
    print(f"   â€¢ Input: {last_interaction['input_text'][:100]}...")
    print(f"   â€¢ Output: {str(last_interaction['agent_output'])[:150]}...")
    print(f"   â€¢ DurÃ©e: {last_interaction['duration_seconds']}s")
    
    # Chercher un golden set similaire (pour avoir un expected_output)
    # Pour ce test, on va utiliser un golden set gÃ©nÃ©rique d'analyse
    test_golden = golden_df[golden_df['test_type'] == 'analysis'].iloc[0]
    
    print(f"\nğŸ“š GOLDEN SET DE RÃ‰FÃ‰RENCE (pour comparaison):")
    print(f"   â€¢ Test ID: {test_golden['test_id']}")
    print(f"   â€¢ Expected Output: {test_golden['expected_output'][:150]}...")
    
    # 5. Ã‰valuer
    print(f"\nğŸ” Ã‰tape 5: Ã‰valuation avec LLM as Judge...")
    print(f"â³ Patience, le LLM analyse la rÃ©ponse...")
    
    try:
        evaluation_result = evaluator.evaluate_response(
            test_id=str(last_interaction['interaction_id']),
            reference_input=str(last_interaction['input_text']),
            reference_output=test_golden['expected_output'],  # Golden set de rÃ©fÃ©rence
            agent_response=str(last_interaction['agent_output']),
            monday_update_id=str(last_interaction['monday_update_id'])
        )
        
        print("\n" + "="*80)
        print("âœ… RÃ‰SULTAT DE L'Ã‰VALUATION")
        print("="*80)
        
        print(f"\nğŸ“Š SCORE GLOBAL: {evaluation_result['llm_score']}/100")
        
        if evaluation_result.get('criteria_scores'):
            print(f"\nğŸ“ˆ SCORES PAR CRITÃˆRE:")
            for criterion, score in evaluation_result['criteria_scores'].items():
                print(f"   â€¢ {criterion.capitalize()}: {score}/100")
        
        print(f"\nğŸ’­ RAISONNEMENT:")
        print(f"{evaluation_result['llm_reasoning']}")
        
        print(f"\nâ±ï¸  DURÃ‰E: {evaluation_result['duration_seconds']:.1f}s")
        
        # DÃ©terminer le statut
        score = evaluation_result['llm_score']
        if score >= 90:
            status = "ğŸŸ¢ EXCELLENT"
        elif score >= 80:
            status = "ğŸŸ¡ BON"
        elif score >= 70:
            status = "ğŸŸ  ACCEPTABLE"
        else:
            status = "ğŸ”´ INSUFFISANT"
        
        print(f"\nğŸ¯ STATUT: {status}")
        
        # 6. Sauvegarder le rÃ©sultat
        print("\nğŸ’¾ Ã‰tape 6: Sauvegarde du rÃ©sultat...")
        
        try:
            dataset_manager.save_evaluation_result(
                test_id=str(last_interaction['interaction_id']),
                llm_score=evaluation_result['score'],
                llm_reasoning=evaluation_result['reasoning'],
                criteria_scores=evaluation_result.get('criteria_scores', {}),
                evaluation_duration=evaluation_result['duration_seconds']
            )
            print("âœ… RÃ©sultat sauvegardÃ© dans evaluation_results.csv")
        except Exception as e:
            print(f"âš ï¸ Erreur sauvegarde: {e}")
        
        print("\n" + "="*80)
        print("âœ… TEST TERMINÃ‰ AVEC SUCCÃˆS")
        print("="*80)
        
        return evaluation_result
        
    except Exception as e:
        print(f"\nâŒ ERREUR LORS DE L'Ã‰VALUATION: {e}")
        import traceback
        traceback.print_exc()
        return None


async def test_scoring_with_golden_set():
    """Teste le scoring avec un test du golden dataset."""
    
    print("\n" + "="*80)
    print("ğŸ§ª TEST AVEC UN CAS DU GOLDEN DATASET")
    print("="*80)
    
    # Charger le golden dataset
    try:
        golden_df = pd.read_csv('data/golden_datasets/golden_sets.csv')
        
        # Prendre le premier test d'analyse
        test = golden_df[golden_df['test_type'] == 'analysis'].iloc[0]
        
        print(f"\nğŸ“š Test sÃ©lectionnÃ©:")
        print(f"   â€¢ ID: {test['test_id']}")
        print(f"   â€¢ Input: {test['input_monday_update']}")
        print(f"   â€¢ Expected: {test['expected_output'][:100]}...")
        
        # Simuler une rÃ©ponse de l'agent (ici on utilise l'expected output avec une petite variation)
        agent_response = test['expected_output'] + " De plus, l'architecture suit les principes SOLID."
        
        print(f"\nğŸ¤– RÃ©ponse de l'agent (simulÃ©e):")
        print(f"   {agent_response[:150]}...")
        
        # Initialiser l'Ã©valuateur
        print("\nğŸ” Ã‰valuation...")
        
        try:
            evaluator = VyDataEvaluator(provider="openai", model_name="gpt-4")
        except:
            try:
                evaluator = VyDataEvaluator(provider="anthropic")
            except Exception as e:
                print(f"âŒ Impossible d'initialiser l'Ã©valuateur: {e}")
                return
        
        result = evaluator.evaluate_response(
            test_id=test['test_id'],
            reference_input=test['input_monday_update'],
            reference_output=test['expected_output'],
            agent_response=agent_response
        )
        
        print(f"\nâœ… Score: {result['llm_score']}/100")
        print(f"\nğŸ’­ Raisonnement:")
        print(f"{result['llm_reasoning']}")
        
        if result.get('criteria_scores'):
            print(f"\nğŸ“Š Scores par critÃ¨re:")
            for criterion, score in result['criteria_scores'].items():
                print(f"   â€¢ {criterion}: {score}/100")
        
        return result
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
        return None


async def main():
    """Point d'entrÃ©e principal."""
    
    print("\nğŸš€ DÃ‰MARRAGE DES TESTS DE SCORING")
    
    # Test 1: Avec donnÃ©es rÃ©elles
    print("\n" + "="*80)
    print("TEST 1: SCORING SUR DONNÃ‰ES RÃ‰ELLES")
    print("="*80)
    
    result1 = await test_scoring_with_real_data()
    
    # Test 2: Avec golden dataset
    print("\n\n" + "="*80)
    print("TEST 2: SCORING SUR GOLDEN DATASET")
    print("="*80)
    
    result2 = await test_scoring_with_golden_set()
    
    print("\n\n" + "="*80)
    print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
    print("="*80)
    
    if result1:
        print(f"\nâœ… Test 1 (DonnÃ©es rÃ©elles): Score = {result1['llm_score']}/100")
    else:
        print(f"\nâŒ Test 1 (DonnÃ©es rÃ©elles): Ã‰CHEC")
    
    if result2:
        print(f"âœ… Test 2 (Golden dataset): Score = {result2['llm_score']}/100")
    else:
        print(f"âŒ Test 2 (Golden dataset): Ã‰CHEC")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    asyncio.run(main())


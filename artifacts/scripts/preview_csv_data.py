#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pour prÃ©visualiser le contenu des fichiers CSV Golden Datasets.
Usage: python scripts/preview_csv_data.py
"""

import pandas as pd
from pathlib import Path

def preview_csv_data():
    """Affiche un aperÃ§u des donnÃ©es CSV."""
    
    # Chemins
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "data" / "golden_datasets"
    
    print("=" * 80)
    print("ğŸ“Š APERÃ‡U DES FICHIERS CSV - GOLDEN DATASETS")
    print("=" * 80)
    
    # 1. Golden Sets
    print("\nğŸ§ª FEUILLE 1 : GOLDEN_SETS (Tests de rÃ©fÃ©rence)")
    print("-" * 80)
    
    try:
        df_golden = pd.read_csv(data_dir / "golden_sets.csv")
        print(f"âœ… {len(df_golden)} tests chargÃ©s\n")
        
        # Statistiques
        print("ğŸ“ˆ Statistiques :")
        print(f"   â€¢ Tests d'analyse (type=analysis) : {len(df_golden[df_golden['test_type'] == 'analysis'])}")
        print(f"   â€¢ Tests de PR (type=pr) : {len(df_golden[df_golden['test_type'] == 'pr'])}")
        print(f"   â€¢ Tests actifs : {len(df_golden[df_golden['active'] == True])}")
        print(f"   â€¢ Tests haute prioritÃ© : {len(df_golden[df_golden['priority'] == 'high'])}")
        
        # AperÃ§u des premiers tests
        print("\nğŸ“‹ Premiers tests :")
        for idx, row in df_golden.head(3).iterrows():
            print(f"\n   [{row['test_id']}] {row['test_type'].upper()}")
            print(f"   Input: {row['input_monday_update'][:60]}...")
            print(f"   Expected: {row['expected_output'][:60]}...")
            print(f"   CritÃ¨res: {row['evaluation_criteria']}")
            print(f"   PrioritÃ©: {row['priority']} | Actif: {row['active']}")
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
    
    # 2. Evaluation Results
    print("\n\nğŸ“ˆ FEUILLE 2 : EVALUATION_RESULTS (RÃ©sultats d'Ã©valuation)")
    print("-" * 80)
    
    try:
        df_eval = pd.read_csv(data_dir / "evaluation_results.csv")
        print(f"âœ… {len(df_eval)} Ã©valuations chargÃ©es\n")
        
        # Statistiques
        print("ğŸ“Š Statistiques :")
        print(f"   â€¢ Tests PASS (score â‰¥70) : {len(df_eval[df_eval['status'] == 'PASS'])}")
        print(f"   â€¢ Tests FAIL (score <70) : {len(df_eval[df_eval['status'] == 'FAIL'])}")
        print(f"   â€¢ Taux de rÃ©ussite : {len(df_eval[df_eval['status'] == 'PASS']) / len(df_eval) * 100:.1f}%")
        print(f"   â€¢ Score LLM moyen : {df_eval['llm_score'].mean():.1f}/100")
        print(f"   â€¢ Score humain moyen : {df_eval['human_score'].dropna().mean():.1f}/100")
        print(f"   â€¢ Score final moyen : {df_eval['final_score'].mean():.1f}/100")
        print(f"   â€¢ DurÃ©e moyenne : {df_eval['duration_seconds'].mean():.1f}s")
        
        # Validations humaines
        print(f"\nğŸ™‹ Validation humaine :")
        print(f"   â€¢ ValidÃ©es : {len(df_eval[df_eval['human_validation_status'] == 'validated'])}")
        print(f"   â€¢ En attente : {len(df_eval[df_eval['human_validation_status'] == 'pending'])}")
        print(f"   â€¢ Ã€ revoir : {len(df_eval[df_eval['human_validation_status'] == 'to_review'])}")
        
        # Top 3 meilleurs scores
        print("\nğŸ† Top 3 meilleurs scores :")
        top3 = df_eval.nlargest(3, 'final_score')
        for idx, row in top3.iterrows():
            print(f"   â€¢ {row['test_id']} : {row['final_score']:.1f}/100 ({row['status']})")
        
        # Tests Ã©chouÃ©s
        failed = df_eval[df_eval['status'] == 'FAIL']
        if len(failed) > 0:
            print(f"\nâš ï¸ Tests Ã©chouÃ©s ({len(failed)}) :")
            for idx, row in failed.iterrows():
                print(f"   â€¢ {row['test_id']} : {row['final_score']:.1f}/100")
                print(f"     Raison: {row['llm_reasoning'][:80]}...")
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
    
    # 3. Performance Metrics
    print("\n\nğŸ“Š FEUILLE 3 : PERFORMANCE_METRICS (MÃ©triques quotidiennes)")
    print("-" * 80)
    
    try:
        df_metrics = pd.read_csv(data_dir / "performance_metrics.csv")
        print(f"âœ… {len(df_metrics)} jours de mÃ©triques chargÃ©s\n")
        
        # DerniÃ¨re semaine
        print("ğŸ“… DerniÃ¨re semaine (7 jours les plus rÃ©cents) :")
        recent = df_metrics.head(7)
        
        for idx, row in recent.iterrows():
            status_icon = {
                'excellent': 'ğŸŸ¢',
                'good': 'ğŸŸ¡',
                'needs_improvement': 'ğŸ”´'
            }.get(row['reliability_status'], 'âšª')
            
            print(f"\n   {row['metric_date']} {status_icon} {row['reliability_status'].upper()}")
            print(f"   â””â”€ Tests: {row['total_tests_run']} | Pass rate: {row['pass_rate_percent']}% | Score: {row['avg_final_score']:.1f}")
            if row['notes']:
                print(f"      Notes: {row['notes']}")
        
        # Tendances
        print("\nğŸ“ˆ Tendances :")
        print(f"   â€¢ Score final moyen (pÃ©riode) : {df_metrics['avg_final_score'].mean():.1f}/100")
        print(f"   â€¢ Taux de rÃ©ussite moyen : {df_metrics['pass_rate_percent'].mean():.1f}%")
        print(f"   â€¢ Jours excellents (â‰¥85) : {len(df_metrics[df_metrics['reliability_status'] == 'excellent'])}")
        print(f"   â€¢ Jours Ã  amÃ©liorer (<70) : {len(df_metrics[df_metrics['reliability_status'] == 'needs_improvement'])}")
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
    
    print("\n" + "=" * 80)
    print("âœ… AperÃ§u terminÃ© !")
    print("=" * 80)
    print("\nğŸ’¡ Pour convertir en Excel, lancez : python scripts/csv_to_excel.py")
    print("ğŸ“– Documentation complÃ¨te : data/golden_datasets/README_CSV.md\n")


if __name__ == "__main__":
    preview_csv_data()












#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
√âvaluation Enrichie avec RAG Multilingue.

Ce script utilise le nouveau LLMJudgeRAGEnriched pour √©valuer le golden dataset
avec enrichissement par recherche s√©mantique.

Comparaison:
- √âvaluation classique (sans contexte)
- √âvaluation enrichie RAG (avec examples similaires)
"""

import asyncio
import sys
from pathlib import Path
import pandas as pd
from datetime import datetime

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.golden_dataset_manager import GoldenDatasetManager
from services.evaluation.llm_judge_rag_enriched import LLMJudgeRAGEnriched
from services.evaluation.llm_judge_service_simplified import LLMJudgeServiceSimplified
from utils.logger import get_logger

logger = get_logger(__name__)


async def evaluer_avec_rag(
    judge_rag: LLMJudgeRAGEnriched,
    manager: GoldenDatasetManager,
    test_index: int = 0
) -> dict:
    """
    √âvalue un test sp√©cifique avec RAG enrichi.
    
    Args:
        judge_rag: Instance du LLM Judge enrichi RAG
        manager: Instance du Golden Dataset Manager
        test_index: Index du test √† √©valuer
        
    Returns:
        R√©sultat d'√©valuation enrichi
    """
    print(f"\n{'='*80}")
    print(f"üß™ TEST #{test_index + 1} - √âVALUATION AVEC RAG ENRICHI")
    print('='*80)
    
    # 1. R√©cup√©rer le test
    test = manager.get_test_by_index(test_index)
    
    print(f"\nüìù Input: {test['input_reference'][:100]}...")
    print(f"üìù Expected Output: {test['output_reference'][:150]}...")
    
    # 2. Simuler une r√©ponse d'agent (pour la d√©mo)
    # En production, ceci viendrait de l'agent r√©el
    agent_response = test['output_reference']  # Pour tester, on utilise la r√©ponse attendue
    
    print(f"\nü§ñ Agent Response: {agent_response[:150]}...")
    
    # 3. √âvaluation enrichie avec RAG
    print(f"\nüîç Recherche d'examples similaires dans le Golden Dataset...")
    
    result = await judge_rag.evaluate_response_with_rag(
        reference_input=test['input_reference'],
        reference_output=test['output_reference'],
        agent_response=agent_response,
        use_rag=True
    )
    
    # 4. Afficher les r√©sultats
    print(f"\nüìä R√©sultat de l'√âvaluation RAG:")
    print(f"   ‚Ä¢ Score: {result['score']}/100")
    print(f"   ‚Ä¢ RAG activ√©: {result['rag_enabled']}")
    print(f"   ‚Ä¢ Examples similaires trouv√©s: {result['rag_similar_count']}")
    print(f"   ‚Ä¢ Langue d√©tect√©e: {result['rag_language_detected']}")
    
    if result.get('rag_similar_examples'):
        print(f"\nüìö Examples similaires utilis√©s:")
        for i, ex in enumerate(result['rag_similar_examples'], 1):
            print(f"   {i}. Similarit√©: {ex['similarity']:.2f} | Langue: {ex['language']}")
            print(f"      Input: {ex['input'][:70]}...")
    
    print(f"\nüí° Raisonnement:")
    reasoning_lines = result['reasoning'].split('\n')
    for line in reasoning_lines[:10]:
        if line.strip():
            print(f"   {line}")
    
    return result


async def comparer_classic_vs_rag(
    judge_rag: LLMJudgeRAGEnriched,
    manager: GoldenDatasetManager,
    test_index: int = 0
) -> dict:
    """
    Compare l'√©valuation classique vs RAG enrichie.
    
    Args:
        judge_rag: Instance du LLM Judge enrichi RAG
        manager: Instance du Golden Dataset Manager
        test_index: Index du test
        
    Returns:
        Comparaison des r√©sultats
    """
    print(f"\n{'='*80}")
    print(f"‚öñÔ∏è  COMPARAISON: CLASSIC vs RAG ENRICHI")
    print('='*80)
    
    # 1. R√©cup√©rer le test
    test = manager.get_test_by_index(test_index)
    
    print(f"\nüìù Test: {test['input_reference'][:100]}...")
    
    # Simuler une r√©ponse d'agent
    agent_response = test['output_reference']
    
    # 2. Comparer les deux m√©thodes
    print(f"\n‚è≥ √âvaluation en cours (2 m√©thodes)...")
    
    comparison = await judge_rag.compare_classic_vs_rag(
        reference_input=test['input_reference'],
        reference_output=test['output_reference'],
        agent_response=agent_response
    )
    
    # 3. Afficher la comparaison
    print(f"\nüìä R√©sultats:")
    print(f"\n   üîπ M√âTHODE CLASSIQUE:")
    print(f"      ‚Ä¢ Score: {comparison['classic']['score']}/100")
    print(f"      ‚Ä¢ M√©thode: {comparison['classic']['method']}")
    
    print(f"\n   üî∏ M√âTHODE RAG ENRICHIE:")
    print(f"      ‚Ä¢ Score: {comparison['rag_enriched']['score']}/100")
    print(f"      ‚Ä¢ M√©thode: {comparison['rag_enriched']['method']}")
    print(f"      ‚Ä¢ Examples similaires: {comparison['rag_enriched']['similar_count']}")
    print(f"      ‚Ä¢ Langue: {comparison['rag_enriched']['language']}")
    print(f"      ‚Ä¢ Similarit√© max: {comparison['rag_enriched']['max_similarity']:.2f}")
    
    print(f"\n   üìà DIFF√âRENCE:")
    print(f"      ‚Ä¢ Delta de score: {comparison['difference']['score_delta']:+.1f}")
    print(f"      ‚Ä¢ Contexte fourni par RAG: {comparison['difference']['rag_provides_context']}")
    
    # 4. Comparaison des raisonnements
    print(f"\nüí° Raisonnement CLASSIQUE:")
    for line in comparison['classic']['reasoning'].split('\n')[:5]:
        if line.strip():
            print(f"   {line}")
    
    print(f"\nüí° Raisonnement RAG ENRICHI:")
    for line in comparison['rag_enriched']['reasoning'].split('\n')[:5]:
        if line.strip():
            print(f"   {line}")
    
    return comparison


async def evaluer_dataset_complet_avec_rag(
    judge_rag: LLMJudgeRAGEnriched,
    manager: GoldenDatasetManager,
    max_tests: int = 10
) -> dict:
    """
    √âvalue un ensemble de tests avec RAG.
    
    Args:
        judge_rag: Instance du LLM Judge enrichi RAG
        manager: Instance du Golden Dataset Manager
        max_tests: Nombre maximum de tests √† √©valuer
        
    Returns:
        Statistiques d'√©valuation
    """
    print(f"\n{'='*80}")
    print(f"üìä √âVALUATION COMPL√àTE DU DATASET AVEC RAG")
    print('='*80)
    
    # Charger le dataset
    df = manager.load_golden_sets()
    total_tests = min(len(df), max_tests)
    
    print(f"\nüìÇ Dataset: {len(df)} tests disponibles")
    print(f"   √âvaluation de {total_tests} tests...")
    
    results = []
    total_score_classic = 0
    total_score_rag = 0
    total_with_rag_context = 0
    
    for i in range(total_tests):
        try:
            test = manager.get_test_by_index(i)
            agent_response = test['output_reference']  # Simulation
            
            # √âvaluer avec RAG
            result_rag = await judge_rag.evaluate_response_with_rag(
                reference_input=test['input_reference'],
                reference_output=test['output_reference'],
                agent_response=agent_response,
                use_rag=True
            )
            
            # √âvaluer sans RAG (pour comparaison)
            result_classic = await judge_rag.evaluate_response_with_rag(
                reference_input=test['input_reference'],
                reference_output=test['output_reference'],
                agent_response=agent_response,
                use_rag=False
            )
            
            results.append({
                'test_index': i,
                'score_classic': result_classic['score'],
                'score_rag': result_rag['score'],
                'rag_similar_count': result_rag.get('rag_similar_count', 0),
                'rag_language': result_rag.get('rag_language_detected', 'en'),
                'delta': result_rag['score'] - result_classic['score']
            })
            
            total_score_classic += result_classic['score']
            total_score_rag += result_rag['score']
            
            if result_rag.get('rag_similar_count', 0) > 0:
                total_with_rag_context += 1
            
            print(f"   Test {i+1}/{total_tests}: Classic={result_classic['score']:.1f}, RAG={result_rag['score']:.1f} (Œî={result_rag['score'] - result_classic['score']:+.1f})")
            
        except Exception as e:
            print(f"   ‚ùå Erreur test {i+1}: {e}")
    
    # Statistiques finales
    avg_classic = total_score_classic / len(results) if results else 0
    avg_rag = total_score_rag / len(results) if results else 0
    improvement = avg_rag - avg_classic
    
    print(f"\n{'='*80}")
    print(f"üìà STATISTIQUES FINALES")
    print('='*80)
    print(f"\nüìä Scores Moyens:")
    print(f"   ‚Ä¢ Classique: {avg_classic:.1f}/100")
    print(f"   ‚Ä¢ RAG Enrichi: {avg_rag:.1f}/100")
    print(f"   ‚Ä¢ Am√©lioration: {improvement:+.1f} points")
    
    print(f"\nüîç Contexte RAG:")
    print(f"   ‚Ä¢ Tests avec contexte RAG: {total_with_rag_context}/{len(results)}")
    print(f"   ‚Ä¢ Taux d'utilisation RAG: {(total_with_rag_context/len(results)*100) if results else 0:.1f}%")
    
    return {
        'total_tests': len(results),
        'avg_score_classic': avg_classic,
        'avg_score_rag': avg_rag,
        'improvement': improvement,
        'rag_usage_rate': (total_with_rag_context/len(results)) if results else 0,
        'results': results
    }


async def main():
    """Fonction principale."""
    print("\n" + "="*80)
    print("üåç √âVALUATION ENRICHIE AVEC RAG MULTILINGUE")
    print("="*80)
    print("\nüí° Fonctionnalit√©s:")
    print("   ‚Ä¢ Recherche d'examples similaires dans le Golden Dataset")
    print("   ‚Ä¢ √âvaluation enrichie avec contexte multilingue")
    print("   ‚Ä¢ Comparaison Classic vs RAG")
    
    # Initialiser les services
    print("\nüìÇ Initialisation des services...")
    manager = GoldenDatasetManager()
    judge_rag = LLMJudgeRAGEnriched(
        provider="anthropic",
        use_rag=True,
        rag_top_k=3,
        rag_threshold=0.6
    )
    print("‚úÖ Services initialis√©s")
    
    # Menu
    print("\n" + "="*80)
    print("Choisissez une option:")
    print("1. √âvaluer un test sp√©cifique avec RAG")
    print("2. Comparer Classic vs RAG (1 test)")
    print("3. √âvaluer le dataset complet avec RAG (10 tests)")
    print("4. Quitter")
    print("="*80)
    
    choice = input("\nVotre choix (1-4): ").strip()
    
    try:
        if choice == "1":
            test_index = int(input("Index du test (0-based): ").strip())
            await evaluer_avec_rag(judge_rag, manager, test_index)
            
        elif choice == "2":
            test_index = int(input("Index du test (0-based): ").strip())
            await comparer_classic_vs_rag(judge_rag, manager, test_index)
            
        elif choice == "3":
            max_tests = int(input("Nombre de tests √† √©valuer (max 50): ").strip())
            max_tests = min(max_tests, 50)
            await evaluer_dataset_complet_avec_rag(judge_rag, manager, max_tests)
            
        elif choice == "4":
            print("\nüëã Au revoir!")
            return 0
            
        else:
            print("‚ùå Choix invalide")
            return 1
        
        print("\n" + "="*80)
        print("‚úÖ √âVALUATION TERMIN√âE")
        print("="*80)
        print("\nüìù Note: L'√©valuation RAG enrichie:")
        print("   ‚Ä¢ Recherche automatiquement des examples similaires")
        print("   ‚Ä¢ D√©tecte la langue du test")
        print("   ‚Ä¢ Fournit un contexte au LLM Judge")
        print("   ‚Ä¢ Am√©liore la pr√©cision de l'√©valuation")
        print()
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrompu par l'utilisateur")
        return 1
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)


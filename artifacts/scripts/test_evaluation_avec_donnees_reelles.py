#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test du systÃ¨me d'Ã©valuation avec les VRAIES donnÃ©es Monday.com

Ce script:
1. Charge les vraies interactions depuis agent_interactions_log.csv
2. Les transforme en format golden dataset
3. Teste le systÃ¨me LLM-as-judge avec ces donnÃ©es rÃ©elles
4. Sauvegarde les rÃ©sultats
"""

import sys
from pathlib import Path
import pandas as pd
import asyncio
from datetime import datetime

# Ajouter le rÃ©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.golden_dataset_manager import GoldenDatasetManager
from services.evaluation.llm_judge_service_simplified import LLMJudgeServiceSimplified
from utils.logger import get_logger

logger = get_logger(__name__)


async def charger_donnees_reelles():
    """
    Charge les vraies interactions depuis agent_interactions_log.csv
    
    Returns:
        DataFrame avec les colonnes: input_text, agent_output, success
    """
    print("\nğŸ“‚ Chargement des vraies donnÃ©es Monday.com...")
    
    csv_path = Path(__file__).parent.parent / "data/golden_datasets/agent_interactions_log.csv"
    
    if not csv_path.exists():
        print(f"âŒ Fichier introuvable: {csv_path}")
        return None
    
    df = pd.read_csv(csv_path)
    
    # Filtrer seulement les interactions rÃ©ussies
    df_success = df[df['success'] == True].copy()
    
    # Nettoyer les NaN
    df_success['input_text'] = df_success['input_text'].fillna("")
    df_success['agent_output'] = df_success['agent_output'].fillna("")
    
    # Filtrer les lignes vides
    df_success = df_success[
        (df_success['input_text'].str.len() > 10) & 
        (df_success['agent_output'].str.len() > 10)
    ]
    
    print(f"âœ… {len(df_success)} interactions rÃ©ussies chargÃ©es")
    print(f"   Total original: {len(df)} interactions")
    print(f"   FiltrÃ©es: {len(df) - len(df_success)} (Ã©checs ou donnÃ©es vides)")
    
    return df_success


async def evaluer_interaction_reelle(
    judge: LLMJudgeServiceSimplified,
    input_text: str,
    agent_output: str,
    index: int,
    total: int,
    verbose: bool = False
):
    """
    Ã‰value une interaction rÃ©elle avec le LLM-as-judge
    
    Args:
        judge: Service LLM-as-judge
        input_text: Question de l'utilisateur
        agent_output: RÃ©ponse gÃ©nÃ©rÃ©e par l'agent
        index: Index de l'interaction
        total: Nombre total de tests
        verbose: Afficher les dÃ©tails (False par dÃ©faut)
        
    Returns:
        RÃ©sultat de l'Ã©valuation
    """
    # Affichage simplifiÃ© : juste une ligne de progression
    print(f"ğŸ§ª Test {index + 1}/{total}... ", end='', flush=True)
    
    # Pour l'Ã©valuation, l'output_reference est le mÃªme que agent_output
    # car ce sont des donnÃ©es rÃ©elles, on teste juste si le systÃ¨me fonctionne
    # On pourrait aussi demander au LLM de juger la qualitÃ© de la rÃ©ponse de faÃ§on absolue
    
    # Option 1: Ã‰valuation "neutre" - l'output est-il de bonne qualitÃ© ?
    output_reference = "Une rÃ©ponse complÃ¨te, prÃ©cise et actionnnable qui rÃ©pond correctement Ã  la question de l'utilisateur."
    
    try:
        result = await judge.evaluate_response(
            reference_input=input_text,
            reference_output=output_reference,
            adam_response=agent_output
        )
        
        # Afficher juste le rÃ©sultat
        status = "âœ… PASS" if result['passed'] else "âŒ FAIL"
        print(f"{status} (Score: {result['llm_score']}/100)")
        
        # Le rÃ©sultat est dÃ©jÃ  au bon format dict
        return result
        
    except Exception as e:
        print(f"âŒ ERREUR")
        logger.error(f"Erreur lors de l'Ã©valuation: {e}", exc_info=True)
        return {
            "timestamp": datetime.now().isoformat(),
            "input_reference": input_text,
            "output_reference": output_reference,
            "agent_output": agent_output,
            "llm_score": 0.0,
            "llm_reasoning": f"Erreur: {str(e)}",
            "passed": False,
            "duration_seconds": None
        }


async def main():
    """
    Fonction principale
    """
    print("\n" + "="*70)
    print("ğŸ¯ TEST D'Ã‰VALUATION AVEC DONNÃ‰ES RÃ‰ELLES MONDAY.COM")
    print("="*70)
    
    # 1. Charger les vraies donnÃ©es
    df_real = await charger_donnees_reelles()
    
    if df_real is None or len(df_real) == 0:
        print("âŒ Aucune donnÃ©e rÃ©elle disponible")
        return
    
    # 2. Initialiser les services
    print("\nğŸ“‚ Initialisation des services...")
    manager = GoldenDatasetManager()
    judge = LLMJudgeServiceSimplified(provider="anthropic")
    print("âœ… Services initialisÃ©s")
    
    # 3. Tester sur quelques interactions (limiter pour la dÃ©mo)
    num_tests = min(5, len(df_real))  # Max 5 tests
    print(f"\nğŸ§ª Ã‰valuation de {num_tests} interactions rÃ©elles...\n")
    
    results = []
    passed = 0
    failed = 0
    total_score = 0
    
    for i in range(num_tests):
        row = df_real.iloc[i]
        
        result = await evaluer_interaction_reelle(
            judge=judge,
            input_text=row['input_text'],
            agent_output=row['agent_output'],
            index=i,
            total=num_tests,
            verbose=False  # Mode silencieux
        )
        
        results.append(result)
        
        if result['passed']:
            passed += 1
        else:
            failed += 1
        
        total_score += result['llm_score']
        
        # Sauvegarder le rÃ©sultat (silencieusement)
        try:
            manager.save_evaluation_result(result)
        except Exception as e:
            logger.error(f"Erreur sauvegarde: {e}")
    
    # 4. Afficher le rÃ©sumÃ©
    print("\n" + "="*70)
    print("ğŸ“ˆ RÃ‰SUMÃ‰ DE L'Ã‰VALUATION")
    print("="*70)
    print(f"Total de tests: {num_tests}")
    print(f"âœ… RÃ©ussis: {passed}")
    print(f"âŒ Ã‰chouÃ©s: {failed}")
    print(f"ğŸ“Š Score moyen: {total_score/num_tests if num_tests > 0 else 0:.1f}/100")
    print(f"ğŸ¯ Taux de rÃ©ussite: {(passed/num_tests*100) if num_tests > 0 else 0:.1f}%")
    
    # 5. Statistiques globales
    print("\nğŸ“Š Statistiques globales (toutes les Ã©valuations):")
    stats = manager.get_statistics_summary()
    
    if "message" in stats:
        print(f"   {stats['message']}")
    else:
        print(f"   Total Ã©valuations: {stats['total_evaluations']}")
        print(f"   RÃ©ussis: {stats['passed']}")
        print(f"   Ã‰chouÃ©s: {stats['failed']}")
        print(f"   Taux de rÃ©ussite: {stats['pass_rate']}%")
        print(f"   Score moyen: {stats['avg_score']}/100")
    
    print("\n" + "="*70)
    print("âœ… Test terminÃ© avec les donnÃ©es rÃ©elles !")
    print("="*70)
    print("\nğŸ“ Notes:")
    print("   â€¢ Les donnÃ©es proviennent de agent_interactions_log.csv")
    print("   â€¢ Ce sont de VRAIES interactions avec Monday.com")
    print("   â€¢ Les rÃ©sultats sont sauvegardÃ©s dans evaluation_results.csv")
    print(f"   â€¢ {len(df_real)} interactions disponibles au total")
    print()


if __name__ == "__main__":
    asyncio.run(main())


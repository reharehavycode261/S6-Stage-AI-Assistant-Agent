#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Exemple complet d'Ã©valuation avec LLM-as-judge simplifiÃ©.

Ce script dÃ©montre comment utiliser le systÃ¨me d'Ã©valuation complet:
1. Charger le Golden Dataset (input_reference, output_reference)
2. Envoyer l'input_reference au systÃ¨me pour obtenir une rÃ©ponse
3. Ã‰valuer la rÃ©ponse avec le LLM-as-judge
4. Sauvegarder les rÃ©sultats
"""

import sys
import asyncio
from pathlib import Path

# Ajouter le rÃ©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.evaluation.golden_dataset_manager import GoldenDatasetManager
from services.evaluation.llm_judge_service_simplified import LLMJudgeServiceSimplified
from datetime import datetime
import time


async def simulate_agent_response(input_text: str) -> str:
    """
    Simule une rÃ©ponse de l'agent VyData.
    
    Dans un vrai scÃ©nario, vous appelleriez votre agent ici.
    Pour la dÃ©mo, on retourne une rÃ©ponse simulÃ©e.
    """
    # Pour la dÃ©mo, retourner une rÃ©ponse basique
    await asyncio.sleep(0.5)  # Simuler le temps de traitement
    
    if "hello" in input_text.lower():
        return "Bonjour ! ğŸ‘‹ Je suis VyData, votre assistant IA. Comment puis-je vous aider ?"
    elif "main.py" in input_text.lower():
        return "Le fichier main.py contient une API FastAPI avec plusieurs endpoints."
    else:
        return f"J'ai analysÃ© votre demande : {input_text}"


async def evaluate_single_test(
    manager: GoldenDatasetManager,
    judge: LLMJudgeServiceSimplified,
    test_index: int
) -> dict:
    """
    Ã‰value un test individuel.
    
    Args:
        manager: Gestionnaire du Golden Dataset
        judge: Service LLM-as-judge
        test_index: Index du test Ã  Ã©valuer
        
    Returns:
        RÃ©sultat de l'Ã©valuation
    """
    # 1. RÃ©cupÃ©rer le test
    test = manager.get_test_by_index(test_index)
    
    print(f"\n{'='*70}")
    print(f"ğŸ§ª Test #{test_index + 1}")
    print(f"{'='*70}")
    print(f"\nğŸ“ Input: {test['input_reference'][:100]}...")
    print(f"ğŸ“„ Output attendu: {test['output_reference'][:100]}...")
    
    # 2. Obtenir la rÃ©ponse de l'agent
    print(f"\nğŸ¤– Envoi de l'input au systÃ¨me...")
    start_time = time.time()
    agent_response = await simulate_agent_response(test['input_reference'])
    duration = time.time() - start_time
    
    print(f"âœ… RÃ©ponse reÃ§ue ({duration:.2f}s):")
    print(f"   {agent_response[:150]}...")
    
    # 3. Ã‰valuer avec le LLM-as-judge
    print(f"\nâš–ï¸  Ã‰valuation par LLM-as-judge...")
    result = await judge.evaluate_response(
        reference_input=test['input_reference'],
        reference_output=test['output_reference'],
        adam_response=agent_response
    )
    
    # Ajouter la durÃ©e
    result['duration_seconds'] = duration
    
    # 4. Afficher les rÃ©sultats
    print(f"\nğŸ“Š RÃ©sultats:")
    print(f"   Score: {result['llm_score']}/100")
    print(f"   Statut: {'âœ… PASS' if result['passed'] else 'âŒ FAIL'} (seuil: 70)")
    print(f"   Reasoning:")
    # Afficher le reasoning avec indentation
    for line in result['llm_reasoning'].split('\n'):
        print(f"      {line}")
    
    return result


async def evaluate_all_tests():
    """
    Ã‰value tous les tests du Golden Dataset.
    """
    print("\n" + "="*70)
    print("ğŸ¯ Ã‰VALUATION COMPLÃˆTE DU GOLDEN DATASET")
    print("="*70)
    
    # 1. Initialiser les services
    print("\nğŸ“‚ Initialisation...")
    manager = GoldenDatasetManager()
    judge = LLMJudgeServiceSimplified(provider="anthropic")  # Utilise Claude par dÃ©faut
    
    # 2. Charger le Golden Dataset
    print("ğŸ“Š Chargement du Golden Dataset...")
    df = manager.load_golden_sets()
    total_tests = len(df)
    print(f"âœ… {total_tests} tests chargÃ©s")
    
    # 3. Ã‰valuer chaque test
    results = []
    passed = 0
    failed = 0
    total_score = 0
    
    for i in range(min(3, total_tests)):  # Limiter Ã  3 tests pour la dÃ©mo
        try:
            result = await evaluate_single_test(manager, judge, i)
            results.append(result)
            
            if result['passed']:
                passed += 1
            else:
                failed += 1
            
            total_score += result['llm_score']
            
            # Sauvegarder le rÃ©sultat
            manager.save_evaluation_result(result)
            
        except Exception as e:
            print(f"\nâŒ Erreur lors du test #{i+1}: {e}")
            failed += 1
    
    # 4. Afficher le rÃ©sumÃ©
    print("\n" + "="*70)
    print("ğŸ“ˆ RÃ‰SUMÃ‰ DE L'Ã‰VALUATION")
    print("="*70)
    print(f"Total de tests: {len(results)}")
    print(f"âœ… RÃ©ussis: {passed}")
    print(f"âŒ Ã‰chouÃ©s: {failed}")
    print(f"ğŸ“Š Score moyen: {total_score/len(results) if results else 0:.1f}/100")
    print(f"ğŸ¯ Taux de rÃ©ussite: {(passed/len(results)*100) if results else 0:.1f}%")
    
    # 5. Afficher les statistiques globales
    print("\nğŸ“Š Statistiques globales (toutes les Ã©valuations):")
    stats = manager.get_statistics_summary()
    
    if "message" in stats:
        print(f"   {stats['message']}")
    else:
        print(f"   Total Ã©valuations: {stats['total_evaluations']}")
        print(f"   RÃ©ussis: {stats['passed']}")
        print(f"   Ã‰chouÃ©s: {stats['failed']}")
        print(f"   Taux de rÃ©ussite: {stats['pass_rate']}%")
        print(f"   Score moyen: {stats['avg_score']}/100")
    
    print("\n" + "="*70)
    print("âœ… Ã‰valuation terminÃ©e!")
    print("="*70)
    print("\nğŸ“ Notes:")
    print("   â€¢ Ce script utilise un agent simulÃ© pour la dÃ©mo")
    print("   â€¢ Les rÃ©sultats sont sauvegardÃ©s dans evaluation_results.csv")
    print("   â€¢ Dans un vrai scÃ©nario, remplacez simulate_agent_response()")
    print("     par votre agent VyData rÃ©el")
    print()


if __name__ == "__main__":
    # ExÃ©cuter l'Ã©valuation
    asyncio.run(evaluate_all_tests())


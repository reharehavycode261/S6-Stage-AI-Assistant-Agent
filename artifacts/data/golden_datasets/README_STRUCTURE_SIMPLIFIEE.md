# ğŸ“‹ Golden Dataset - Structure SimplifiÃ©e

## ğŸ¯ Vue d'ensemble

Le Golden Dataset a Ã©tÃ© **simplifiÃ©** pour ne contenir que **2 colonnes essentielles**. Cette simplification facilite la maintenance et la comprÃ©hension du systÃ¨me d'Ã©valuation.

---

## ğŸ“Š Structure du fichier `golden_sets.csv`

### **Colonnes (seulement 2)**

| Colonne | Description | Exemple |
|---------|-------------|---------|
| `input_reference` | La question ou commande de test Ã  envoyer au systÃ¨me | "Analyse le fichier main.py" |
| `output_reference` | La rÃ©ponse parfaite attendue OU instruction d'Ã©valuation pour le LLM-as-judge | "Le fichier main.py contient une API FastAPI avec..." |

---

## ğŸ”„ Workflow d'Ã©valuation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Charger le Golden Dataset (input_reference)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Envoyer input_reference au systÃ¨me              â”‚
â”‚     â†’ Le systÃ¨me gÃ©nÃ¨re agent_output                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. LLM-as-judge compare:                           â”‚
â”‚     â€¢ agent_output (rÃ©ponse gÃ©nÃ©rÃ©e)                â”‚
â”‚     â€¢ output_reference (rÃ©ponse attendue)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. LLM-as-judge gÃ©nÃ¨re:                            â”‚
â”‚     â€¢ Score /100                                    â”‚
â”‚     â€¢ Reasoning (justification)                     â”‚
â”‚     â€¢ Passed (true/false)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Fichiers du systÃ¨me

### 1. **`golden_sets.csv`** - Dataset de rÃ©fÃ©rence

**Format:**
```csv
input_reference,output_reference
"Analyse le fichier main.py","Le fichier main.py contient une API FastAPI avec 5 endpoints principaux..."
"hello","Bonjour ! ğŸ‘‹ Je suis VyData, votre assistant IA de dÃ©veloppement..."
"CrÃ©e un formulaire de login","PR crÃ©Ã©e avec succÃ¨s sur la branche feat/login-form..."
```

**Nombre de tests actuels:** 16

---

### 2. **`evaluation_results.csv`** - RÃ©sultats d'Ã©valuation

**Format:**
```csv
timestamp,input_reference,output_reference,agent_output,llm_score,llm_reasoning,passed,duration_seconds
2025-11-10T14:30:00,Analyse main.py,"Fichier attendu...","Fichier gÃ©nÃ©rÃ©...",85.0,"Excellente analyse...",true,2.5
```

**Colonnes:**
- `timestamp`: Date et heure de l'Ã©valuation (ISO 8601)
- `input_reference`: Input du test
- `output_reference`: Output attendu
- `agent_output`: RÃ©ponse gÃ©nÃ©rÃ©e par l'agent
- `llm_score`: Score attribuÃ© par le LLM-as-judge (0-100)
- `llm_reasoning`: Justification du score
- `passed`: Test rÃ©ussi (true/false, seuil par dÃ©faut: 70)
- `duration_seconds`: DurÃ©e d'exÃ©cution

---

## ğŸš€ Utilisation

### **Charger le Golden Dataset**

```python
from services.evaluation.golden_dataset_manager import GoldenDatasetManager

# Initialiser le gestionnaire
manager = GoldenDatasetManager()

# Charger tous les tests
df_tests = manager.load_golden_sets()

# Afficher
print(f"ğŸ“Š {len(df_tests)} tests chargÃ©s")
print(df_tests.head())
```

### **RÃ©cupÃ©rer un test par index**

```python
# RÃ©cupÃ©rer le test Ã  l'index 0
test = manager.get_test_by_index(0)

print(f"Input: {test['input_reference']}")
print(f"Output attendu: {test['output_reference']}")
```

### **Sauvegarder un rÃ©sultat d'Ã©valuation**

```python
from datetime import datetime

result = {
    "timestamp": datetime.now().isoformat(),
    "input_reference": "Analyse le fichier main.py",
    "output_reference": "Le fichier main.py contient...",
    "agent_output": "RÃ©ponse gÃ©nÃ©rÃ©e par l'agent...",
    "llm_score": 85.0,
    "llm_reasoning": "Excellente analyse, trÃ¨s complÃ¨te...",
    "passed": True,
    "duration_seconds": 2.5
}

manager.save_evaluation_result(result)
```

### **RÃ©cupÃ©rer les statistiques**

```python
# Statistiques globales
stats = manager.get_statistics_summary()

print(f"Total Ã©valuations: {stats['total_evaluations']}")
print(f"Taux de rÃ©ussite: {stats['pass_rate']}%")
print(f"Score moyen: {stats['avg_score']}/100")
```

---

## âœ… Avantages de la structure simplifiÃ©e

1. **SimplicitÃ©** : Seulement 2 colonnes au lieu de 8
2. **ClartÃ©** : Facile Ã  comprendre et maintenir
3. **FlexibilitÃ©** : `output_reference` peut Ãªtre une rÃ©ponse attendue OU une instruction d'Ã©valuation
4. **EfficacitÃ©** : Moins de mÃ©tadonnÃ©es inutiles
5. **Focus** : Se concentre sur l'essentiel (input â†’ output)

---

## ğŸ“ Exemple complet de test

### Test dans `golden_sets.csv`:

```csv
input_reference,output_reference
"Analyse le fichier main.py","Le fichier main.py contient une API FastAPI avec 5 endpoints principaux: /health, /process, /status, /evaluation/run, /evaluation/report. Il utilise un agent VyData pour traiter les requÃªtes, intÃ¨gre Monday.com et GitHub, et gÃ¨re un workflow asynchrone avec LangGraph."
```

### RÃ©sultat dans `evaluation_results.csv`:

```csv
timestamp,input_reference,output_reference,agent_output,llm_score,llm_reasoning,passed,duration_seconds
2025-11-10T14:30:00,"Analyse le fichier main.py","Le fichier main.py contient...","L'agent a rÃ©pondu: Le fichier main.py...",85.0,"L'analyse est trÃ¨s complÃ¨te et prÃ©cise. Tous les endpoints sont identifiÃ©s correctement.",true,2.5
```

---

## ğŸ”§ Migration depuis l'ancien format

Un backup de l'ancien fichier a Ã©tÃ© crÃ©Ã© automatiquement:
- **Backup**: `golden_sets_old_backup.csv` (8 colonnes)
- **Nouveau**: `golden_sets.csv` (2 colonnes)

Les anciennes colonnes supprimÃ©es:
- `test_id` (remplacÃ© par l'index de ligne)
- `test_type` (non nÃ©cessaire)
- `expected_output_type` (non nÃ©cessaire)
- `evaluation_criteria` (gÃ©rÃ© par le LLM-as-judge)
- `priority` (non nÃ©cessaire)
- `active` (tous les tests sont actifs par dÃ©faut)

---

## ğŸ“ Support

Pour toute question sur la nouvelle structure, rÃ©fÃ©rez-vous Ã :
- **ModÃ¨les**: `/models/evaluation_models.py`
- **Gestionnaire**: `/services/evaluation/golden_dataset_manager.py`
- **Documentation complÃ¨te**: `/docs/GOLDEN_DATASET_EXPLICATION.md`


from typing import Dict, Any, List
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnableSequence
from ai.llm.llm_factory import get_llm_with_fallback
from utils.logger import get_logger

logger = get_logger(__name__)


class LLMJudgment(BaseModel):
    
    score: float = Field(
        ...,
        ge=0,
        le=100,
        description="Score global /100"
    )
    
    reasoning: str = Field(
        ...,
        description="Raisonnement d√©taill√© de l'√©valuation"
    )
    
    criteria_scores: Dict[str, float] = Field(
        default_factory=dict,
        description="Scores par crit√®re /100"
    )
    
    strengths: List[str] = Field(
        default_factory=list,
        description="Points forts de l'output de l'agent"
    )
    
    weaknesses: List[str] = Field(
        default_factory=list,
        description="Points faibles de l'output de l'agent"
    )
    
    suggestions: List[str] = Field(
        default_factory=list,
        description="Suggestions d'am√©lioration"
    )


def create_llm_judge_chain(
    provider: str = "anthropic",
    fallback_to_openai: bool = True
) -> RunnableSequence:
    """
    Cr√©e une cha√Æne LangChain pour √©valuer les outputs de l'agent.
    
    Args:
        provider: Provider LLM principal (anthropic ou openai)
        fallback_to_openai: Utiliser OpenAI en fallback si provider principal √©choue
        
    Returns:
        RunnableSequence configur√©e
    """
    logger.info(f"üîó Cr√©ation LLM Judge chain (provider: {provider})")
    
    parser = PydanticOutputParser(pydantic_object=LLMJudgment)
    
    system_prompt = """Tu es un **√©valuateur expert** charg√© de juger la qualit√© des outputs d'un agent IA de d√©veloppement.

Ton r√¥le est de comparer:
1. **L'output g√©n√©r√© par l'agent** (r√©ponse ou Pull Request)
2. **L'output attendu** (r√©ponse de r√©f√©rence ou PR de r√©f√©rence)

Tu dois √©valuer selon des **crit√®res sp√©cifiques** et attribuer:
- Un **score global /100**
- Des **scores par crit√®re /100**
- Un **raisonnement d√©taill√©**

## Principes d'√©valuation:

### Pour les QUESTIONS (Dataset 1):
- **Pertinence** (25%): La r√©ponse r√©pond-elle √† la question?
- **Pr√©cision technique** (30%): Les informations sont-elles correctes?
- **Clart√©** (20%): La r√©ponse est-elle compr√©hensible?
- **Utilisation du contexte** (25%): L'agent a-t-il bien utilis√© le contexte du projet?

### Pour les COMMANDES (Dataset 2):
- **Impl√©mentation** (35%): La fonctionnalit√© est-elle correctement impl√©ment√©e?
- **Qualit√© du code** (30%): Le code est-il propre, maintenable, bien structur√©?
- **Tests** (20%): Des tests appropri√©s sont-ils pr√©sents?
- **Documentation** (15%): Le PR est-il bien document√©?

## Bar√®me de notation:
- **90-100**: Excellent - D√©passe les attentes
- **75-89**: Tr√®s bon - R√©pond aux attentes
- **60-74**: Acceptable - R√©pond partiellement aux attentes
- **40-59**: Insuffisant - N√©cessite des am√©liorations importantes
- **0-39**: Tr√®s insuffisant - Ne r√©pond pas aux attentes

## Instructions:
1. Analyse l'output de l'agent avec attention
2. Compare avec l'output attendu
3. √âvalue selon les crit√®res fournis
4. Sois **objectif** mais **constructif**
5. Fournis des suggestions d'am√©lioration concr√®tes

{format_instructions}
"""
    
    user_prompt = """## üìã √âVALUATION √Ä EFFECTUER

### Type de test:
{test_type}

### Input de l'agent:
```
{input_text}
```

### Contexte additionnel:
{input_context}

### Output g√©n√©r√© par l'agent:
```
{agent_output}
```

### Output attendu (r√©f√©rence):
```
{expected_output}
```

### Crit√®res d'√©valuation √† utiliser:
{criteria}

---

**Effectue maintenant ton √©valuation d√©taill√©e en remplissant tous les champs demand√©s.**
"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", user_prompt)
    ])
    
    llm = get_llm_with_fallback(
        primary_provider=provider,
        fallback_providers=["openai"] if fallback_to_openai and provider != "openai" else None
    )
    
    chain = prompt | llm | parser
    
    logger.info("‚úÖ LLM Judge chain cr√©√©e avec succ√®s")
    
    return chain


async def evaluate_with_llm_judge(
    test_type: str,
    input_text: str,
    input_context: Dict[str, Any],
    agent_output: str,
    expected_output: str,
    criteria: List[str],
    provider: str = "anthropic",
    fallback_to_openai: bool = True
) -> LLMJudgment:
    """
    √âvalue un output de l'agent en utilisant le LLM Judge.
    
    Args:
        test_type: Type de test (question ou commande)
        input_text: Input donn√© √† l'agent
        input_context: Contexte de l'input
        agent_output: Output g√©n√©r√© par l'agent
        expected_output: Output attendu
        criteria: Crit√®res d'√©valuation
        provider: Provider LLM
        fallback_to_openai: Utiliser fallback
        
    Returns:
        LLMJudgment avec score et raisonnement
    """
    logger.info("‚öñÔ∏è √âvaluation avec LLM Judge...")
    logger.info(f"   Test type: {test_type}")
    logger.info(f"   Crit√®res: {len(criteria)}")
    
    try:
        chain = create_llm_judge_chain(provider, fallback_to_openai)
        
        criteria_str = "\n".join([f"- {c}" for c in criteria])
        context_str = "\n".join([f"- {k}: {v}" for k, v in input_context.items()])
        
        parser = PydanticOutputParser(pydantic_object=LLMJudgment)
        
        judgment = await chain.ainvoke({
            "test_type": test_type,
            "input_text": input_text,
            "input_context": context_str if context_str else "Aucun contexte additionnel",
            "agent_output": agent_output,
            "expected_output": expected_output,
            "criteria": criteria_str,
            "format_instructions": parser.get_format_instructions()
        })
        
        logger.info(f"‚úÖ √âvaluation termin√©e: Score {judgment.score}/100")
        logger.info(f"   Reasoning: {judgment.reasoning[:100]}...")
        
        return judgment
    
    except Exception as e:
        logger.error(f"‚ùå Erreur √©valuation LLM Judge: {e}", exc_info=True)
        raise


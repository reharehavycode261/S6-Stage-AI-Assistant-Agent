from enum import Enum
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

from config.settings import get_settings
from utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()


class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ImplementationStep(BaseModel):
    step_number: int = Field(description="NumÃ©ro de l'Ã©tape (sÃ©quentiel)")
    title: str = Field(description="Titre court de l'Ã©tape")
    description: str = Field(description="Description dÃ©taillÃ©e de ce qui doit Ãªtre fait")
    files_to_modify: List[str] = Field(
        default_factory=list,
        description="Liste des fichiers Ã  crÃ©er ou modifier"
    )
    dependencies: List[str] = Field(
        default_factory=list,
        description="DÃ©pendances/packages requis pour cette Ã©tape"
    )
    estimated_complexity: int = Field(
        ge=1,
        le=10,
        description="ComplexitÃ© estimÃ©e (1=trÃ¨s simple, 10=trÃ¨s complexe)"
    )
    risk_level: RiskLevel = Field(
        default=RiskLevel.LOW,
        description="Niveau de risque de cette Ã©tape"
    )
    risk_mitigation: Optional[str] = Field(
        default=None,
        description="StratÃ©gie de mitigation du risque si risque > LOW"
    )
    validation_criteria: List[str] = Field(
        default_factory=list,
        description="CritÃ¨res de validation pour considÃ©rer l'Ã©tape comme terminÃ©e"
    )


class ImplementationPlan(BaseModel):
    task_summary: str = Field(description="RÃ©sumÃ© de la tÃ¢che Ã  implÃ©menter")
    architecture_approach: str = Field(
        description="Approche architecturale recommandÃ©e"
    )
    steps: List[ImplementationStep] = Field(
        min_length=1,
        description="Liste ordonnÃ©e des Ã©tapes d'implÃ©mentation"
    )
    total_estimated_complexity: int = Field(
        ge=1,
        description="ComplexitÃ© totale estimÃ©e (somme des complexitÃ©s)"
    )
    overall_risk_assessment: str = Field(
        description="Ã‰valuation globale des risques du projet"
    )
    recommended_testing_strategy: str = Field(
        description="StratÃ©gie de tests recommandÃ©e"
    )
    potential_blockers: List[str] = Field(
        default_factory=list,
        description="Liste des bloqueurs potentiels identifiÃ©s"
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "task_summary": "CrÃ©er une API REST pour gÃ©rer les utilisateurs",
                "architecture_approach": "Architecture MVC avec FastAPI",
                "steps": [
                    {
                        "step_number": 1,
                        "title": "CrÃ©er les modÃ¨les de donnÃ©es",
                        "description": "DÃ©finir les modÃ¨les Pydantic pour User",
                        "files_to_modify": ["models/user.py"],
                        "dependencies": ["pydantic"],
                        "estimated_complexity": 3,
                        "risk_level": "low",
                        "validation_criteria": ["ModÃ¨les valident correctement"]
                    }
                ],
                "total_estimated_complexity": 15,
                "overall_risk_assessment": "Risque faible, technologies matures",
                "recommended_testing_strategy": "Tests unitaires + tests d'intÃ©gration",
                "potential_blockers": ["SchÃ©ma DB non finalisÃ©"]
            }
        }


def create_implementation_plan_chain(
    provider: str = "anthropic",
    model: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: int = 4000
):
    """
    CrÃ©e une chaÃ®ne LCEL pour gÃ©nÃ©rer un plan d'implÃ©mentation structurÃ©.
    
    Args:
        provider: Provider LLM Ã  utiliser ("anthropic" ou "openai")
        model: Nom du modÃ¨le (optionnel, utilise le dÃ©faut du provider)
        temperature: TempÃ©rature du modÃ¨le (0.0-1.0)
        max_tokens: Nombre maximum de tokens
        
    Returns:
        ChaÃ®ne LCEL configurÃ©e (Prompt â†’ LLM â†’ Parser)
        
    Raises:
        ValueError: Si le provider n'est pas supportÃ©
        Exception: Si les clÃ©s API sont manquantes
    """
    logger.info(f"ðŸ”— CrÃ©ation implementation_plan_chain avec provider={provider}")
    
    parser = PydanticOutputParser(pydantic_object=ImplementationPlan)
    
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """Tu es un architecte logiciel expert qui crÃ©e des plans d'implÃ©mentation dÃ©taillÃ©s et structurÃ©s.
Tu dois analyser la tÃ¢che fournie et gÃ©nÃ©rer un plan complet au format JSON strict.

IMPORTANT: Tu DOIS retourner UNIQUEMENT du JSON valide, sans texte avant ou aprÃ¨s.
Utilise le schÃ©ma suivant:

{format_instructions}

Sois prÃ©cis, actionnable et identifie tous les risques potentiels."""),
        ("user", """TÃ¢che Ã  analyser:

Titre: {task_title}
Description: {task_description}
Type: {task_type}
Contexte additionnel: {additional_context}

GÃ©nÃ¨re un plan d'implÃ©mentation complet et structurÃ©.""")
    ])
    
    prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
    
    if provider.lower() == "anthropic":
        if not settings.anthropic_api_key:
            raise Exception("ANTHROPIC_API_KEY manquante dans la configuration")
        
        llm = ChatAnthropic(
            model=model or "claude-3-5-sonnet-20241022",
            anthropic_api_key=settings.anthropic_api_key,
            temperature=temperature,
            max_tokens=max_tokens
        )
        logger.info(f"âœ… LLM Anthropic initialisÃ©: {model or 'claude-3-5-sonnet-20241022'}")
        
    elif provider.lower() == "openai":
        if not settings.openai_api_key:
            raise Exception("OPENAI_API_KEY manquante dans la configuration")
        
        llm = ChatOpenAI(
            model=model or "gpt-4",
            openai_api_key=settings.openai_api_key,
            temperature=temperature,
            max_tokens=max_tokens
        )
        logger.info(f"âœ… LLM OpenAI initialisÃ©: {model or 'gpt-4'}")
        
    else:
        raise ValueError(f"Provider non supportÃ©: {provider}. Utilisez 'anthropic' ou 'openai'")
    
    chain = prompt | llm | parser
    
    logger.info("âœ… Implementation plan chain crÃ©Ã©e avec succÃ¨s")
    return chain


async def generate_implementation_plan(
    task_title: str,
    task_description: str,
    task_type: str = "feature",
    additional_context: Optional[Dict[str, Any]] = None,
    provider: str = "anthropic",
    fallback_to_openai: bool = True,
    run_step_id: Optional[int] = None
) -> ImplementationPlan:
    """
    GÃ©nÃ¨re un plan d'implÃ©mentation structurÃ© avec fallback automatique.
    
    Args:
        task_title: Titre de la tÃ¢che
        task_description: Description dÃ©taillÃ©e
        task_type: Type de tÃ¢che (feature, bugfix, refactor, etc.)
        additional_context: Contexte additionnel (dict)
        provider: Provider principal ("anthropic" ou "openai")
        fallback_to_openai: Si True, fallback vers OpenAI si le provider principal Ã©choue
        
    Returns:
        ImplementationPlan validÃ© par Pydantic
        
    Raises:
        Exception: Si tous les providers Ã©chouent
    """
    context_str = str(additional_context) if additional_context else "Aucun contexte additionnel"
    
    inputs = {
        "task_title": task_title,
        "task_description": task_description,
        "task_type": task_type,
        "additional_context": context_str
    }
    
    callbacks = []
    if run_step_id:
        from utils.langchain_db_callback import create_db_callback
        callbacks = [create_db_callback(run_step_id)]
        logger.debug(f"ðŸ“ Callback DB activÃ© pour run_step_id={run_step_id}")
    
    try:
        logger.info(f"ðŸš€ GÃ©nÃ©ration plan avec {provider}...")
        chain = create_implementation_plan_chain(provider=provider)
        plan = await chain.ainvoke(inputs, config={"callbacks": callbacks})
        
        logger.info(f"âœ… Plan gÃ©nÃ©rÃ© avec succÃ¨s: {len(plan.steps)} Ã©tapes, complexitÃ©={plan.total_estimated_complexity}")
        return plan
        
    except Exception as e:
        logger.warning(f"âš ï¸ Ã‰chec gÃ©nÃ©ration plan avec {provider}: {e}")
        
        if fallback_to_openai and provider.lower() != "openai":
            try:
                logger.info("ðŸ”„ Fallback vers OpenAI...")
                chain_fallback = create_implementation_plan_chain(provider="openai")
                plan = await chain_fallback.ainvoke(inputs, config={"callbacks": callbacks})
                
                logger.info(f"âœ… Plan gÃ©nÃ©rÃ© avec succÃ¨s (fallback OpenAI): {len(plan.steps)} Ã©tapes")
                return plan
                
            except Exception as fallback_error:
                logger.error(f"âŒ Fallback OpenAI Ã©chouÃ©: {fallback_error}")
                raise Exception(f"Tous les providers ont Ã©chouÃ©. Principal: {e}, Fallback: {fallback_error}")
        
        raise Exception(f"GÃ©nÃ©ration plan Ã©chouÃ©e avec {provider}: {e}")


def extract_plan_metrics(plan: ImplementationPlan) -> Dict[str, Any]:
    high_risk_steps = [s for s in plan.steps if s.risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL]]
    files_to_modify = set()
    for step in plan.steps:
        files_to_modify.update(step.files_to_modify)
    
    return {
        "total_steps": len(plan.steps),
        "total_complexity": plan.total_estimated_complexity,
        "average_complexity": plan.total_estimated_complexity / len(plan.steps) if plan.steps else 0,
        "high_risk_steps_count": len(high_risk_steps),
        "high_risk_steps_percentage": (len(high_risk_steps) / len(plan.steps) * 100) if plan.steps else 0,
        "total_files_to_modify": len(files_to_modify),
        "total_blockers": len(plan.potential_blockers),
        "has_critical_risks": any(s.risk_level == RiskLevel.CRITICAL for s in plan.steps)
    }

